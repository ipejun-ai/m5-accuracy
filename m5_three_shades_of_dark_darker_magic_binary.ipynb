{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "m5_three_shades_of_dark_darker_magic_binary.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipejun-ai/m5-accuracy/blob/master/m5_three_shades_of_dark_darker_magic_binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "VY1mXqL-PvFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "\n",
        "# custom imports\n",
        "from multiprocessing import Pool        # Multiprocess Runs\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da5VwfTzQYpY",
        "colab_type": "code",
        "outputId": "05e1724f-8ba9-4e8f-949a-950905d37446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ0uXtDQQhAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIRPATH=\"/content/gdrive/My Drive/kaggle/m5-forecasting-accuracy/\"\n",
        "#DIRPATH=\"C:/Users/peiju/Documents/Study/kaggle/m5-forecasting-accuracy/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm1pyYhzPvFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helpers\n",
        "#################################################################################\n",
        "## Seeder\n",
        "# :seed to make all processes deterministic     # type: int\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    \n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hces_X43_pcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nolimittation of store\n",
        "def get_data():\n",
        "  # Read and contact basic feature\n",
        "    df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)\n",
        "    \n",
        "    # Leave only relevant store\n",
        "    #df = df[df['store_id']==store]\n",
        "\n",
        "    # With memory limits we have to read \n",
        "    # lags and mean encoding features\n",
        "    # separately and drop items that we don't need.\n",
        "    # As our Features Grids are aligned \n",
        "    # we can use index to keep only necessary rows\n",
        "    # Alignment is good for us as concat uses less memory than merge.\n",
        "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
        "    df2 = df2[df2.index.isin(df.index)]\n",
        "    \n",
        "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
        "    df3 = df3[df3.index.isin(df.index)]\n",
        "\n",
        "\n",
        "    df4 = pd.read_pickle(SNAP).iloc[:,2:]\n",
        "    df4 = df4[df4.index.isin(df.index)]\n",
        "\n",
        "    #df5 = pd.read_pickle(EVENT_TYPE).iloc[:,2:]\n",
        "    #df5 = df5[df5.index.isin(df.index)]\n",
        "\n",
        "    df6 = pd.read_pickle(EVENT_VALUE).iloc[:,2]\n",
        "    df6 = df6[df6.index.isin(df.index)]\n",
        "\n",
        "    df7 = pd.read_pickle(LAGS_364).iloc[:,2:]\n",
        "    df7 = df7[df7.index.isin(df.index)]\n",
        "\n",
        "    df8 = pd.read_pickle(HOLIDAY).iloc[:,2:]\n",
        "    df8 = df8[df8.index.isin(df.index)]\n",
        "\n",
        "    #df5 = pd.read_pickle(EVENT_TYPE).iloc[:,2:]\n",
        "    #df5 = df5[df5.index.isin(df.index)]\n",
        "    \n",
        "    df = pd.concat([df, df2], axis=1)\n",
        "    del df2 # to not reach memory limit \n",
        "    \n",
        "    df = pd.concat([df, df3], axis=1)\n",
        "    del df3 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df4], axis=1)\n",
        "    del df4 # to not reach memory limit \n",
        "\n",
        "    #df = pd.concat([df, df5], axis=1)\n",
        "    #del df5 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df6], axis=1)\n",
        "    del df6 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df7], axis=1)\n",
        "    del df7 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df8], axis=1)\n",
        "    del df8 # to not reach memory limit  \n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "    # Create features list\n",
        "    features = [col for col in list(df) if col not in remove_features]\n",
        "    df = df[['id','d',TARGET]+features]\n",
        "    \n",
        "    # Skipping first n rows\n",
        "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
        "    \n",
        "    return df, features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "413KNvT2PvFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helper to load data by store ID\n",
        "#################################################################################\n",
        "# Read data\n",
        "def get_data_by_store(store):\n",
        "    \n",
        "    # Read and contact basic feature\n",
        "    df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)\n",
        "    \n",
        "    # Leave only relevant store\n",
        "    df = df[df['store_id']==store]\n",
        "\n",
        "    # With memory limits we have to read \n",
        "    # lags and mean encoding features\n",
        "    # separately and drop items that we don't need.\n",
        "    # As our Features Grids are aligned \n",
        "    # we can use index to keep only necessary rows\n",
        "    # Alignment is good for us as concat uses less memory than merge.\n",
        "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
        "    df2 = df2[df2.index.isin(df.index)]\n",
        "    \n",
        "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
        "    df3 = df3[df3.index.isin(df.index)]\n",
        "\n",
        "\n",
        "    df4 = pd.read_pickle(SNAP).iloc[:,2:]\n",
        "    df4 = df4[df4.index.isin(df.index)]\n",
        "\n",
        "    #df5 = pd.read_pickle(EVENT_TYPE).iloc[:,2:]\n",
        "    #df5 = df5[df5.index.isin(df.index)]\n",
        "\n",
        "    df6 = pd.read_pickle(EVENT_VALUE).iloc[:,2]\n",
        "    df6 = df6[df6.index.isin(df.index)]\n",
        "\n",
        "    df7 = pd.read_pickle(LAGS_364).iloc[:,2:]\n",
        "    df7 = df7[df7.index.isin(df.index)]\n",
        "\n",
        "    df8 = pd.read_pickle(HOLIDAY).iloc[:,2:]\n",
        "    df8 = df8[df8.index.isin(df.index)]\n",
        "\n",
        "    #df5 = pd.read_pickle(EVENT_TYPE).iloc[:,2:]\n",
        "    #df5 = df5[df5.index.isin(df.index)]\n",
        "    \n",
        "    df = pd.concat([df, df2], axis=1)\n",
        "    del df2 # to not reach memory limit \n",
        "    \n",
        "    df = pd.concat([df, df3], axis=1)\n",
        "    del df3 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df4], axis=1)\n",
        "    del df4 # to not reach memory limit \n",
        "\n",
        "    #df = pd.concat([df, df5], axis=1)\n",
        "    #del df5 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df6], axis=1)\n",
        "    del df6 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df7], axis=1)\n",
        "    del df7 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df8], axis=1)\n",
        "    del df8 # to not reach memory limit  \n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "    # Create features list\n",
        "    features = [col for col in list(df) if col not in remove_features]\n",
        "    df = df[['id','d',TARGET]+features]\n",
        "    \n",
        "    # Skipping first n rows\n",
        "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
        "    \n",
        "    return df, features\n",
        "\n",
        "# Recombine Test set after training\n",
        "def get_base_test():\n",
        "    base_test = pd.DataFrame()\n",
        "\n",
        "    for store_id in STORES_IDS:\n",
        "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
        "        temp_df['store_id'] = store_id\n",
        "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
        "    \n",
        "    return base_test\n",
        "\n",
        "\n",
        "########################### Helper to make dynamic rolling lags\n",
        "#################################################################################\n",
        "def make_lag(LAG_DAY):\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "\n",
        "def make_lag_roll(LAG_DAY):\n",
        "    shift_day = LAG_DAY[0]\n",
        "    roll_wind = LAG_DAY[1]\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
        "    return lag_df[[col_name]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NDcdsu0PvFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Model params\n",
        "#################################################################################\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    #'boosting_type': 'gbdt',\n",
        "                    'boosting': 'dart',          # dart (drop out trees) often performs better\n",
        "                    'application': 'binary',\n",
        "                   \n",
        "                    'metric': 'auc',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    #'n_estimators': 1400,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': -1,\n",
        "                } \n",
        "\n",
        "# Let's look closer on params\n",
        "\n",
        "## 'boosting_type': 'gbdt'\n",
        "# we have 'goss' option for faster training\n",
        "# but it normally leads to underfit.\n",
        "# Also there is good 'dart' mode\n",
        "# but it takes forever to train\n",
        "# and model performance depends \n",
        "# a lot on random factor \n",
        "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
        "\n",
        "## 'objective': 'tweedie'\n",
        "# Tweedie Gradient Boosting for Extremely\n",
        "# Unbalanced Zero-inflated Data\n",
        "# https://arxiv.org/pdf/1811.10192.pdf\n",
        "# and many more articles about tweediie\n",
        "#\n",
        "# Strange (for me) but Tweedie is close in results\n",
        "# to my own ugly loss.\n",
        "# My advice here - make OWN LOSS function\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
        "# I think many of you already using it (after poisson kernel appeared) \n",
        "# (kagglers are very good with \"params\" testing and tuning).\n",
        "# Try to figure out why Tweedie works.\n",
        "# probably it will show you new features options\n",
        "# or data transformation (Target transformation?).\n",
        "\n",
        "## 'tweedie_variance_power': 1.1\n",
        "# default = 1.5\n",
        "# set this closer to 2 to shift towards a Gamma distribution\n",
        "# set this closer to 1 to shift towards a Poisson distribution\n",
        "# my CV shows 1.1 is optimal \n",
        "# but you can make your own choice\n",
        "\n",
        "## 'metric': 'rmse'\n",
        "# Doesn't mean anything to us\n",
        "# as competition metric is different\n",
        "# and we don't use early stoppings here.\n",
        "# So rmse serves just for general \n",
        "# model performance overview.\n",
        "# Also we use \"fake\" validation set\n",
        "# (as it makes part of the training set)\n",
        "# so even general rmse score doesn't mean anything))\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
        "\n",
        "## 'subsample': 0.5\n",
        "# Serves to fight with overfit\n",
        "# this will randomly select part of data without resampling\n",
        "# Chosen by CV (my CV can be wrong!)\n",
        "# Next kernel will be about CV\n",
        "\n",
        "##'subsample_freq': 1\n",
        "# frequency for bagging\n",
        "# default value - seems ok\n",
        "\n",
        "## 'learning_rate': 0.03\n",
        "# Chosen by CV\n",
        "# Smaller - longer training\n",
        "# but there is an option to stop \n",
        "# in \"local minimum\"\n",
        "# Bigger - faster training\n",
        "# but there is a chance to\n",
        "# not find \"global minimum\" minimum\n",
        "\n",
        "## 'num_leaves': 2**11-1\n",
        "## 'min_data_in_leaf': 2**12-1\n",
        "# Force model to use more features\n",
        "# We need it to reduce \"recursive\"\n",
        "# error impact.\n",
        "# Also it leads to overfit\n",
        "# that's why we use small \n",
        "# 'max_bin': 100\n",
        "\n",
        "## l1, l2 regularizations\n",
        "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
        "# Good tiny explanation\n",
        "# l2 can work with bigger num_leaves\n",
        "# but my CV doesn't show boost\n",
        "                    \n",
        "## 'n_estimators': 1400\n",
        "# CV shows that there should be\n",
        "# different values for each state/store.\n",
        "# Current value was chosen \n",
        "# for general purpose.\n",
        "# As we don't use any early stopings\n",
        "# careful to not overfit Public LB.\n",
        "\n",
        "##'feature_fraction': 0.5\n",
        "# LightGBM will randomly select \n",
        "# part of features on each iteration (tree).\n",
        "# We have maaaany features\n",
        "# and many of them are \"duplicates\"\n",
        "# and many just \"noise\"\n",
        "# good values here - 0.5-0.7 (by CV)\n",
        "\n",
        "## 'boost_from_average': False\n",
        "# There is some \"problem\"\n",
        "# to code boost_from_average for \n",
        "# custom loss\n",
        "# 'True' makes training faster\n",
        "# BUT carefull use it\n",
        "# https://github.com/microsoft/LightGBM/issues/1514\n",
        "# not our case but good to know cons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvwIzPcOPvFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIBvKIAor2D-",
        "colab_type": "text"
      },
      "source": [
        "# Vars & Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEfqHZHpPvFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Vars\n",
        "#################################################################################\n",
        "VER = 22                          # Our model version\n",
        "SEED = 42                        # We want all things\n",
        "seed_everything(SEED)            # to be as deterministic \n",
        "lgb_params['seed'] = SEED        # as possible\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "\n",
        "#LIMITS and const\n",
        "TARGET      = 'sales'            # Our target\n",
        "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
        "P_HORIZON   = 28\n",
        "END_TRAIN   = 1913-P_HORIZON             # End day of our train set\n",
        "                 # Prediction horizon\n",
        "USE_AUX     = False           # Use or not pretrained models\n",
        "USE_PKL     =False\n",
        "\n",
        "#FEATURES to remove\n",
        "## These features lead to overfit\n",
        "## or values not present in test set\n",
        "remove_features = ['id','state_id'\n",
        "                    #,'store_id',\n",
        "                   'date','wm_yr_wk','d','snap_CA','snap_TX','snap_WI'\n",
        "                   ,\"event_name_1\",\n",
        "                   #\"event_type_1\",\"event_name_2\",\"event_type_2\",\n",
        "                   TARGET]\n",
        "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
        "                   'enc_dept_id_mean','enc_dept_id_std',\n",
        "                   'enc_item_id_mean','enc_item_id_std'] \n",
        "\n",
        "##selected feature by boruta\n",
        "selected_feature=['item_id', 'dept_id', 'release', 'sell_price', 'price_max', 'price_min',\n",
        "       'price_std', 'price_mean', 'price_norm', 'item_nunique',\n",
        "       'price_momentum_m', 'price_momentum_y', 'event_type_1', 'tm_d', 'tm_w',\n",
        "       'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_dept_id_mean',\n",
        "       'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std',\n",
        "       'sales_lag_28', 'sales_lag_35', 'rolling_mean_7', 'rolling_std_7',\n",
        "       'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30',\n",
        "       'rolling_std_30', 'rolling_mean_60', 'rolling_std_60',\n",
        "       'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7',\n",
        "       'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30',\n",
        "       'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7',\n",
        "       'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30',\n",
        "       'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7',\n",
        "       'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30',\n",
        "       'rolling_mean_tmp_14_60', 'snap', 'Event_total',\n",
        "       'sales_lag_28_364', 'rolling_mean_7_364',\n",
        "       'rolling_std_7_364', 'rolling_mean_14_364', 'rolling_std_14_364',\n",
        "       'rolling_mean_30_364', 'rolling_std_30_364', 'holiday',\n",
        "       'rolling_holiday_mean_7', 'rolling_holiday_mean_14',\n",
        "       'rolling_holiday_mean_30', 'rolling_holiday_mean_60',\n",
        "       'rolling_holiday_mean_180']\n",
        "\n",
        "#PATHS for Features\n",
        "ORIGINAL = DIRPATH\n",
        "BASE     = DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'\n",
        "PRICE    = DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl'\n",
        "CALENDAR = DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl'\n",
        "LAGS     = DIRPATH+'/output/m5-lags-features/lags_df_28.pkl'\n",
        "MEAN_ENC = DIRPATH+'/output/m5-custom-features/mean_encoding_df.pkl'\n",
        "FINAL_SALES = DIRPATH+'/output/m5-custom-features/finalsales_df.pkl'\n",
        "SNAP      =  DIRPATH+'/output/m5-custom-features/snap_df.pkl'\n",
        "EVENT_TYPE      =  DIRPATH+'/output/m5-custom-features/event_type.pkl'\n",
        "EVENT_VALUE      =  DIRPATH+'/output/m5-custom-features/event_value.pkl'\n",
        "LAGS_364=  DIRPATH+'/output/m5-custom-features/lags_df_364.pkl'\n",
        "HOLIDAY=DIRPATH+'/output/m5-custom-features/holiday_mean.pkl'\n",
        "\n",
        "\n",
        "#PATHS for Features\n",
        "\"\"\"\n",
        "ORIGINAL = DIRPATH\n",
        "BASE     = DIRPATH+'/pickle/grid_part_1.pkl'\n",
        "PRICE    = DIRPATH+'/pickle/grid_part_2.pkl'\n",
        "CALENDAR = DIRPATH+'/pickle/grid_part_3.pkl'\n",
        "LAGS     = DIRPATH+'//pickle//lags_df_28.pkl'\n",
        "MEAN_ENC = DIRPATH+'//pickle//mean_encoding_df.pkl'\n",
        "FINAL_SALES = DIRPATH+'//pickle//finalsales_df.pkl'\n",
        "SNAP      =  DIRPATH+'//pickle//snap_df.pkl'\n",
        "EVENT_TYPE      =  DIRPATH+'//pickle//event_type.pkl'\n",
        "EVENT_VALUE      =  DIRPATH+'//pickle//event_value.pkl'\n",
        "LAGS_364=  DIRPATH+'//pickle//lags_df_364.pkl'\n",
        "HOLIDAY=DIRPATH+'//pickle//holiday_mean.pkl'\n",
        "\"\"\"\n",
        "# AUX(pretrained) Models paths\n",
        "AUX_MODELS = DIRPATH+'/output/m5-three-shades-of-dark-darker-magic_binary/'\n",
        "\n",
        "SAVE_DIR = DIRPATH+'/output/m5-three-shades-of-dark-darker-magic_binary/'\n",
        "\n",
        "\n",
        "#STORES ids\n",
        "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
        "STORES_IDS = list(STORES_IDS.unique())\n",
        "\n",
        "\n",
        "#SPLITS for lags creation\n",
        "SHIFT_DAY  = 28\n",
        "N_LAGS     = 15\n",
        "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
        "ROLS_SPLIT = []\n",
        "for i in [1,7,14]:\n",
        "    for j in [7,14,30,60]:\n",
        "        ROLS_SPLIT.append([i,j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61rzWwz3PvFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Aux Models\n",
        "# If you don't want to wait hours and hours\n",
        "# to have result you can train each store \n",
        "# in separate kernel and then just join result.\n",
        "\n",
        "# If we want to use pretrained models we can \n",
        "## skip training \n",
        "## (in our case do dummy training\n",
        "##  to show that we are good with memory\n",
        "##  and you can safely use this (all kernel) code)\n",
        "if USE_AUX:\n",
        "    lgb_params['n_estimators'] = 2\n",
        "    \n",
        "# Here is some 'logs' that can compare\n",
        "#Train CA_1\n",
        "#[100]\tvalid_0's rmse: 2.02289\n",
        "#[200]\tvalid_0's rmse: 2.0017\n",
        "#[300]\tvalid_0's rmse: 1.99239\n",
        "#[400]\tvalid_0's rmse: 1.98471\n",
        "#[500]\tvalid_0's rmse: 1.97923\n",
        "#[600]\tvalid_0's rmse: 1.97284\n",
        "#[700]\tvalid_0's rmse: 1.96763\n",
        "#[800]\tvalid_0's rmse: 1.9624\n",
        "#[900]\tvalid_0's rmse: 1.95673\n",
        "#[1000]\tvalid_0's rmse: 1.95201\n",
        "#[1100]\tvalid_0's rmse: 1.9476\n",
        "#[1200]\tvalid_0's rmse: 1.9434\n",
        "#[1300]\tvalid_0's rmse: 1.9392\n",
        "#[1400]\tvalid_0's rmse: 1.93446\n",
        "\n",
        "#Train CA_2\n",
        "#[100]\tvalid_0's rmse: 1.88949\n",
        "#[200]\tvalid_0's rmse: 1.84767\n",
        "#[300]\tvalid_0's rmse: 1.83653\n",
        "#[400]\tvalid_0's rmse: 1.82909\n",
        "#[500]\tvalid_0's rmse: 1.82265\n",
        "#[600]\tvalid_0's rmse: 1.81725\n",
        "#[700]\tvalid_0's rmse: 1.81252\n",
        "#[800]\tvalid_0's rmse: 1.80736\n",
        "#[900]\tvalid_0's rmse: 1.80242\n",
        "#[1000]\tvalid_0's rmse: 1.79821\n",
        "#[1100]\tvalid_0's rmse: 1.794\n",
        "#[1200]\tvalid_0's rmse: 1.78973\n",
        "#[1300]\tvalid_0's rmse: 1.78552\n",
        "#[1400]\tvalid_0's rmse: 1.78158"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53crVk-U7-eY",
        "colab_type": "code",
        "outputId": "66d09d81-7c6c-4cbb-d03d-7c15210ae524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "if USE_PKL:\n",
        "  grid_df_all=pd.read_pickle(AUX_MODELS+'grid_df_all_v'+str(VER)+'.pkl') \n",
        "  features=pd.read_csv(AUX_MODELS+'feature_v'+str(VER)+'.csv',header=None)\n",
        "  features_columns=list(features[0])\n",
        "else:\n",
        "  grid_df_all, features_columns = get_data()\n",
        "  grid_df_all.to_pickle(AUX_MODELS+'grid_df_all_v'+str(VER)+'.pkl')\n",
        "  imp=pd.DataFrame({'feature': features_columns})\n",
        "  imp.to_csv(AUX_MODELS+'feature_v'+str(VER)+'.csv', index=False)\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nif USE_PKL:\\n  grid_df_all=pd.read_pickle(AUX_MODELS+'grid_df_all_v'+str(VER)+'.pkl') \\n  features=pd.read_csv(AUX_MODELS+'feature_v'+str(VER)+'.csv',header=None)\\n  features_columns=list(features[0])\\nelse:\\n  grid_df_all, features_columns = get_data()\\n  grid_df_all.to_pickle(AUX_MODELS+'grid_df_all_v'+str(VER)+'.pkl')\\n  imp=pd.DataFrame({'feature': features_columns})\\n  imp.to_csv(AUX_MODELS+'feature_v'+str(VER)+'.csv', index=False)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQV9fhMUrpTd",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJJPOV1IPvF3",
        "colab_type": "code",
        "outputId": "02814d76-1986-4c22-bf63-05db22621263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "########################### Train Models\n",
        "#################################################################################\n",
        "for store_id in STORES_IDS:\n",
        "    print('Train', store_id)\n",
        "    \n",
        "    # Get grid for current store\n",
        "    grid_df, features_columns = get_data_by_store(store_id)\n",
        "    #change_binary\n",
        "    grid_df.loc[grid_df[\"sales\"]>0,\"sales\"]=1\n",
        "    grid_df.loc[grid_df[\"sales\"]==0,\"sales\"]=0\n",
        "\n",
        "    #grid_df=grid_df_all[grid_df_all['store_id']==store_id]\n",
        "    #grid_df=grid_df.drop(\"store_id\",axis=1)\n",
        "    ##borutaで選択した特徴量に変換\n",
        "    features_columns=[x for x in features_columns if x in selected_feature]\n",
        "    \n",
        "    # Masks for \n",
        "    # Train (All data less than 1913)\n",
        "    # \"Validation\" (Last 28 days - not real validatio set)\n",
        "    # Test (All data greater than 1913 day, \n",
        "    #       with some gap for recursive features)\n",
        "    train_mask = grid_df['d']<=END_TRAIN\n",
        "    valid_mask = (grid_df['d']<=(END_TRAIN+P_HORIZON))&(grid_df['d']>(END_TRAIN))\n",
        "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
        "    \n",
        "    # Apply masks and save lgb dataset as bin\n",
        "    # to reduce memory spikes during dtype convertations\n",
        "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
        "    # \"To avoid any conversions, you should always use np.float32\"\n",
        "    # or save to bin before start training\n",
        "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
        "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
        "                       label=grid_df[train_mask][TARGET])\n",
        "    train_data.save_binary('train_data.bin')\n",
        "    train_data = lgb.Dataset('train_data.bin')\n",
        "    \n",
        "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
        "                       label=grid_df[valid_mask][TARGET])\n",
        "    \n",
        "    # Saving part of the dataset for later predictions\n",
        "    # Removing features that we need to calculate recursively \n",
        "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
        "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
        "    grid_df = grid_df[keep_cols]\n",
        "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
        "    del grid_df\n",
        "    \n",
        "    # Launch seeder again to make lgb training 100% deterministic\n",
        "    # with each \"code line\" np.random \"evolves\" \n",
        "    # so we need (may want) to \"reset\" it\n",
        "    seed_everything(SEED)\n",
        "    estimator = lgb.train(lgb_params,\n",
        "                          train_data,\n",
        "                          valid_sets = [valid_data],\n",
        "                          verbose_eval = 100,\n",
        "                          )\n",
        "    \n",
        "    # Save model - it's not real '.bin' but a pickle file\n",
        "    # estimator = lgb.Booster(model_file='model.txt')\n",
        "    # can only predict with the best iteration (or the saving iteration)\n",
        "    # pickle.dump gives us more flexibility\n",
        "    # like estimator.predict(TEST, num_iteration=100)\n",
        "    # num_iteration - number of iteration want to predict with, \n",
        "    # NULL or <= 0 means use best iteration\n",
        "    if USE_AUX==False:\n",
        "      model_name = AUX_MODELS+'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
        "      pickle.dump(estimator, open(model_name, 'wb'))\n",
        "\n",
        "    # Remove temporary files and objects \n",
        "    # to free some hdd space and ram memory\n",
        "    !rm train_data.bin\n",
        "    del train_data, valid_data, estimator\n",
        "    gc.collect()\n",
        "    \n",
        "    # \"Keep\" models features for predictions\n",
        "    MODEL_FEATURES = features_columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train CA_1\n",
            "[100]\tvalid_0's auc: 0.825178\n",
            "Train CA_2\n",
            "[100]\tvalid_0's auc: 0.810465\n",
            "Train CA_3\n",
            "[100]\tvalid_0's auc: 0.829121\n",
            "Train CA_4\n",
            "[100]\tvalid_0's auc: 0.797963\n",
            "Train TX_1\n",
            "[100]\tvalid_0's auc: 0.819183\n",
            "Train TX_2\n",
            "[100]\tvalid_0's auc: 0.808797\n",
            "Train TX_3\n",
            "[100]\tvalid_0's auc: 0.822968\n",
            "Train WI_1\n",
            "[100]\tvalid_0's auc: 0.820104\n",
            "Train WI_2\n",
            "[100]\tvalid_0's auc: 0.846052\n",
            "Train WI_3\n",
            "[100]\tvalid_0's auc: 0.824103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPs4HqEApxpn",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAnA3lpLoqJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############save importance\n",
        "for store_id in STORES_IDS:\n",
        "  model_path = AUX_MODELS + 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
        "  estimator = pickle.load(open(model_path, 'rb'))\n",
        "  \n",
        "  if store_id==\"CA_1\":\n",
        "    imp=pd.DataFrame({'feature': features_columns,store_id:estimator.feature_importance()})\n",
        "  else:\n",
        "    tmp=pd.DataFrame({'feature': features_columns,store_id:estimator.feature_importance()})\n",
        "    imp=pd.merge(imp,tmp,how=\"left\",on=\"feature\")\n",
        "\n",
        "imp.index=imp[\"feature\"]\n",
        "imp=imp.drop(\"feature\",axis=1)\n",
        "imp[\"mean\"]=imp.mean(axis=1)\n",
        "imp.to_csv(AUX_MODELS+\"importance_v\"+str(VER)+\".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0khRIZMq5w0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "import gc\n",
        "\n",
        "from sklearn import preprocessing\n",
        "import lightgbm as lgb\n",
        "\n",
        "from typing import Union\n",
        "from tqdm.notebook import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIHrqkKTgA-i",
        "colab_type": "code",
        "outputId": "5e5f9767-9a45-46b4-a082-8569449bdb4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########################### Predict Train & Valid\n",
        "#################################################################################\n",
        "\n",
        "# Create Dummy DataFrame to store predictions\n",
        "all_preds = pd.DataFrame()\n",
        "\n",
        "# Join back the Test dataset with \n",
        "# a small part of the training data \n",
        "# to make recursive features\n",
        "base_test = get_base_test()\n",
        "\n",
        "# Timer to measure predictions time \n",
        "main_time = time.time()\n",
        "\n",
        "# Loop over each prediction day\n",
        "# As rolling lags are the most timeconsuming\n",
        "# we will calculate it for whole day\n",
        "for PREDICT_DAY in range(1,29):    \n",
        "#for PREDICT_DAY in range(1,2):    \n",
        " \n",
        "    print('Predict | Day:', PREDICT_DAY,END_TRAIN+PREDICT_DAY)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Make temporary grid to calculate rolling lags\n",
        "    grid_df = base_test.copy()\n",
        "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "        \n",
        "    for store_id in STORES_IDS:\n",
        "        \n",
        "        # Read all our models and make predictions\n",
        "        # for each day/store pairs\n",
        "        model_path = AUX_MODELS + 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
        "        if USE_AUX:\n",
        "            model_path = model_path\n",
        "        \n",
        "        estimator = pickle.load(open(model_path, 'rb'))\n",
        "        \n",
        "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
        "        store_mask = base_test['store_id']==store_id\n",
        "        \n",
        "        mask = (day_mask)&(store_mask)\n",
        "        #base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "        base_test.loc[mask,TARGET]=estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "    # Make good column naming and add \n",
        "    # to all_preds DataFrame\n",
        "    temp_df = base_test[day_mask][['id',TARGET]]\n",
        "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "    if 'id' in list(all_preds):\n",
        "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "    else:\n",
        "        all_preds = temp_df.copy()\n",
        "        \n",
        "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    del temp_df\n",
        "    \n",
        "all_preds = all_preds.reset_index(drop=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict | Day: 1 1886\n",
            "##########  0.66 min round |  0.66 min total |  11159.71 day sales |\n",
            "Predict | Day: 2 1887\n",
            "##########  0.63 min round |  1.29 min total |  11402.00 day sales |\n",
            "Predict | Day: 3 1888\n",
            "##########  0.61 min round |  1.89 min total |  11455.79 day sales |\n",
            "Predict | Day: 4 1889\n",
            "##########  0.61 min round |  2.50 min total |  11379.37 day sales |\n",
            "Predict | Day: 5 1890\n",
            "##########  0.62 min round |  3.13 min total |  11874.62 day sales |\n",
            "Predict | Day: 6 1891\n",
            "##########  0.62 min round |  3.74 min total |  12726.47 day sales |\n",
            "Predict | Day: 7 1892\n",
            "##########  0.63 min round |  4.37 min total |  12732.15 day sales |\n",
            "Predict | Day: 8 1893\n",
            "##########  0.62 min round |  4.99 min total |  11554.57 day sales |\n",
            "Predict | Day: 9 1894\n",
            "##########  0.61 min round |  5.60 min total |  11598.49 day sales |\n",
            "Predict | Day: 10 1895\n",
            "##########  0.61 min round |  6.21 min total |  11564.79 day sales |\n",
            "Predict | Day: 11 1896\n",
            "##########  0.61 min round |  6.82 min total |  11512.35 day sales |\n",
            "Predict | Day: 12 1897\n",
            "##########  0.61 min round |  7.43 min total |  11722.55 day sales |\n",
            "Predict | Day: 13 1898\n",
            "##########  0.61 min round |  8.03 min total |  12611.72 day sales |\n",
            "Predict | Day: 14 1899\n",
            "##########  0.61 min round |  8.65 min total |  12566.96 day sales |\n",
            "Predict | Day: 15 1900\n",
            "##########  0.62 min round |  9.26 min total |  11483.87 day sales |\n",
            "Predict | Day: 16 1901\n",
            "##########  0.61 min round |  9.88 min total |  11434.22 day sales |\n",
            "Predict | Day: 17 1902\n",
            "##########  0.61 min round |  10.49 min total |  11370.62 day sales |\n",
            "Predict | Day: 18 1903\n",
            "##########  0.61 min round |  11.11 min total |  11372.48 day sales |\n",
            "Predict | Day: 19 1904\n",
            "##########  0.61 min round |  11.72 min total |  11609.77 day sales |\n",
            "Predict | Day: 20 1905\n",
            "##########  0.61 min round |  12.33 min total |  12462.69 day sales |\n",
            "Predict | Day: 21 1906\n",
            "##########  0.61 min round |  12.95 min total |  12453.99 day sales |\n",
            "Predict | Day: 22 1907\n",
            "##########  0.62 min round |  13.57 min total |  11299.23 day sales |\n",
            "Predict | Day: 23 1908\n",
            "##########  0.62 min round |  14.18 min total |  11230.36 day sales |\n",
            "Predict | Day: 24 1909\n",
            "##########  0.62 min round |  14.80 min total |  11209.82 day sales |\n",
            "Predict | Day: 25 1910\n",
            "##########  0.62 min round |  15.41 min total |  11199.15 day sales |\n",
            "Predict | Day: 26 1911\n",
            "##########  0.61 min round |  16.02 min total |  11430.74 day sales |\n",
            "Predict | Day: 27 1912\n",
            "##########  0.61 min round |  16.63 min total |  12379.41 day sales |\n",
            "Predict | Day: 28 1913\n",
            "##########  0.61 min round |  17.24 min total |  12345.61 day sales |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb-gUz08ouEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reload data\n",
        "d_dtypes = {}\n",
        "for i in range(1914):\n",
        "    d_dtypes[f'd_{i}'] = np.int32\n",
        "sales = pd.read_csv(DIRPATH + 'sales_train_validation.csv',\n",
        "                    dtype=d_dtypes)\n",
        "\n",
        "# changing wide format to long format for model training\n",
        "d = ['d_' + str(i) for i in range(1802,1914)]\n",
        "sales_mlt = pd.melt(sales, id_vars=['item_id','dept_id','cat_id','store_id',\n",
        "                                    'state_id'], value_vars=d)\n",
        "sales_mlt = sales_mlt.rename(columns={'variable':'d', 'value':'sales'})\n",
        "\n",
        "calendar = pd.read_csv(DIRPATH + 'calendar.csv',\n",
        "                       dtype={'wm_yr_wk': np.int32, 'wday': np.int32, \n",
        "                              'month': np.int32, 'year': np.int32, \n",
        "                              'snap_CA': np.int32, 'snap_TX': np.int32,\n",
        "                              'snap_WI': np.int32})\n",
        "\n",
        "# subsetting calender by traning period\n",
        "calendar = calendar.loc[calendar.d.apply(lambda x: int(x[2:])) \\\n",
        "                        >= int(sales_mlt.d[0][2:]), :]\n",
        "\n",
        "prices = pd.read_csv(DIRPATH + 'sell_prices.csv',\n",
        "                          dtype={'wm_yr_wk': np.int32, \n",
        "                                 'sell_price': np.float32})\n",
        "\n",
        "train_df = sales.iloc[:, :-28]\n",
        "valid_df = sales.iloc[:, -28:]\n",
        "\n",
        "#evaluator = WRMSSEEvaluator(train_df, valid_df, calendar, prices)\n",
        "\n",
        "#sort\n",
        "all_preds=pd.merge(train_df[\"id\"],all_preds,on=\"id\",how=\"left\")\n",
        "all_preds=all_preds.drop(\"id\",axis=1)\n",
        "all_preds.to_csv(AUX_MODELS+'validation_v'+str(VER)+'.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXzxmfrS28t0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f3f00b7c-7704-41df-bb81-7680599c03c0"
      },
      "source": [
        "sales_tmp=sales.iloc[:,len(sales.columns)-28:]\n",
        "sales_tmp[sales_tmp>=1]=1\n",
        "sales_tmp\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# FPR, TPR(, しきい値) を算出\n",
        "fpr, tpr, thresholds = metrics.roc_curve(np.array(sales_tmp).flatten(),np.array(all_preds).flatten(),pos_label=0)\n",
        "\n",
        "# ついでにAUCも\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "print(auc,thresholds)\n",
        "\n",
        "# ROC曲線をプロット\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\n",
        "plt.legend()\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.grid(True)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-bf1081c4872c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# FPR, TPR(, しきい値) を算出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ついでにAUCも\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \"\"\"\n\u001b[1;32m    770\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 771\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [853720, 884210]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkVPzJ9MDH9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eab03733-3203-4d6b-8449-8a28706675cd"
      },
      "source": [
        "#####confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "y_true=np.array(sales_tmp).flatten()\n",
        "y_pred=np.array(all_preds).flatten()\n",
        "cm=confusion_matrix(y_true, y_pred>0.3)\n",
        "tn, fp, fn, tp = cm.flatten()\n",
        "\n",
        "precision_score(y_true, y_pred>0.3,pos_label=0)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8841016521415075"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihuXTmEaZsOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eec1edc0-47e2-4718-d923-c3544fbe1030"
      },
      "source": [
        "np.arange(0.1,1.0, 0.05)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 ,\n",
              "       0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lVukGjxXFAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame({'thr':np.arange(0.1,0.6, 0.03)})\n",
        "df['zero_precision'] = df.apply(lambda row: precision_score(y_true, y_pred >= row[\"thr\"],pos_label=0), axis=1)\n",
        "df[\"tn\"]=df.apply(lambda row: confusion_matrix(y_true, y_pred >= row[\"thr\"]).flatten()[0], axis=1)\n",
        "df[\"fp\"]=df.apply(lambda row: confusion_matrix(y_true, y_pred >= row[\"thr\"]).flatten()[1], axis=1)\n",
        "df[\"fn\"]=df.apply(lambda row: confusion_matrix(y_true, y_pred >= row[\"thr\"]).flatten()[2], axis=1)\n",
        "df[\"tp\"]=df.apply(lambda row: confusion_matrix(y_true, y_pred >= row[\"thr\"]).flatten()[3], axis=1)\n",
        "df.to_csv(AUX_MODELS+\"confusion_matrix_v\"+str(VER)+\".csv\")\n",
        "#df['FPR'] = df.apply(lambda row: fpr_score(y_true, y_score >= row['score']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34H4HlBnlt2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "33374fe8-caed-410d-d862-13995b0dcbb8"
      },
      "source": [
        "df"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>thr</th>\n",
              "      <th>zero_precision</th>\n",
              "      <th>tn</th>\n",
              "      <th>fp</th>\n",
              "      <th>fn</th>\n",
              "      <th>tp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>480460</td>\n",
              "      <td>0</td>\n",
              "      <td>373260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>480460</td>\n",
              "      <td>0</td>\n",
              "      <td>373260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>480460</td>\n",
              "      <td>0</td>\n",
              "      <td>373260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.990120</td>\n",
              "      <td>1403</td>\n",
              "      <td>479057</td>\n",
              "      <td>14</td>\n",
              "      <td>373246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.961070</td>\n",
              "      <td>6937</td>\n",
              "      <td>473523</td>\n",
              "      <td>281</td>\n",
              "      <td>372979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.932462</td>\n",
              "      <td>42676</td>\n",
              "      <td>437784</td>\n",
              "      <td>3091</td>\n",
              "      <td>370169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.903628</td>\n",
              "      <td>96306</td>\n",
              "      <td>384154</td>\n",
              "      <td>10271</td>\n",
              "      <td>362989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.874047</td>\n",
              "      <td>155097</td>\n",
              "      <td>325363</td>\n",
              "      <td>22350</td>\n",
              "      <td>350910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.844878</td>\n",
              "      <td>217796</td>\n",
              "      <td>262664</td>\n",
              "      <td>39988</td>\n",
              "      <td>333272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.810441</td>\n",
              "      <td>283061</td>\n",
              "      <td>197399</td>\n",
              "      <td>66207</td>\n",
              "      <td>307053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.768812</td>\n",
              "      <td>347347</td>\n",
              "      <td>133113</td>\n",
              "      <td>104450</td>\n",
              "      <td>268810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.43</td>\n",
              "      <td>0.714476</td>\n",
              "      <td>404577</td>\n",
              "      <td>75883</td>\n",
              "      <td>161680</td>\n",
              "      <td>211580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.649697</td>\n",
              "      <td>445987</td>\n",
              "      <td>34473</td>\n",
              "      <td>240467</td>\n",
              "      <td>132793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.601699</td>\n",
              "      <td>469041</td>\n",
              "      <td>11419</td>\n",
              "      <td>310487</td>\n",
              "      <td>62773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.574099</td>\n",
              "      <td>477905</td>\n",
              "      <td>2555</td>\n",
              "      <td>354538</td>\n",
              "      <td>18722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.564772</td>\n",
              "      <td>480108</td>\n",
              "      <td>352</td>\n",
              "      <td>369983</td>\n",
              "      <td>3277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.563011</td>\n",
              "      <td>480428</td>\n",
              "      <td>32</td>\n",
              "      <td>372891</td>\n",
              "      <td>369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     thr  zero_precision      tn      fp      fn      tp\n",
              "0   0.10        0.000000       0  480460       0  373260\n",
              "1   0.13        0.000000       0  480460       0  373260\n",
              "2   0.16        0.000000       0  480460       0  373260\n",
              "3   0.19        0.990120    1403  479057      14  373246\n",
              "4   0.22        0.961070    6937  473523     281  372979\n",
              "5   0.25        0.932462   42676  437784    3091  370169\n",
              "6   0.28        0.903628   96306  384154   10271  362989\n",
              "7   0.31        0.874047  155097  325363   22350  350910\n",
              "8   0.34        0.844878  217796  262664   39988  333272\n",
              "9   0.37        0.810441  283061  197399   66207  307053\n",
              "10  0.40        0.768812  347347  133113  104450  268810\n",
              "11  0.43        0.714476  404577   75883  161680  211580\n",
              "12  0.46        0.649697  445987   34473  240467  132793\n",
              "13  0.49        0.601699  469041   11419  310487   62773\n",
              "14  0.52        0.574099  477905    2555  354538   18722\n",
              "15  0.55        0.564772  480108     352  369983    3277\n",
              "16  0.58        0.563011  480428      32  372891     369"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGU_xQhcpqsU",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZE0QJMyPvGB",
        "colab_type": "code",
        "outputId": "c448e800-dc4b-4e96-e503-affd3b90b530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########################### Predict\n",
        "#################################################################################\n",
        "\n",
        "# Create Dummy DataFrame to store predictions\n",
        "all_preds = pd.DataFrame()\n",
        "\n",
        "# Join back the Test dataset with \n",
        "# a small part of the training data \n",
        "# to make recursive features\n",
        "base_test = get_base_test()\n",
        "\n",
        "# Timer to measure predictions time \n",
        "main_time = time.time()\n",
        "\n",
        "# Loop over each prediction day\n",
        "# As rolling lags are the most timeconsuming\n",
        "# we will calculate it for whole day\n",
        "for PREDICT_DAY in range(1,29):    \n",
        "#for PREDICT_DAY in range(1,2):    \n",
        " \n",
        "    print('Predict | Day:', PREDICT_DAY,END_TRAIN+PREDICT_DAY+P_HORIZON)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Make temporary grid to calculate rolling lags\n",
        "    grid_df = base_test.copy()\n",
        "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "        \n",
        "    for store_id in STORES_IDS:\n",
        "        \n",
        "        # Read all our models and make predictions\n",
        "        # for each day/store pairs\n",
        "        model_path = AUX_MODELS + 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
        "        if USE_AUX:\n",
        "            model_path = model_path\n",
        "        \n",
        "        estimator = pickle.load(open(model_path, 'rb'))\n",
        "        \n",
        "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY+P_HORIZON)\n",
        "        store_mask = base_test['store_id']==store_id\n",
        "        \n",
        "        mask = (day_mask)&(store_mask)\n",
        "        #base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "        base_test.loc[mask,TARGET]=estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "    # Make good column naming and add \n",
        "    # to all_preds DataFrame\n",
        "    temp_df = base_test[day_mask][['id',TARGET]]\n",
        "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "    if 'id' in list(all_preds):\n",
        "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "    else:\n",
        "        all_preds = temp_df.copy()\n",
        "        \n",
        "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    del temp_df\n",
        "    \n",
        "all_preds = all_preds.reset_index(drop=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict | Day: 1 1914\n",
            "##########  0.63 min round |  0.63 min total |  11204.35 day sales |\n",
            "Predict | Day: 2 1915\n",
            "##########  0.63 min round |  1.27 min total |  11342.18 day sales |\n",
            "Predict | Day: 3 1916\n",
            "##########  0.60 min round |  1.87 min total |  11410.05 day sales |\n",
            "Predict | Day: 4 1917\n",
            "##########  0.61 min round |  2.48 min total |  11325.73 day sales |\n",
            "Predict | Day: 5 1918\n",
            "##########  0.62 min round |  3.10 min total |  11602.93 day sales |\n",
            "Predict | Day: 6 1919\n",
            "##########  0.62 min round |  3.71 min total |  12374.80 day sales |\n",
            "Predict | Day: 7 1920\n",
            "##########  0.60 min round |  4.31 min total |  12498.00 day sales |\n",
            "Predict | Day: 8 1921\n",
            "##########  0.61 min round |  4.92 min total |  11563.99 day sales |\n",
            "Predict | Day: 9 1922\n",
            "##########  0.61 min round |  5.53 min total |  11570.04 day sales |\n",
            "Predict | Day: 10 1923\n",
            "##########  0.61 min round |  6.14 min total |  11438.47 day sales |\n",
            "Predict | Day: 11 1924\n",
            "##########  0.61 min round |  6.75 min total |  11491.84 day sales |\n",
            "Predict | Day: 12 1925\n",
            "##########  0.62 min round |  7.36 min total |  11696.73 day sales |\n",
            "Predict | Day: 13 1926\n",
            "##########  0.61 min round |  7.97 min total |  12382.45 day sales |\n",
            "Predict | Day: 14 1927\n",
            "##########  0.61 min round |  8.58 min total |  12319.56 day sales |\n",
            "Predict | Day: 15 1928\n",
            "##########  0.61 min round |  9.19 min total |  11413.18 day sales |\n",
            "Predict | Day: 16 1929\n",
            "##########  0.61 min round |  9.81 min total |  11292.96 day sales |\n",
            "Predict | Day: 17 1930\n",
            "##########  0.60 min round |  10.41 min total |  11329.93 day sales |\n",
            "Predict | Day: 18 1931\n",
            "##########  0.61 min round |  11.02 min total |  11315.93 day sales |\n",
            "Predict | Day: 19 1932\n",
            "##########  0.61 min round |  11.62 min total |  11493.36 day sales |\n",
            "Predict | Day: 20 1933\n",
            "##########  0.61 min round |  12.23 min total |  12330.16 day sales |\n",
            "Predict | Day: 21 1934\n",
            "##########  0.62 min round |  12.85 min total |  12324.85 day sales |\n",
            "Predict | Day: 22 1935\n",
            "##########  0.62 min round |  13.47 min total |  11230.41 day sales |\n",
            "Predict | Day: 23 1936\n",
            "##########  0.60 min round |  14.07 min total |  11146.38 day sales |\n",
            "Predict | Day: 24 1937\n",
            "##########  0.61 min round |  14.68 min total |  11119.14 day sales |\n",
            "Predict | Day: 25 1938\n",
            "##########  0.61 min round |  15.29 min total |  11103.34 day sales |\n",
            "Predict | Day: 26 1939\n",
            "##########  0.61 min round |  15.90 min total |  11333.62 day sales |\n",
            "Predict | Day: 27 1940\n",
            "##########  0.62 min round |  16.51 min total |  12191.70 day sales |\n",
            "Predict | Day: 28 1941\n",
            "##########  0.62 min round |  17.13 min total |  12173.19 day sales |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym40qYzLPvGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Export\n",
        "#################################################################################\n",
        "# Reading competition sample submission and\n",
        "# merging our predictions\n",
        "# As we have predictions only for \"_validation\" data\n",
        "# we need to do fillna() for \"_evaluation\" items\n",
        "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
        "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
        "submission.to_csv(AUX_MODELS+'submission_v'+str(VER)+'.csv', index=False)\n",
        "submission.to_csv('submission_v'+str(VER)+'.csv', index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU0HId86PvGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary\n",
        "\n",
        "# Of course here is no magic at all.\n",
        "# No \"Novel\" features and no brilliant ideas.\n",
        "# We just carefully joined all\n",
        "# our previous fe work and created a model.\n",
        "\n",
        "# Also!\n",
        "# In my opinion this strategy is a \"dead end\".\n",
        "# Overfits a lot LB and with 1 final submission \n",
        "# you have no option to risk.\n",
        "\n",
        "\n",
        "# Improvement should come from:\n",
        "# Loss function\n",
        "# Data representation\n",
        "# Stable CV\n",
        "# Good features reduction strategy\n",
        "# Predictions stabilization with NN\n",
        "# Trend prediction\n",
        "# Real zero sales detection/classification\n",
        "\n",
        "\n",
        "# Good kernels references \n",
        "## (the order is random and the list is not complete):\n",
        "# https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
        "# https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
        "# https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
        "# https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n",
        "# https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n",
        "# https://www.kaggle.com/yassinealouini/seq2seq\n",
        "# https://www.kaggle.com/kailex/m5-forecaster-v2\n",
        "# https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n",
        "\n",
        "\n",
        "# Features were created in these kernels:\n",
        "## \n",
        "# Mean encodings and PCA options\n",
        "# https://www.kaggle.com/kyakovlev/m5-custom-features\n",
        "##\n",
        "# Lags and rolling lags\n",
        "# https://www.kaggle.com/kyakovlev/m5-lags-features\n",
        "##\n",
        "# Base Grid and base features (calendar/price/etc)\n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "\n",
        "\n",
        "# Personal request\n",
        "# Please don't upvote any ensemble and copypaste kernels\n",
        "## The worst case is ensemble without any analyse.\n",
        "## The best choice - just ignore it.\n",
        "## I would like to see more kernels with interesting and original approaches.\n",
        "## Don't feed copypasters with upvotes.\n",
        "\n",
        "## It doesn't mean that you should not fork and improve others kernels\n",
        "## but I would like to see params and code tuning based on some CV and analyse\n",
        "## and not only on LB probing.\n",
        "## Small changes could be shared in comments and authors can improve their kernel.\n",
        "\n",
        "## Feel free to criticize this kernel as my knowlege is very limited\n",
        "## and I can be wrong in code and descriptions. \n",
        "## Thank you."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDEDPNgNlsV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imp=pd.DataFrame({'feature': features_columns,\n",
        "\n",
        " 'importance':estimator.feature_importance()}).sort_values('importance',\n",
        "\n",
        " ascending=False)\n",
        "imp.to_csv(AUX_MODELS+'importance_v'+str(VER)+'.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa7kjHFYmJDi",
        "colab_type": "code",
        "outputId": "9e579ff4-a75f-4e7b-8f80-a8f28f0c09a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "for store_id in STORES_IDS:\n",
        "  model_path = AUX_MODELS + 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
        "  estimator = pickle.load(open(model_path, 'rb'))\n",
        "  \n",
        "  if store_id==\"CA_1\":\n",
        "    imp=pd.DataFrame({'feature': features_columns,store_id:estimator.feature_importance()})\n",
        "  else:\n",
        "    tmp=pd.DataFrame({'feature': features_columns,store_id:estimator.feature_importance()})\n",
        "    imp=pd.merge(imp,tmp,how=\"left\",on=\"feature\")\n",
        "\n",
        "imp.index=imp[\"feature\"]\n",
        "imp=imp.drop(\"feature\",axis=1)\n",
        "imp[\"mean\"]=imp.mean(axis=1)\n",
        "imp.to_csv(AUX_MODELS+\"importance_v\"+str(VER)+\".csv\")\n",
        "\n",
        "\n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature\n",
              "item_id                     619640\n",
              "dept_id                      93551\n",
              "release                      70404\n",
              "sell_price                   78391\n",
              "price_max                    72620\n",
              "                             ...  \n",
              "rolling_holiday_mean_7       60800\n",
              "rolling_holiday_mean_14      73897\n",
              "rolling_holiday_mean_30      93357\n",
              "rolling_holiday_mean_60     121183\n",
              "rolling_holiday_mean_180    155797\n",
              "Length: 63, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eMqhKjFTtbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}