{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "m5-three-shades-of-dark-darker-magic.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipejun-ai/m5-accuracy/blob/master/m5_three_shades_of_dark_darker_magic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "VY1mXqL-PvFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "\n",
        "# custom imports\n",
        "from multiprocessing import Pool        # Multiprocess Runs\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da5VwfTzQYpY",
        "colab_type": "code",
        "outputId": "ec2bf494-47a3-49f7-b6de-0cac98bafc4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ0uXtDQQhAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIRPATH=\"/content/gdrive/My Drive/kaggle/\"\n",
        "#DIRPATH=\"C:/Users/peiju/Documents/Study/kaggle/m5-forecasting-accuracy/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm1pyYhzPvFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helpers\n",
        "#################################################################################\n",
        "## Seeder\n",
        "# :seed to make all processes deterministic     # type: int\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    \n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "413KNvT2PvFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helper to load data by store ID\n",
        "#################################################################################\n",
        "# Read data\n",
        "def get_data_by_store(store):\n",
        "    \n",
        "    # Read and contact basic feature\n",
        "    df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)\n",
        "    \n",
        "    # Leave only relevant store\n",
        "    df = df[df['store_id']==store]\n",
        "\n",
        "    # With memory limits we have to read \n",
        "    # lags and mean encoding features\n",
        "    # separately and drop items that we don't need.\n",
        "    # As our Features Grids are aligned \n",
        "    # we can use index to keep only necessary rows\n",
        "    # Alignment is good for us as concat uses less memory than merge.\n",
        "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
        "    df2 = df2[df2.index.isin(df.index)]\n",
        "    \n",
        "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
        "    df3 = df3[df3.index.isin(df.index)]\n",
        "\n",
        "\n",
        "    df4 = pd.read_pickle(SNAP).iloc[:,2:]\n",
        "    df4 = df4[df4.index.isin(df.index)]\n",
        "    \n",
        "    df = pd.concat([df, df2], axis=1)\n",
        "    del df2 # to not reach memory limit \n",
        "    \n",
        "    df = pd.concat([df, df3], axis=1)\n",
        "    del df3 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df4], axis=1)\n",
        "    del df4 # to not reach memory limit \n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "    # Create features list\n",
        "    features = [col for col in list(df) if col not in remove_features]\n",
        "    df = df[['id','d',TARGET]+features]\n",
        "    \n",
        "    # Skipping first n rows\n",
        "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
        "    \n",
        "    return df, features\n",
        "\n",
        "# Recombine Test set after training\n",
        "def get_base_test():\n",
        "    base_test = pd.DataFrame()\n",
        "\n",
        "    for store_id in STORES_IDS:\n",
        "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
        "        temp_df['store_id'] = store_id\n",
        "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
        "    \n",
        "    return base_test\n",
        "\n",
        "\n",
        "########################### Helper to make dynamic rolling lags\n",
        "#################################################################################\n",
        "def make_lag(LAG_DAY):\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "\n",
        "def make_lag_roll(LAG_DAY):\n",
        "    shift_day = LAG_DAY[0]\n",
        "    roll_wind = LAG_DAY[1]\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
        "    return lag_df[[col_name]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Boc3vLWX48",
        "colab_type": "code",
        "outputId": "b8fa9964-f641-4a45-8e85-4c26ee8124d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "    df4 = pd.read_pickle(FINAL_SALES).iloc[:,2:]\n",
        "    df4.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>last_sale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   last_sale\n",
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NDcdsu0PvFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Model params\n",
        "#################################################################################\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.1,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 1400,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': -1,\n",
        "                } \n",
        "\n",
        "# Let's look closer on params\n",
        "\n",
        "## 'boosting_type': 'gbdt'\n",
        "# we have 'goss' option for faster training\n",
        "# but it normally leads to underfit.\n",
        "# Also there is good 'dart' mode\n",
        "# but it takes forever to train\n",
        "# and model performance depends \n",
        "# a lot on random factor \n",
        "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
        "\n",
        "## 'objective': 'tweedie'\n",
        "# Tweedie Gradient Boosting for Extremely\n",
        "# Unbalanced Zero-inflated Data\n",
        "# https://arxiv.org/pdf/1811.10192.pdf\n",
        "# and many more articles about tweediie\n",
        "#\n",
        "# Strange (for me) but Tweedie is close in results\n",
        "# to my own ugly loss.\n",
        "# My advice here - make OWN LOSS function\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
        "# I think many of you already using it (after poisson kernel appeared) \n",
        "# (kagglers are very good with \"params\" testing and tuning).\n",
        "# Try to figure out why Tweedie works.\n",
        "# probably it will show you new features options\n",
        "# or data transformation (Target transformation?).\n",
        "\n",
        "## 'tweedie_variance_power': 1.1\n",
        "# default = 1.5\n",
        "# set this closer to 2 to shift towards a Gamma distribution\n",
        "# set this closer to 1 to shift towards a Poisson distribution\n",
        "# my CV shows 1.1 is optimal \n",
        "# but you can make your own choice\n",
        "\n",
        "## 'metric': 'rmse'\n",
        "# Doesn't mean anything to us\n",
        "# as competition metric is different\n",
        "# and we don't use early stoppings here.\n",
        "# So rmse serves just for general \n",
        "# model performance overview.\n",
        "# Also we use \"fake\" validation set\n",
        "# (as it makes part of the training set)\n",
        "# so even general rmse score doesn't mean anything))\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
        "\n",
        "## 'subsample': 0.5\n",
        "# Serves to fight with overfit\n",
        "# this will randomly select part of data without resampling\n",
        "# Chosen by CV (my CV can be wrong!)\n",
        "# Next kernel will be about CV\n",
        "\n",
        "##'subsample_freq': 1\n",
        "# frequency for bagging\n",
        "# default value - seems ok\n",
        "\n",
        "## 'learning_rate': 0.03\n",
        "# Chosen by CV\n",
        "# Smaller - longer training\n",
        "# but there is an option to stop \n",
        "# in \"local minimum\"\n",
        "# Bigger - faster training\n",
        "# but there is a chance to\n",
        "# not find \"global minimum\" minimum\n",
        "\n",
        "## 'num_leaves': 2**11-1\n",
        "## 'min_data_in_leaf': 2**12-1\n",
        "# Force model to use more features\n",
        "# We need it to reduce \"recursive\"\n",
        "# error impact.\n",
        "# Also it leads to overfit\n",
        "# that's why we use small \n",
        "# 'max_bin': 100\n",
        "\n",
        "## l1, l2 regularizations\n",
        "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
        "# Good tiny explanation\n",
        "# l2 can work with bigger num_leaves\n",
        "# but my CV doesn't show boost\n",
        "                    \n",
        "## 'n_estimators': 1400\n",
        "# CV shows that there should be\n",
        "# different values for each state/store.\n",
        "# Current value was chosen \n",
        "# for general purpose.\n",
        "# As we don't use any early stopings\n",
        "# careful to not overfit Public LB.\n",
        "\n",
        "##'feature_fraction': 0.5\n",
        "# LightGBM will randomly select \n",
        "# part of features on each iteration (tree).\n",
        "# We have maaaany features\n",
        "# and many of them are \"duplicates\"\n",
        "# and many just \"noise\"\n",
        "# good values here - 0.5-0.7 (by CV)\n",
        "\n",
        "## 'boost_from_average': False\n",
        "# There is some \"problem\"\n",
        "# to code boost_from_average for \n",
        "# custom loss\n",
        "# 'True' makes training faster\n",
        "# BUT carefull use it\n",
        "# https://github.com/microsoft/LightGBM/issues/1514\n",
        "# not our case but good to know cons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvwIzPcOPvFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEfqHZHpPvFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Vars\n",
        "#################################################################################\n",
        "VER = 3                          # Our model version\n",
        "SEED = 42                        # We want all things\n",
        "seed_everything(SEED)            # to be as deterministic \n",
        "lgb_params['seed'] = SEED        # as possible\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "\n",
        "#LIMITS and const\n",
        "TARGET      = 'sales'            # Our target\n",
        "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
        "END_TRAIN   = 1913               # End day of our train set\n",
        "P_HORIZON   = 28                 # Prediction horizon\n",
        "USE_AUX     = False            # Use or not pretrained models\n",
        "\n",
        "#FEATURES to remove\n",
        "## These features lead to overfit\n",
        "## or values not present in test set\n",
        "remove_features = ['id','state_id','store_id',\n",
        "                   'date','wm_yr_wk','d','snap_CA','snap_TX','snap_WI',TARGET]\n",
        "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
        "                   'enc_dept_id_mean','enc_dept_id_std',\n",
        "                   'enc_item_id_mean','enc_item_id_std'] \n",
        "\n",
        "#PATHS for Features\n",
        "ORIGINAL = DIRPATH\n",
        "BASE     = DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'\n",
        "PRICE    = DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl'\n",
        "CALENDAR = DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl'\n",
        "LAGS     = DIRPATH+'/output/m5-lags-features/lags_df_28.pkl'\n",
        "MEAN_ENC = DIRPATH+'/output/m5-custom-features/mean_encoding_df.pkl'\n",
        "FINAL_SALES = DIRPATH+'/output/m5-custom-features/finalsales_df.pkl'\n",
        "SNAP      =  DIRPATH+'/output/m5-custom-features/snap_df.pkl'\n",
        "# AUX(pretrained) Models paths\n",
        "AUX_MODELS = DIRPATH+'/output/m5-three-shades-of-dark-darker-magic/'\n",
        "\n",
        "\n",
        "#STORES ids\n",
        "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
        "STORES_IDS = list(STORES_IDS.unique())\n",
        "\n",
        "\n",
        "#SPLITS for lags creation\n",
        "SHIFT_DAY  = 28\n",
        "N_LAGS     = 15\n",
        "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
        "ROLS_SPLIT = []\n",
        "for i in [1,7,14]:\n",
        "    for j in [7,14,30,60]:\n",
        "        ROLS_SPLIT.append([i,j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61rzWwz3PvFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Aux Models\n",
        "# If you don't want to wait hours and hours\n",
        "# to have result you can train each store \n",
        "# in separate kernel and then just join result.\n",
        "\n",
        "# If we want to use pretrained models we can \n",
        "## skip training \n",
        "## (in our case do dummy training\n",
        "##  to show that we are good with memory\n",
        "##  and you can safely use this (all kernel) code)\n",
        "if USE_AUX:\n",
        "    lgb_params['n_estimators'] = 2\n",
        "    \n",
        "# Here is some 'logs' that can compare\n",
        "#Train CA_1\n",
        "#[100]\tvalid_0's rmse: 2.02289\n",
        "#[200]\tvalid_0's rmse: 2.0017\n",
        "#[300]\tvalid_0's rmse: 1.99239\n",
        "#[400]\tvalid_0's rmse: 1.98471\n",
        "#[500]\tvalid_0's rmse: 1.97923\n",
        "#[600]\tvalid_0's rmse: 1.97284\n",
        "#[700]\tvalid_0's rmse: 1.96763\n",
        "#[800]\tvalid_0's rmse: 1.9624\n",
        "#[900]\tvalid_0's rmse: 1.95673\n",
        "#[1000]\tvalid_0's rmse: 1.95201\n",
        "#[1100]\tvalid_0's rmse: 1.9476\n",
        "#[1200]\tvalid_0's rmse: 1.9434\n",
        "#[1300]\tvalid_0's rmse: 1.9392\n",
        "#[1400]\tvalid_0's rmse: 1.93446\n",
        "\n",
        "#Train CA_2\n",
        "#[100]\tvalid_0's rmse: 1.88949\n",
        "#[200]\tvalid_0's rmse: 1.84767\n",
        "#[300]\tvalid_0's rmse: 1.83653\n",
        "#[400]\tvalid_0's rmse: 1.82909\n",
        "#[500]\tvalid_0's rmse: 1.82265\n",
        "#[600]\tvalid_0's rmse: 1.81725\n",
        "#[700]\tvalid_0's rmse: 1.81252\n",
        "#[800]\tvalid_0's rmse: 1.80736\n",
        "#[900]\tvalid_0's rmse: 1.80242\n",
        "#[1000]\tvalid_0's rmse: 1.79821\n",
        "#[1100]\tvalid_0's rmse: 1.794\n",
        "#[1200]\tvalid_0's rmse: 1.78973\n",
        "#[1300]\tvalid_0's rmse: 1.78552\n",
        "#[1400]\tvalid_0's rmse: 1.78158"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJJPOV1IPvF3",
        "colab_type": "code",
        "outputId": "b2f527ab-827a-4e0f-bc49-4a2c7d35145e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########################### Train Models\n",
        "#################################################################################\n",
        "for store_id in STORES_IDS:\n",
        "    print('Train', store_id)\n",
        "    \n",
        "    # Get grid for current store\n",
        "    grid_df, features_columns = get_data_by_store(store_id)\n",
        "    \n",
        "    # Masks for \n",
        "    # Train (All data less than 1913)\n",
        "    # \"Validation\" (Last 28 days - not real validatio set)\n",
        "    # Test (All data greater than 1913 day, \n",
        "    #       with some gap for recursive features)\n",
        "    train_mask = grid_df['d']<=END_TRAIN\n",
        "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
        "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
        "    \n",
        "    # Apply masks and save lgb dataset as bin\n",
        "    # to reduce memory spikes during dtype convertations\n",
        "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
        "    # \"To avoid any conversions, you should always use np.float32\"\n",
        "    # or save to bin before start training\n",
        "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
        "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
        "                       label=grid_df[train_mask][TARGET])\n",
        "    train_data.save_binary('train_data.bin')\n",
        "    train_data = lgb.Dataset('train_data.bin')\n",
        "    \n",
        "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
        "                       label=grid_df[valid_mask][TARGET])\n",
        "    \n",
        "    # Saving part of the dataset for later predictions\n",
        "    # Removing features that we need to calculate recursively \n",
        "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
        "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
        "    grid_df = grid_df[keep_cols]\n",
        "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
        "    del grid_df\n",
        "    \n",
        "    # Launch seeder again to make lgb training 100% deterministic\n",
        "    # with each \"code line\" np.random \"evolves\" \n",
        "    # so we need (may want) to \"reset\" it\n",
        "    seed_everything(SEED)\n",
        "    estimator = lgb.train(lgb_params,\n",
        "                          train_data,\n",
        "                          valid_sets = [valid_data],\n",
        "                          verbose_eval = 100,\n",
        "                          )\n",
        "    \n",
        "    # Save model - it's not real '.bin' but a pickle file\n",
        "    # estimator = lgb.Booster(model_file='model.txt')\n",
        "    # can only predict with the best iteration (or the saving iteration)\n",
        "    # pickle.dump gives us more flexibility\n",
        "    # like estimator.predict(TEST, num_iteration=100)\n",
        "    # num_iteration - number of iteration want to predict with, \n",
        "    # NULL or <= 0 means use best iteration\n",
        "    model_name = AUX_MODELS+'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
        "    pickle.dump(estimator, open(model_name, 'wb'))\n",
        "\n",
        "    # Remove temporary files and objects \n",
        "    # to free some hdd space and ram memory\n",
        "    !rm train_data.bin\n",
        "    del train_data, valid_data, estimator\n",
        "    gc.collect()\n",
        "    \n",
        "    # \"Keep\" models features for predictions\n",
        "    MODEL_FEATURES = features_columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train CA_1\n",
            "[100]\tvalid_0's rmse: 2.02066\n",
            "[200]\tvalid_0's rmse: 2.00049\n",
            "[300]\tvalid_0's rmse: 1.99121\n",
            "[400]\tvalid_0's rmse: 1.9846\n",
            "[500]\tvalid_0's rmse: 1.97943\n",
            "[600]\tvalid_0's rmse: 1.97331\n",
            "[700]\tvalid_0's rmse: 1.96812\n",
            "[800]\tvalid_0's rmse: 1.96224\n",
            "[900]\tvalid_0's rmse: 1.95665\n",
            "[1000]\tvalid_0's rmse: 1.95113\n",
            "[1100]\tvalid_0's rmse: 1.94654\n",
            "[1200]\tvalid_0's rmse: 1.94217\n",
            "[1300]\tvalid_0's rmse: 1.93804\n",
            "[1400]\tvalid_0's rmse: 1.93348\n",
            "Train CA_2\n",
            "[100]\tvalid_0's rmse: 1.88931\n",
            "[200]\tvalid_0's rmse: 1.84853\n",
            "[300]\tvalid_0's rmse: 1.83684\n",
            "[400]\tvalid_0's rmse: 1.82902\n",
            "[500]\tvalid_0's rmse: 1.82267\n",
            "[600]\tvalid_0's rmse: 1.81684\n",
            "[700]\tvalid_0's rmse: 1.81193\n",
            "[800]\tvalid_0's rmse: 1.80739\n",
            "[900]\tvalid_0's rmse: 1.8023\n",
            "[1000]\tvalid_0's rmse: 1.79771\n",
            "[1100]\tvalid_0's rmse: 1.79337\n",
            "[1200]\tvalid_0's rmse: 1.78944\n",
            "[1300]\tvalid_0's rmse: 1.78531\n",
            "[1400]\tvalid_0's rmse: 1.78163\n",
            "Train CA_3\n",
            "[100]\tvalid_0's rmse: 2.50637\n",
            "[200]\tvalid_0's rmse: 2.45967\n",
            "[300]\tvalid_0's rmse: 2.43996\n",
            "[400]\tvalid_0's rmse: 2.42697\n",
            "[500]\tvalid_0's rmse: 2.4181\n",
            "[600]\tvalid_0's rmse: 2.41079\n",
            "[700]\tvalid_0's rmse: 2.4046\n",
            "[800]\tvalid_0's rmse: 2.39834\n",
            "[900]\tvalid_0's rmse: 2.3925\n",
            "[1000]\tvalid_0's rmse: 2.38675\n",
            "[1100]\tvalid_0's rmse: 2.38136\n",
            "[1200]\tvalid_0's rmse: 2.3772\n",
            "[1300]\tvalid_0's rmse: 2.37335\n",
            "[1400]\tvalid_0's rmse: 2.36932\n",
            "Train CA_4\n",
            "[100]\tvalid_0's rmse: 1.33157\n",
            "[200]\tvalid_0's rmse: 1.32507\n",
            "[300]\tvalid_0's rmse: 1.31997\n",
            "[400]\tvalid_0's rmse: 1.31615\n",
            "[500]\tvalid_0's rmse: 1.31317\n",
            "[600]\tvalid_0's rmse: 1.30987\n",
            "[700]\tvalid_0's rmse: 1.307\n",
            "[800]\tvalid_0's rmse: 1.30413\n",
            "[900]\tvalid_0's rmse: 1.30138\n",
            "[1000]\tvalid_0's rmse: 1.29903\n",
            "[1100]\tvalid_0's rmse: 1.29648\n",
            "[1200]\tvalid_0's rmse: 1.29408\n",
            "[1300]\tvalid_0's rmse: 1.29154\n",
            "[1400]\tvalid_0's rmse: 1.28927\n",
            "Train TX_1\n",
            "[100]\tvalid_0's rmse: 1.60876\n",
            "[200]\tvalid_0's rmse: 1.59009\n",
            "[300]\tvalid_0's rmse: 1.58295\n",
            "[400]\tvalid_0's rmse: 1.57759\n",
            "[500]\tvalid_0's rmse: 1.57274\n",
            "[600]\tvalid_0's rmse: 1.56872\n",
            "[700]\tvalid_0's rmse: 1.56496\n",
            "[800]\tvalid_0's rmse: 1.5612\n",
            "[900]\tvalid_0's rmse: 1.55699\n",
            "[1000]\tvalid_0's rmse: 1.55291\n",
            "[1100]\tvalid_0's rmse: 1.55011\n",
            "[1200]\tvalid_0's rmse: 1.54661\n",
            "[1300]\tvalid_0's rmse: 1.54384\n",
            "[1400]\tvalid_0's rmse: 1.5403\n",
            "Train TX_2\n",
            "[100]\tvalid_0's rmse: 1.71905\n",
            "[200]\tvalid_0's rmse: 1.69787\n",
            "[300]\tvalid_0's rmse: 1.68426\n",
            "[400]\tvalid_0's rmse: 1.67774\n",
            "[500]\tvalid_0's rmse: 1.67269\n",
            "[600]\tvalid_0's rmse: 1.66834\n",
            "[700]\tvalid_0's rmse: 1.66503\n",
            "[800]\tvalid_0's rmse: 1.66022\n",
            "[900]\tvalid_0's rmse: 1.65722\n",
            "[1000]\tvalid_0's rmse: 1.6545\n",
            "[1100]\tvalid_0's rmse: 1.65089\n",
            "[1200]\tvalid_0's rmse: 1.64792\n",
            "[1300]\tvalid_0's rmse: 1.6449\n",
            "[1400]\tvalid_0's rmse: 1.64164\n",
            "Train TX_3\n",
            "[100]\tvalid_0's rmse: 1.68927\n",
            "[200]\tvalid_0's rmse: 1.67757\n",
            "[300]\tvalid_0's rmse: 1.67112\n",
            "[400]\tvalid_0's rmse: 1.66558\n",
            "[500]\tvalid_0's rmse: 1.66088\n",
            "[600]\tvalid_0's rmse: 1.65605\n",
            "[700]\tvalid_0's rmse: 1.65088\n",
            "[800]\tvalid_0's rmse: 1.64615\n",
            "[900]\tvalid_0's rmse: 1.64208\n",
            "[1000]\tvalid_0's rmse: 1.63759\n",
            "[1100]\tvalid_0's rmse: 1.63456\n",
            "[1200]\tvalid_0's rmse: 1.63082\n",
            "[1300]\tvalid_0's rmse: 1.62639\n",
            "[1400]\tvalid_0's rmse: 1.62319\n",
            "Train WI_1\n",
            "[100]\tvalid_0's rmse: 1.5963\n",
            "[200]\tvalid_0's rmse: 1.57743\n",
            "[300]\tvalid_0's rmse: 1.56966\n",
            "[400]\tvalid_0's rmse: 1.56392\n",
            "[500]\tvalid_0's rmse: 1.55993\n",
            "[600]\tvalid_0's rmse: 1.55546\n",
            "[700]\tvalid_0's rmse: 1.55158\n",
            "[800]\tvalid_0's rmse: 1.54741\n",
            "[900]\tvalid_0's rmse: 1.5437\n",
            "[1000]\tvalid_0's rmse: 1.54032\n",
            "[1100]\tvalid_0's rmse: 1.53641\n",
            "[1200]\tvalid_0's rmse: 1.53327\n",
            "[1300]\tvalid_0's rmse: 1.53021\n",
            "[1400]\tvalid_0's rmse: 1.5271\n",
            "Train WI_2\n",
            "[100]\tvalid_0's rmse: 2.69132\n",
            "[200]\tvalid_0's rmse: 2.58799\n",
            "[300]\tvalid_0's rmse: 2.55486\n",
            "[400]\tvalid_0's rmse: 2.53106\n",
            "[500]\tvalid_0's rmse: 2.51367\n",
            "[600]\tvalid_0's rmse: 2.49971\n",
            "[700]\tvalid_0's rmse: 2.48508\n",
            "[800]\tvalid_0's rmse: 2.47378\n",
            "[900]\tvalid_0's rmse: 2.46133\n",
            "[1000]\tvalid_0's rmse: 2.44977\n",
            "[1100]\tvalid_0's rmse: 2.43871\n",
            "[1200]\tvalid_0's rmse: 2.4296\n",
            "[1300]\tvalid_0's rmse: 2.41979\n",
            "[1400]\tvalid_0's rmse: 2.41216\n",
            "Train WI_3\n",
            "[100]\tvalid_0's rmse: 1.91712\n",
            "[200]\tvalid_0's rmse: 1.84656\n",
            "[300]\tvalid_0's rmse: 1.82614\n",
            "[400]\tvalid_0's rmse: 1.81489\n",
            "[500]\tvalid_0's rmse: 1.80644\n",
            "[600]\tvalid_0's rmse: 1.79891\n",
            "[700]\tvalid_0's rmse: 1.79309\n",
            "[800]\tvalid_0's rmse: 1.78698\n",
            "[900]\tvalid_0's rmse: 1.78097\n",
            "[1000]\tvalid_0's rmse: 1.77472\n",
            "[1100]\tvalid_0's rmse: 1.77064\n",
            "[1200]\tvalid_0's rmse: 1.76604\n",
            "[1300]\tvalid_0's rmse: 1.76111\n",
            "[1400]\tvalid_0's rmse: 1.75569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyY4qfmmPvF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZE0QJMyPvGB",
        "colab_type": "code",
        "outputId": "6d6f90ff-b961-4cdd-e0d5-33ee3d280673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########################### Predict\n",
        "#################################################################################\n",
        "\n",
        "# Create Dummy DataFrame to store predictions\n",
        "all_preds = pd.DataFrame()\n",
        "\n",
        "# Join back the Test dataset with \n",
        "# a small part of the training data \n",
        "# to make recursive features\n",
        "base_test = get_base_test()\n",
        "\n",
        "# Timer to measure predictions time \n",
        "main_time = time.time()\n",
        "\n",
        "# Loop over each prediction day\n",
        "# As rolling lags are the most timeconsuming\n",
        "# we will calculate it for whole day\n",
        "for PREDICT_DAY in range(1,29):    \n",
        "    print('Predict | Day:', PREDICT_DAY)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Make temporary grid to calculate rolling lags\n",
        "    grid_df = base_test.copy()\n",
        "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "        \n",
        "    for store_id in STORES_IDS:\n",
        "        \n",
        "        # Read all our models and make predictions\n",
        "        # for each day/store pairs\n",
        "        model_path = AUX_MODELS + 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
        "        if USE_AUX:\n",
        "            model_path = AUX_MODELS + model_path\n",
        "        \n",
        "        estimator = pickle.load(open(model_path, 'rb'))\n",
        "        \n",
        "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
        "        store_mask = base_test['store_id']==store_id\n",
        "        \n",
        "        mask = (day_mask)&(store_mask)\n",
        "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "    \n",
        "    # Make good column naming and add \n",
        "    # to all_preds DataFrame\n",
        "    temp_df = base_test[day_mask][['id',TARGET]]\n",
        "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "    if 'id' in list(all_preds):\n",
        "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "    else:\n",
        "        all_preds = temp_df.copy()\n",
        "        \n",
        "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    del temp_df\n",
        "    \n",
        "all_preds = all_preds.reset_index(drop=True)\n",
        "all_preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict | Day: 1\n",
            "##########  0.88 min round |  0.88 min total |  37293.41 day sales |\n",
            "Predict | Day: 2\n",
            "##########  0.67 min round |  1.56 min total |  35272.58 day sales |\n",
            "Predict | Day: 3\n",
            "##########  0.65 min round |  2.21 min total |  34608.14 day sales |\n",
            "Predict | Day: 4\n",
            "##########  0.66 min round |  2.86 min total |  35228.92 day sales |\n",
            "Predict | Day: 5\n",
            "##########  0.65 min round |  3.52 min total |  41585.00 day sales |\n",
            "Predict | Day: 6\n",
            "##########  0.66 min round |  4.18 min total |  51090.26 day sales |\n",
            "Predict | Day: 7\n",
            "##########  0.65 min round |  4.83 min total |  53641.78 day sales |\n",
            "Predict | Day: 8\n",
            "##########  0.65 min round |  5.48 min total |  44110.90 day sales |\n",
            "Predict | Day: 9\n",
            "##########  0.67 min round |  6.15 min total |  44371.58 day sales |\n",
            "Predict | Day: 10\n",
            "##########  0.65 min round |  6.80 min total |  38834.70 day sales |\n",
            "Predict | Day: 11\n",
            "##########  0.65 min round |  7.46 min total |  40880.72 day sales |\n",
            "Predict | Day: 12\n",
            "##########  0.65 min round |  8.11 min total |  45866.69 day sales |\n",
            "Predict | Day: 13\n",
            "##########  0.65 min round |  8.76 min total |  54110.92 day sales |\n",
            "Predict | Day: 14\n",
            "##########  0.67 min round |  9.43 min total |  46435.28 day sales |\n",
            "Predict | Day: 15\n",
            "##########  0.67 min round |  10.09 min total |  44835.49 day sales |\n",
            "Predict | Day: 16\n",
            "##########  0.66 min round |  10.75 min total |  39556.76 day sales |\n",
            "Predict | Day: 17\n",
            "##########  0.66 min round |  11.42 min total |  40340.20 day sales |\n",
            "Predict | Day: 18\n",
            "##########  0.65 min round |  12.07 min total |  40937.67 day sales |\n",
            "Predict | Day: 19\n",
            "##########  0.65 min round |  12.72 min total |  43989.62 day sales |\n",
            "Predict | Day: 20\n",
            "##########  0.65 min round |  13.38 min total |  53918.10 day sales |\n",
            "Predict | Day: 21\n",
            "##########  0.65 min round |  14.03 min total |  55911.84 day sales |\n",
            "Predict | Day: 22\n",
            "##########  0.66 min round |  14.69 min total |  42003.45 day sales |\n",
            "Predict | Day: 23\n",
            "##########  0.65 min round |  15.34 min total |  38060.56 day sales |\n",
            "Predict | Day: 24\n",
            "##########  0.65 min round |  15.99 min total |  37184.77 day sales |\n",
            "Predict | Day: 25\n",
            "##########  0.66 min round |  16.65 min total |  37117.21 day sales |\n",
            "Predict | Day: 26\n",
            "##########  0.66 min round |  17.31 min total |  42000.85 day sales |\n",
            "Predict | Day: 27\n",
            "##########  0.69 min round |  18.00 min total |  50971.11 day sales |\n",
            "Predict | Day: 28\n",
            "##########  0.67 min round |  18.67 min total |  51626.25 day sales |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>F9</th>\n",
              "      <th>F10</th>\n",
              "      <th>F11</th>\n",
              "      <th>F12</th>\n",
              "      <th>F13</th>\n",
              "      <th>F14</th>\n",
              "      <th>F15</th>\n",
              "      <th>F16</th>\n",
              "      <th>F17</th>\n",
              "      <th>F18</th>\n",
              "      <th>F19</th>\n",
              "      <th>F20</th>\n",
              "      <th>F21</th>\n",
              "      <th>F22</th>\n",
              "      <th>F23</th>\n",
              "      <th>F24</th>\n",
              "      <th>F25</th>\n",
              "      <th>F26</th>\n",
              "      <th>F27</th>\n",
              "      <th>F28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
              "      <td>0.805470</td>\n",
              "      <td>0.775305</td>\n",
              "      <td>0.767977</td>\n",
              "      <td>0.781276</td>\n",
              "      <td>1.015159</td>\n",
              "      <td>1.117572</td>\n",
              "      <td>1.178913</td>\n",
              "      <td>0.922195</td>\n",
              "      <td>0.978473</td>\n",
              "      <td>0.856891</td>\n",
              "      <td>0.827370</td>\n",
              "      <td>0.968962</td>\n",
              "      <td>1.150332</td>\n",
              "      <td>0.840201</td>\n",
              "      <td>0.879441</td>\n",
              "      <td>0.902192</td>\n",
              "      <td>0.857664</td>\n",
              "      <td>0.803760</td>\n",
              "      <td>0.882869</td>\n",
              "      <td>1.146095</td>\n",
              "      <td>1.064822</td>\n",
              "      <td>0.954925</td>\n",
              "      <td>0.893848</td>\n",
              "      <td>0.801657</td>\n",
              "      <td>0.857421</td>\n",
              "      <td>1.026657</td>\n",
              "      <td>1.154126</td>\n",
              "      <td>1.046657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
              "      <td>0.195890</td>\n",
              "      <td>0.186259</td>\n",
              "      <td>0.166257</td>\n",
              "      <td>0.192223</td>\n",
              "      <td>0.216466</td>\n",
              "      <td>0.275454</td>\n",
              "      <td>0.308840</td>\n",
              "      <td>0.231841</td>\n",
              "      <td>0.222554</td>\n",
              "      <td>0.208632</td>\n",
              "      <td>0.208789</td>\n",
              "      <td>0.220276</td>\n",
              "      <td>0.272124</td>\n",
              "      <td>0.229447</td>\n",
              "      <td>0.205268</td>\n",
              "      <td>0.183776</td>\n",
              "      <td>0.180012</td>\n",
              "      <td>0.180407</td>\n",
              "      <td>0.197663</td>\n",
              "      <td>0.238336</td>\n",
              "      <td>0.252812</td>\n",
              "      <td>0.185559</td>\n",
              "      <td>0.171608</td>\n",
              "      <td>0.182775</td>\n",
              "      <td>0.185753</td>\n",
              "      <td>0.205512</td>\n",
              "      <td>0.272774</td>\n",
              "      <td>0.272037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
              "      <td>0.404298</td>\n",
              "      <td>0.402439</td>\n",
              "      <td>0.446361</td>\n",
              "      <td>0.440341</td>\n",
              "      <td>0.687484</td>\n",
              "      <td>0.842174</td>\n",
              "      <td>0.734053</td>\n",
              "      <td>0.448786</td>\n",
              "      <td>0.447920</td>\n",
              "      <td>0.425156</td>\n",
              "      <td>0.489019</td>\n",
              "      <td>0.557510</td>\n",
              "      <td>0.707093</td>\n",
              "      <td>0.510882</td>\n",
              "      <td>0.455910</td>\n",
              "      <td>0.482729</td>\n",
              "      <td>0.457301</td>\n",
              "      <td>0.508043</td>\n",
              "      <td>0.558501</td>\n",
              "      <td>0.687440</td>\n",
              "      <td>0.709543</td>\n",
              "      <td>0.487504</td>\n",
              "      <td>0.454403</td>\n",
              "      <td>0.459494</td>\n",
              "      <td>0.472237</td>\n",
              "      <td>0.611898</td>\n",
              "      <td>0.735803</td>\n",
              "      <td>0.682508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
              "      <td>1.648211</td>\n",
              "      <td>1.351295</td>\n",
              "      <td>1.381731</td>\n",
              "      <td>1.591307</td>\n",
              "      <td>1.930451</td>\n",
              "      <td>3.032543</td>\n",
              "      <td>3.292765</td>\n",
              "      <td>1.721498</td>\n",
              "      <td>1.368289</td>\n",
              "      <td>1.376492</td>\n",
              "      <td>1.470252</td>\n",
              "      <td>1.829441</td>\n",
              "      <td>3.122716</td>\n",
              "      <td>2.630687</td>\n",
              "      <td>1.595877</td>\n",
              "      <td>1.561949</td>\n",
              "      <td>1.454348</td>\n",
              "      <td>1.464929</td>\n",
              "      <td>1.881935</td>\n",
              "      <td>2.674113</td>\n",
              "      <td>3.237932</td>\n",
              "      <td>1.678512</td>\n",
              "      <td>1.347135</td>\n",
              "      <td>1.370704</td>\n",
              "      <td>1.313127</td>\n",
              "      <td>2.062336</td>\n",
              "      <td>3.144200</td>\n",
              "      <td>3.333766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
              "      <td>0.923010</td>\n",
              "      <td>0.802809</td>\n",
              "      <td>0.794006</td>\n",
              "      <td>0.870516</td>\n",
              "      <td>1.061211</td>\n",
              "      <td>1.521929</td>\n",
              "      <td>1.582376</td>\n",
              "      <td>1.007271</td>\n",
              "      <td>0.948355</td>\n",
              "      <td>0.922045</td>\n",
              "      <td>0.969058</td>\n",
              "      <td>0.995457</td>\n",
              "      <td>1.375875</td>\n",
              "      <td>1.127271</td>\n",
              "      <td>0.932105</td>\n",
              "      <td>0.950779</td>\n",
              "      <td>0.971923</td>\n",
              "      <td>0.975738</td>\n",
              "      <td>1.040149</td>\n",
              "      <td>1.503115</td>\n",
              "      <td>1.586622</td>\n",
              "      <td>1.015009</td>\n",
              "      <td>0.881957</td>\n",
              "      <td>0.887551</td>\n",
              "      <td>0.921509</td>\n",
              "      <td>1.092785</td>\n",
              "      <td>1.485494</td>\n",
              "      <td>1.472579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30485</th>\n",
              "      <td>FOODS_3_823_WI_3_validation</td>\n",
              "      <td>0.437287</td>\n",
              "      <td>0.414107</td>\n",
              "      <td>0.417996</td>\n",
              "      <td>0.402059</td>\n",
              "      <td>0.422472</td>\n",
              "      <td>0.443834</td>\n",
              "      <td>0.538004</td>\n",
              "      <td>0.553766</td>\n",
              "      <td>0.536174</td>\n",
              "      <td>0.429721</td>\n",
              "      <td>0.553670</td>\n",
              "      <td>0.561950</td>\n",
              "      <td>0.731543</td>\n",
              "      <td>0.788598</td>\n",
              "      <td>0.617259</td>\n",
              "      <td>0.467967</td>\n",
              "      <td>0.528957</td>\n",
              "      <td>0.547503</td>\n",
              "      <td>0.567666</td>\n",
              "      <td>0.766609</td>\n",
              "      <td>0.770420</td>\n",
              "      <td>0.568662</td>\n",
              "      <td>0.473513</td>\n",
              "      <td>0.445605</td>\n",
              "      <td>0.419232</td>\n",
              "      <td>0.465734</td>\n",
              "      <td>0.506423</td>\n",
              "      <td>0.574184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30486</th>\n",
              "      <td>FOODS_3_824_WI_3_validation</td>\n",
              "      <td>0.278826</td>\n",
              "      <td>0.259931</td>\n",
              "      <td>0.248713</td>\n",
              "      <td>0.234993</td>\n",
              "      <td>0.266554</td>\n",
              "      <td>0.361493</td>\n",
              "      <td>0.313995</td>\n",
              "      <td>0.404809</td>\n",
              "      <td>0.429679</td>\n",
              "      <td>0.294306</td>\n",
              "      <td>0.355739</td>\n",
              "      <td>0.334586</td>\n",
              "      <td>0.388941</td>\n",
              "      <td>0.403092</td>\n",
              "      <td>0.387391</td>\n",
              "      <td>0.298019</td>\n",
              "      <td>0.359873</td>\n",
              "      <td>0.365486</td>\n",
              "      <td>0.325013</td>\n",
              "      <td>0.467099</td>\n",
              "      <td>0.503385</td>\n",
              "      <td>0.352357</td>\n",
              "      <td>0.281082</td>\n",
              "      <td>0.264274</td>\n",
              "      <td>0.240418</td>\n",
              "      <td>0.259270</td>\n",
              "      <td>0.347480</td>\n",
              "      <td>0.343308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30487</th>\n",
              "      <td>FOODS_3_825_WI_3_validation</td>\n",
              "      <td>0.695545</td>\n",
              "      <td>0.523262</td>\n",
              "      <td>0.479657</td>\n",
              "      <td>0.489042</td>\n",
              "      <td>0.682003</td>\n",
              "      <td>0.808927</td>\n",
              "      <td>0.892382</td>\n",
              "      <td>1.144444</td>\n",
              "      <td>1.126406</td>\n",
              "      <td>0.777918</td>\n",
              "      <td>0.998370</td>\n",
              "      <td>1.157079</td>\n",
              "      <td>1.196482</td>\n",
              "      <td>1.315645</td>\n",
              "      <td>1.388543</td>\n",
              "      <td>0.911805</td>\n",
              "      <td>1.175968</td>\n",
              "      <td>1.131277</td>\n",
              "      <td>0.930986</td>\n",
              "      <td>1.363071</td>\n",
              "      <td>1.513772</td>\n",
              "      <td>1.159821</td>\n",
              "      <td>0.825908</td>\n",
              "      <td>0.769648</td>\n",
              "      <td>0.670055</td>\n",
              "      <td>0.723705</td>\n",
              "      <td>0.827569</td>\n",
              "      <td>0.853397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30488</th>\n",
              "      <td>FOODS_3_826_WI_3_validation</td>\n",
              "      <td>0.924792</td>\n",
              "      <td>0.903743</td>\n",
              "      <td>0.840086</td>\n",
              "      <td>0.809211</td>\n",
              "      <td>0.940038</td>\n",
              "      <td>1.293652</td>\n",
              "      <td>1.144941</td>\n",
              "      <td>1.224439</td>\n",
              "      <td>1.261772</td>\n",
              "      <td>0.913353</td>\n",
              "      <td>1.173968</td>\n",
              "      <td>1.175660</td>\n",
              "      <td>1.285773</td>\n",
              "      <td>1.264000</td>\n",
              "      <td>1.260461</td>\n",
              "      <td>0.942477</td>\n",
              "      <td>1.044551</td>\n",
              "      <td>1.109059</td>\n",
              "      <td>1.010800</td>\n",
              "      <td>1.309754</td>\n",
              "      <td>1.419119</td>\n",
              "      <td>0.947424</td>\n",
              "      <td>0.912381</td>\n",
              "      <td>0.852497</td>\n",
              "      <td>0.771748</td>\n",
              "      <td>0.953457</td>\n",
              "      <td>0.978468</td>\n",
              "      <td>1.224990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30489</th>\n",
              "      <td>FOODS_3_827_WI_3_validation</td>\n",
              "      <td>0.189990</td>\n",
              "      <td>1.056143</td>\n",
              "      <td>1.074143</td>\n",
              "      <td>1.460027</td>\n",
              "      <td>1.970019</td>\n",
              "      <td>2.507388</td>\n",
              "      <td>2.237874</td>\n",
              "      <td>1.796011</td>\n",
              "      <td>1.817659</td>\n",
              "      <td>1.577191</td>\n",
              "      <td>1.960009</td>\n",
              "      <td>1.923742</td>\n",
              "      <td>2.418824</td>\n",
              "      <td>1.919360</td>\n",
              "      <td>2.029080</td>\n",
              "      <td>1.711559</td>\n",
              "      <td>1.816316</td>\n",
              "      <td>1.959649</td>\n",
              "      <td>1.839610</td>\n",
              "      <td>2.531508</td>\n",
              "      <td>2.219309</td>\n",
              "      <td>1.677950</td>\n",
              "      <td>1.604961</td>\n",
              "      <td>1.443221</td>\n",
              "      <td>1.418298</td>\n",
              "      <td>1.659979</td>\n",
              "      <td>2.114732</td>\n",
              "      <td>1.980177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30490 rows  29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id        F1  ...       F27       F28\n",
              "0      HOBBIES_1_001_CA_1_validation  0.805470  ...  1.154126  1.046657\n",
              "1      HOBBIES_1_002_CA_1_validation  0.195890  ...  0.272774  0.272037\n",
              "2      HOBBIES_1_003_CA_1_validation  0.404298  ...  0.735803  0.682508\n",
              "3      HOBBIES_1_004_CA_1_validation  1.648211  ...  3.144200  3.333766\n",
              "4      HOBBIES_1_005_CA_1_validation  0.923010  ...  1.485494  1.472579\n",
              "...                              ...       ...  ...       ...       ...\n",
              "30485    FOODS_3_823_WI_3_validation  0.437287  ...  0.506423  0.574184\n",
              "30486    FOODS_3_824_WI_3_validation  0.278826  ...  0.347480  0.343308\n",
              "30487    FOODS_3_825_WI_3_validation  0.695545  ...  0.827569  0.853397\n",
              "30488    FOODS_3_826_WI_3_validation  0.924792  ...  0.978468  1.224990\n",
              "30489    FOODS_3_827_WI_3_validation  0.189990  ...  2.114732  1.980177\n",
              "\n",
              "[30490 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym40qYzLPvGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Export\n",
        "#################################################################################\n",
        "# Reading competition sample submission and\n",
        "# merging our predictions\n",
        "# As we have predictions only for \"_validation\" data\n",
        "# we need to do fillna() for \"_evaluation\" items\n",
        "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
        "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
        "submission.to_csv(AUX_MODELS+'submission_v'+str(VER)+'.csv', index=False)\n",
        "submission.to_csv('submission_v'+str(VER)+'.csv', index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU0HId86PvGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary\n",
        "\n",
        "# Of course here is no magic at all.\n",
        "# No \"Novel\" features and no brilliant ideas.\n",
        "# We just carefully joined all\n",
        "# our previous fe work and created a model.\n",
        "\n",
        "# Also!\n",
        "# In my opinion this strategy is a \"dead end\".\n",
        "# Overfits a lot LB and with 1 final submission \n",
        "# you have no option to risk.\n",
        "\n",
        "\n",
        "# Improvement should come from:\n",
        "# Loss function\n",
        "# Data representation\n",
        "# Stable CV\n",
        "# Good features reduction strategy\n",
        "# Predictions stabilization with NN\n",
        "# Trend prediction\n",
        "# Real zero sales detection/classification\n",
        "\n",
        "\n",
        "# Good kernels references \n",
        "## (the order is random and the list is not complete):\n",
        "# https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
        "# https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
        "# https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
        "# https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n",
        "# https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n",
        "# https://www.kaggle.com/yassinealouini/seq2seq\n",
        "# https://www.kaggle.com/kailex/m5-forecaster-v2\n",
        "# https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n",
        "\n",
        "\n",
        "# Features were created in these kernels:\n",
        "## \n",
        "# Mean encodings and PCA options\n",
        "# https://www.kaggle.com/kyakovlev/m5-custom-features\n",
        "##\n",
        "# Lags and rolling lags\n",
        "# https://www.kaggle.com/kyakovlev/m5-lags-features\n",
        "##\n",
        "# Base Grid and base features (calendar/price/etc)\n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "\n",
        "\n",
        "# Personal request\n",
        "# Please don't upvote any ensemble and copypaste kernels\n",
        "## The worst case is ensemble without any analyse.\n",
        "## The best choice - just ignore it.\n",
        "## I would like to see more kernels with interesting and original approaches.\n",
        "## Don't feed copypasters with upvotes.\n",
        "\n",
        "## It doesn't mean that you should not fork and improve others kernels\n",
        "## but I would like to see params and code tuning based on some CV and analyse\n",
        "## and not only on LB probing.\n",
        "## Small changes could be shared in comments and authors can improve their kernel.\n",
        "\n",
        "## Feel free to criticize this kernel as my knowlege is very limited\n",
        "## and I can be wrong in code and descriptions. \n",
        "## Thank you."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzgr6gRwU0gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df, features_columns = get_data_by_store(\"CA_1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR8EZRFZk0d7",
        "colab_type": "code",
        "outputId": "fb1e4859-0a1b-4ca6-bce6-f16310c626d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "grid_df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'd', 'sales', 'item_id', 'dept_id', 'cat_id', 'release',\n",
              "       'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean',\n",
              "       'price_norm', 'price_nunique', 'item_nunique', 'price_momentum',\n",
              "       'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1',\n",
              "       'event_name_2', 'event_type_2', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm',\n",
              "       'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std',\n",
              "       'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean',\n",
              "       'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30',\n",
              "       'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34',\n",
              "       'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38',\n",
              "       'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42',\n",
              "       'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14',\n",
              "       'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60',\n",
              "       'rolling_std_60', 'rolling_mean_180', 'rolling_std_180',\n",
              "       'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14',\n",
              "       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n",
              "       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n",
              "       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n",
              "       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n",
              "       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'snap'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gruRmp9kliLo",
        "colab_type": "code",
        "outputId": "ee72bc5a-7f18-4e35-f5ab-fa05b1e5e9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "features_columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['item_id',\n",
              " 'dept_id',\n",
              " 'cat_id',\n",
              " 'release',\n",
              " 'sell_price',\n",
              " 'price_max',\n",
              " 'price_min',\n",
              " 'price_std',\n",
              " 'price_mean',\n",
              " 'price_norm',\n",
              " 'price_nunique',\n",
              " 'item_nunique',\n",
              " 'price_momentum',\n",
              " 'price_momentum_m',\n",
              " 'price_momentum_y',\n",
              " 'event_name_1',\n",
              " 'event_type_1',\n",
              " 'event_name_2',\n",
              " 'event_type_2',\n",
              " 'tm_d',\n",
              " 'tm_w',\n",
              " 'tm_m',\n",
              " 'tm_y',\n",
              " 'tm_wm',\n",
              " 'tm_dw',\n",
              " 'tm_w_end',\n",
              " 'enc_cat_id_mean',\n",
              " 'enc_cat_id_std',\n",
              " 'enc_dept_id_mean',\n",
              " 'enc_dept_id_std',\n",
              " 'enc_item_id_mean',\n",
              " 'enc_item_id_std',\n",
              " 'sales_lag_28',\n",
              " 'sales_lag_29',\n",
              " 'sales_lag_30',\n",
              " 'sales_lag_31',\n",
              " 'sales_lag_32',\n",
              " 'sales_lag_33',\n",
              " 'sales_lag_34',\n",
              " 'sales_lag_35',\n",
              " 'sales_lag_36',\n",
              " 'sales_lag_37',\n",
              " 'sales_lag_38',\n",
              " 'sales_lag_39',\n",
              " 'sales_lag_40',\n",
              " 'sales_lag_41',\n",
              " 'sales_lag_42',\n",
              " 'rolling_mean_7',\n",
              " 'rolling_std_7',\n",
              " 'rolling_mean_14',\n",
              " 'rolling_std_14',\n",
              " 'rolling_mean_30',\n",
              " 'rolling_std_30',\n",
              " 'rolling_mean_60',\n",
              " 'rolling_std_60',\n",
              " 'rolling_mean_180',\n",
              " 'rolling_std_180',\n",
              " 'rolling_mean_tmp_1_7',\n",
              " 'rolling_mean_tmp_1_14',\n",
              " 'rolling_mean_tmp_1_30',\n",
              " 'rolling_mean_tmp_1_60',\n",
              " 'rolling_mean_tmp_7_7',\n",
              " 'rolling_mean_tmp_7_14',\n",
              " 'rolling_mean_tmp_7_30',\n",
              " 'rolling_mean_tmp_7_60',\n",
              " 'rolling_mean_tmp_14_7',\n",
              " 'rolling_mean_tmp_14_14',\n",
              " 'rolling_mean_tmp_14_30',\n",
              " 'rolling_mean_tmp_14_60',\n",
              " 'snap']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDEDPNgNlsV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imp=pd.DataFrame({'feature': features_columns,\n",
        "\n",
        " 'importance':estimator.feature_importance()}).sort_values('importance',\n",
        "\n",
        " ascending=False)\n",
        "imp.to_csv(AUX_MODELS+'importance_v'+str(VER)+'.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7hrVgx2R4-F",
        "colab_type": "code",
        "outputId": "a7ada755-8cbc-41ff-c021-be5ccac01a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "grid_df.last_sale"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-16d656408134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_sale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'last_sale'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eMqhKjFTtbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}