{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "check_imporatance.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipejun-ai/m5-accuracy/blob/master/check_imporatance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "VY1mXqL-PvFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "\n",
        "# custom imports\n",
        "from multiprocessing import Pool        # Multiprocess Runs\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da5VwfTzQYpY",
        "colab_type": "code",
        "outputId": "190993bf-cfc6-4ff3-c45b-8deba4a028b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ0uXtDQQhAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIRPATH=\"/content/gdrive/My Drive/kaggle/\"\n",
        "#DIRPATH=\"C:/Users/peiju/Documents/Study/kaggle/m5-forecasting-accuracy/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHG3l04qCpWw",
        "colab_type": "code",
        "outputId": "79b7bd64-eb17-4cd9-ac5e-24aa3c77f811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install Boruta"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.6/dist-packages (0.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from Boruta) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from Boruta) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from Boruta) (1.18.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17.1->Boruta) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqGRFhY6Crpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from boruta import BorutaPy\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from boruta import BorutaPy\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.utils import check_random_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30TdFXfMioLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########LGM wrapper\n",
        "\n",
        "\n",
        "class BorutaPyForLGB(BorutaPy):\n",
        "    def __init__(self, estimator, n_estimators=1000, perc=100, alpha=0.05,\n",
        "                 two_step=True, max_iter=100, random_state=None, verbose=0):\n",
        "        super().__init__(estimator, n_estimators, perc, alpha,\n",
        "                         two_step, max_iter, random_state, verbose)\n",
        "        if random_state is None:\n",
        "            self.random_state_input = np.random.randint(0, 2**64-1)\n",
        "        elif isinstance(random_state, int):\n",
        "            self.random_state_input = random_state\n",
        "        else:\n",
        "            raise TypeError('random_state must be int or None')\n",
        "\n",
        "    def _get_tree_num(self, n_feat):\n",
        "        depth = self.estimator.get_params()['max_depth']\n",
        "        if (depth == None) or (depth <= 0):\n",
        "            depth = 10\n",
        "        f_repr = 100\n",
        "        multi = ((n_feat * 2) / (np.sqrt(n_feat * 2) * depth))\n",
        "        n_estimators = int(multi * f_repr)\n",
        "        return n_estimators\n",
        "\n",
        "    def _fit(self, X, y):\n",
        "        # check input params\n",
        "        self._check_params(X, y)\n",
        "        self.random_state = check_random_state(self.random_state)\n",
        "        # setup variables for Boruta\n",
        "        n_sample, n_feat = X.shape\n",
        "        _iter = 1\n",
        "        # holds the decision about each feature:\n",
        "        # 0  - default state = tentative in original code\n",
        "        # 1  - accepted in original code\n",
        "        # -1 - rejected in original code\n",
        "        dec_reg = np.zeros(n_feat, dtype=np.int)\n",
        "        # counts how many times a given feature was more important than\n",
        "        # the best of the shadow features\n",
        "        hit_reg = np.zeros(n_feat, dtype=np.int)\n",
        "        # these record the history of the iterations\n",
        "        imp_history = np.zeros(n_feat, dtype=np.float)\n",
        "        sha_max_history = []\n",
        "\n",
        "        # set n_estimators\n",
        "        if self.n_estimators != 'auto':\n",
        "            self.estimator.set_params(n_estimators=self.n_estimators)\n",
        "\n",
        "        # main feature selection loop\n",
        "        while np.any(dec_reg == 0) and _iter < self.max_iter:\n",
        "            # find optimal number of trees and depth\n",
        "            if self.n_estimators == 'auto':\n",
        "                # number of features that aren't rejected\n",
        "                not_rejected = np.where(dec_reg >= 0)[0].shape[0]\n",
        "                n_tree = self._get_tree_num(not_rejected)\n",
        "                self.estimator.set_params(n_estimators=n_tree)\n",
        "\n",
        "            # make sure we start with a new tree in each iteration\n",
        "            self.estimator.set_params(random_state=self.random_state_input)\n",
        "\n",
        "            # add shadow attributes, shuffle them and train estimator, get imps\n",
        "            cur_imp = self._add_shadows_get_imps(X, y, dec_reg)\n",
        "\n",
        "            # get the threshold of shadow importances we will use for rejection\n",
        "            imp_sha_max = np.percentile(cur_imp[1], self.perc)\n",
        "\n",
        "            # record importance history\n",
        "            sha_max_history.append(imp_sha_max)\n",
        "            imp_history = np.vstack((imp_history, cur_imp[0]))\n",
        "\n",
        "            # register which feature is more imp than the max of shadows\n",
        "            hit_reg = self._assign_hits(hit_reg, cur_imp, imp_sha_max)\n",
        "\n",
        "            # based on hit_reg we check if a feature is doing better than\n",
        "            # expected by chance\n",
        "            dec_reg = self._do_tests(dec_reg, hit_reg, _iter)\n",
        "\n",
        "            # print out confirmed features\n",
        "            if self.verbose > 0 and _iter < self.max_iter:\n",
        "                self._print_results(dec_reg, _iter, 0)\n",
        "            if _iter < self.max_iter:\n",
        "                _iter += 1\n",
        "\n",
        "        # we automatically apply R package's rough fix for tentative ones\n",
        "        confirmed = np.where(dec_reg == 1)[0]\n",
        "        tentative = np.where(dec_reg == 0)[0]\n",
        "        # ignore the first row of zeros\n",
        "        tentative_median = np.median(imp_history[1:, tentative], axis=0)\n",
        "        # which tentative to keep\n",
        "        tentative_confirmed = np.where(tentative_median\n",
        "                                       > np.median(sha_max_history))[0]\n",
        "        tentative = tentative[tentative_confirmed]\n",
        "\n",
        "        # basic result variables\n",
        "        self.n_features_ = confirmed.shape[0]\n",
        "        self.support_ = np.zeros(n_feat, dtype=np.bool)\n",
        "        self.support_[confirmed] = 1\n",
        "        self.support_weak_ = np.zeros(n_feat, dtype=np.bool)\n",
        "        self.support_weak_[tentative] = 1\n",
        "\n",
        "        # ranking, confirmed variables are rank 1\n",
        "        self.ranking_ = np.ones(n_feat, dtype=np.int)\n",
        "        # tentative variables are rank 2\n",
        "        self.ranking_[tentative] = 2\n",
        "        # selected = confirmed and tentative\n",
        "        selected = np.hstack((confirmed, tentative))\n",
        "        # all rejected features are sorted by importance history\n",
        "        not_selected = np.setdiff1d(np.arange(n_feat), selected)\n",
        "        # large importance values should rank higher = lower ranks -> *(-1)\n",
        "        imp_history_rejected = imp_history[1:, not_selected] * -1\n",
        "\n",
        "        # update rank for not_selected features\n",
        "        if not_selected.shape[0] > 0:\n",
        "                # calculate ranks in each iteration, then median of ranks across feats\n",
        "                iter_ranks = self._nanrankdata(imp_history_rejected, axis=1)\n",
        "                rank_medians = np.nanmedian(iter_ranks, axis=0)\n",
        "                ranks = self._nanrankdata(rank_medians, axis=0)\n",
        "\n",
        "                # set smallest rank to 3 if there are tentative feats\n",
        "                if tentative.shape[0] > 0:\n",
        "                    ranks = ranks - np.min(ranks) + 3\n",
        "                else:\n",
        "                    # and 2 otherwise\n",
        "                    ranks = ranks - np.min(ranks) + 2\n",
        "                self.ranking_[not_selected] = ranks\n",
        "        else:\n",
        "            # all are selected, thus we set feature supports to True\n",
        "            self.support_ = np.ones(n_feat, dtype=np.bool)\n",
        "\n",
        "        # notify user\n",
        "        if self.verbose > 0:\n",
        "            self._print_results(dec_reg, _iter, 1)\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm1pyYhzPvFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helpers\n",
        "#################################################################################\n",
        "## Seeder\n",
        "# :seed to make all processes deterministic     # type: int\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    \n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "413KNvT2PvFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helper to load data by store ID\n",
        "#################################################################################\n",
        "# Read data\n",
        "def get_data_by_store(store):\n",
        "    \n",
        "    # Read and contact basic feature\n",
        "    df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)\n",
        "    \n",
        "    # Leave only relevant store\n",
        "    df = df[df['store_id']==store]\n",
        "\n",
        "    # With memory limits we have to read \n",
        "    # lags and mean encoding features\n",
        "    # separately and drop items that we don't need.\n",
        "    # As our Features Grids are aligned \n",
        "    # we can use index to keep only necessary rows\n",
        "    # Alignment is good for us as concat uses less memory than merge.\n",
        "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
        "    df2 = df2[df2.index.isin(df.index)]\n",
        "    \n",
        "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
        "    df3 = df3[df3.index.isin(df.index)]\n",
        "\n",
        "\n",
        "    df4 = pd.read_pickle(SNAP).iloc[:,2:]\n",
        "    df4 = df4[df4.index.isin(df.index)]\n",
        "\n",
        "    df5 = pd.read_pickle(EVENT_TYPE).iloc[:,2:]\n",
        "    df5 = df5[df5.index.isin(df.index)]\n",
        "\n",
        "    df6 = pd.read_pickle(EVENT_VALUE).iloc[:,2]\n",
        "    df6 = df6[df6.index.isin(df.index)]\n",
        "\n",
        "    df7 = pd.read_pickle(LAGS_364).iloc[:,2:]\n",
        "    df7 = df7[df7.index.isin(df.index)]\n",
        "\n",
        "    df8 = pd.read_pickle(HOLIDAY).iloc[:,2:]\n",
        "    df8 = df8[df8.index.isin(df.index)]\n",
        "\n",
        "    #df5 = pd.read_pickle(EVENT_TYPE).iloc[:,2:]\n",
        "    #df5 = df5[df5.index.isin(df.index)]\n",
        "    \n",
        "    df = pd.concat([df, df2], axis=1)\n",
        "    del df2 # to not reach memory limit \n",
        "    \n",
        "    df = pd.concat([df, df3], axis=1)\n",
        "    del df3 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df4], axis=1)\n",
        "    del df4 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df5], axis=1)\n",
        "    del df5 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df6], axis=1)\n",
        "    del df6 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df7], axis=1)\n",
        "    del df7 # to not reach memory limit \n",
        "\n",
        "    df = pd.concat([df, df8], axis=1)\n",
        "    del df8 # to not reach memory limit \n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "    # Create features list\n",
        "    features = [col for col in list(df) if col not in remove_features]\n",
        "    df = df[['id','d',TARGET]+features]\n",
        "    \n",
        "    # Skipping first n rows\n",
        "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
        "    \n",
        "    return df, features\n",
        "\n",
        "# Recombine Test set after training\n",
        "def get_base_test():\n",
        "    base_test = pd.DataFrame()\n",
        "\n",
        "    for store_id in STORES_IDS:\n",
        "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
        "        temp_df['store_id'] = store_id\n",
        "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
        "    \n",
        "    return base_test\n",
        "\n",
        "\n",
        "########################### Helper to make dynamic rolling lags\n",
        "#################################################################################\n",
        "def make_lag(LAG_DAY):\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "\n",
        "def make_lag_roll(LAG_DAY):\n",
        "    shift_day = LAG_DAY[0]\n",
        "    roll_wind = LAG_DAY[1]\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
        "    return lag_df[[col_name]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NDcdsu0PvFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Model params\n",
        "#################################################################################\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.1,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 1400,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': -1,\n",
        "                } \n",
        "\n",
        "# Let's look closer on params\n",
        "\n",
        "## 'boosting_type': 'gbdt'\n",
        "# we have 'goss' option for faster training\n",
        "# but it normally leads to underfit.\n",
        "# Also there is good 'dart' mode\n",
        "# but it takes forever to train\n",
        "# and model performance depends \n",
        "# a lot on random factor \n",
        "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
        "\n",
        "## 'objective': 'tweedie'\n",
        "# Tweedie Gradient Boosting for Extremely\n",
        "# Unbalanced Zero-inflated Data\n",
        "# https://arxiv.org/pdf/1811.10192.pdf\n",
        "# and many more articles about tweediie\n",
        "#\n",
        "# Strange (for me) but Tweedie is close in results\n",
        "# to my own ugly loss.\n",
        "# My advice here - make OWN LOSS function\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
        "# I think many of you already using it (after poisson kernel appeared) \n",
        "# (kagglers are very good with \"params\" testing and tuning).\n",
        "# Try to figure out why Tweedie works.\n",
        "# probably it will show you new features options\n",
        "# or data transformation (Target transformation?).\n",
        "\n",
        "## 'tweedie_variance_power': 1.1\n",
        "# default = 1.5\n",
        "# set this closer to 2 to shift towards a Gamma distribution\n",
        "# set this closer to 1 to shift towards a Poisson distribution\n",
        "# my CV shows 1.1 is optimal \n",
        "# but you can make your own choice\n",
        "\n",
        "## 'metric': 'rmse'\n",
        "# Doesn't mean anything to us\n",
        "# as competition metric is different\n",
        "# and we don't use early stoppings here.\n",
        "# So rmse serves just for general \n",
        "# model performance overview.\n",
        "# Also we use \"fake\" validation set\n",
        "# (as it makes part of the training set)\n",
        "# so even general rmse score doesn't mean anything))\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
        "\n",
        "## 'subsample': 0.5\n",
        "# Serves to fight with overfit\n",
        "# this will randomly select part of data without resampling\n",
        "# Chosen by CV (my CV can be wrong!)\n",
        "# Next kernel will be about CV\n",
        "\n",
        "##'subsample_freq': 1\n",
        "# frequency for bagging\n",
        "# default value - seems ok\n",
        "\n",
        "## 'learning_rate': 0.03\n",
        "# Chosen by CV\n",
        "# Smaller - longer training\n",
        "# but there is an option to stop \n",
        "# in \"local minimum\"\n",
        "# Bigger - faster training\n",
        "# but there is a chance to\n",
        "# not find \"global minimum\" minimum\n",
        "\n",
        "## 'num_leaves': 2**11-1\n",
        "## 'min_data_in_leaf': 2**12-1\n",
        "# Force model to use more features\n",
        "# We need it to reduce \"recursive\"\n",
        "# error impact.\n",
        "# Also it leads to overfit\n",
        "# that's why we use small \n",
        "# 'max_bin': 100\n",
        "\n",
        "## l1, l2 regularizations\n",
        "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
        "# Good tiny explanation\n",
        "# l2 can work with bigger num_leaves\n",
        "# but my CV doesn't show boost\n",
        "                    \n",
        "## 'n_estimators': 1400\n",
        "# CV shows that there should be\n",
        "# different values for each state/store.\n",
        "# Current value was chosen \n",
        "# for general purpose.\n",
        "# As we don't use any early stopings\n",
        "# careful to not overfit Public LB.\n",
        "\n",
        "##'feature_fraction': 0.5\n",
        "# LightGBM will randomly select \n",
        "# part of features on each iteration (tree).\n",
        "# We have maaaany features\n",
        "# and many of them are \"duplicates\"\n",
        "# and many just \"noise\"\n",
        "# good values here - 0.5-0.7 (by CV)\n",
        "\n",
        "## 'boost_from_average': False\n",
        "# There is some \"problem\"\n",
        "# to code boost_from_average for \n",
        "# custom loss\n",
        "# 'True' makes training faster\n",
        "# BUT carefull use it\n",
        "# https://github.com/microsoft/LightGBM/issues/1514\n",
        "# not our case but good to know cons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvwIzPcOPvFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEfqHZHpPvFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Vars\n",
        "#################################################################################\n",
        "VER = 6                          # Our model version\n",
        "SEED = 42                        # We want all things\n",
        "seed_everything(SEED)            # to be as deterministic \n",
        "lgb_params['seed'] = SEED        # as possible\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "\n",
        "#LIMITS and const\n",
        "TARGET      = 'sales'            # Our target\n",
        "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
        "END_TRAIN   = 1913               # End day of our train set\n",
        "P_HORIZON   = 28                 # Prediction horizon\n",
        "USE_AUX     = False            # Use or not pretrained models\n",
        "\n",
        "#FEATURES to remove\n",
        "## These features lead to overfit\n",
        "## or values not present in test set\n",
        "remove_features = ['id','state_id','store_id',\n",
        "                   'date','wm_yr_wk','d','snap_CA','snap_TX','snap_WI'\n",
        "                    ,\n",
        "                   TARGET]\n",
        "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
        "                   'enc_dept_id_mean','enc_dept_id_std',\n",
        "                   'enc_item_id_mean','enc_item_id_std'] \n",
        "\n",
        "#PATHS for Features\n",
        "ORIGINAL = DIRPATH\n",
        "BASE     = DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'\n",
        "PRICE    = DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl'\n",
        "CALENDAR = DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl'\n",
        "LAGS     = DIRPATH+'/output/m5-lags-features/lags_df_28.pkl'\n",
        "MEAN_ENC = DIRPATH+'/output/m5-custom-features/mean_encoding_df.pkl'\n",
        "FINAL_SALES = DIRPATH+'/output/m5-custom-features/finalsales_df.pkl'\n",
        "SNAP      =  DIRPATH+'/output/m5-custom-features/snap_df.pkl'\n",
        "EVENT_TYPE      =  DIRPATH+'/output/m5-custom-features/event_type.pkl'\n",
        "EVENT_VALUE      =  DIRPATH+'/output/m5-custom-features/event_value.pkl'\n",
        "LAGS_364=  DIRPATH+'/output/m5-custom-features/lags_df_364.pkl'\n",
        "HOLIDAY=DIRPATH+'/output/m5-custom-features/holiday_mean.pkl'\n",
        "# AUX(pretrained) Models paths\n",
        "AUX_MODELS = DIRPATH+'/output/m5-three-shades-of-dark-darker-magic/'\n",
        "\n",
        "\n",
        "#STORES ids\n",
        "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
        "STORES_IDS = list(STORES_IDS.unique())\n",
        "\n",
        "\n",
        "#SPLITS for lags creation\n",
        "SHIFT_DAY  = 28\n",
        "N_LAGS     = 15\n",
        "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
        "ROLS_SPLIT = []\n",
        "for i in [1,7,14]:\n",
        "    for j in [7,14,30,60]:\n",
        "        ROLS_SPLIT.append([i,j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usB9f3Hdgqp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df, features_columns = get_data_by_store(\"CA_1\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RFAOauwEo0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mask = grid_df['d']<=END_TRAIN\n",
        "valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
        "preds_mask = grid_df['d']>(END_TRAIN-100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiB_nUoAvlpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#欠損値\n",
        "#grid_df1=grid_df[~grid_df.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "#grid_df1.head()\n",
        "#grid_df.loc[:,~grid_df.notnull().all()]\n",
        "category_col=[\"sales\",\"id\",\"d\",\"item_id\",\"dept_id\",\"cat_id\",\"event_name_1\",\"event_type_1\",\"event_name_2\",\"event_type_2\"]\n",
        "tmp_col=[x for x in grid_df.columns if x not in category_col]\n",
        "grid_df.loc[:,tmp_col]=grid_df[tmp_col].fillna(0)\n",
        "#grid_df=pd.concat([grid_df[category_col],grid_df[tmp_col].fillna(0)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VL8MQT_vpO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f20b2870-2860-486d-8662-b487613e95a3"
      },
      "source": [
        "grid_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>release</th>\n",
              "      <th>sell_price</th>\n",
              "      <th>price_max</th>\n",
              "      <th>price_min</th>\n",
              "      <th>price_std</th>\n",
              "      <th>price_mean</th>\n",
              "      <th>price_norm</th>\n",
              "      <th>price_nunique</th>\n",
              "      <th>item_nunique</th>\n",
              "      <th>price_momentum</th>\n",
              "      <th>price_momentum_m</th>\n",
              "      <th>price_momentum_y</th>\n",
              "      <th>event_name_1</th>\n",
              "      <th>event_type_1</th>\n",
              "      <th>event_name_2</th>\n",
              "      <th>event_type_2</th>\n",
              "      <th>tm_d</th>\n",
              "      <th>tm_w</th>\n",
              "      <th>tm_m</th>\n",
              "      <th>tm_y</th>\n",
              "      <th>tm_wm</th>\n",
              "      <th>tm_dw</th>\n",
              "      <th>tm_w_end</th>\n",
              "      <th>enc_cat_id_mean</th>\n",
              "      <th>enc_cat_id_std</th>\n",
              "      <th>enc_dept_id_mean</th>\n",
              "      <th>enc_dept_id_std</th>\n",
              "      <th>enc_item_id_mean</th>\n",
              "      <th>enc_item_id_std</th>\n",
              "      <th>sales_lag_28</th>\n",
              "      <th>sales_lag_29</th>\n",
              "      <th>sales_lag_30</th>\n",
              "      <th>sales_lag_31</th>\n",
              "      <th>sales_lag_32</th>\n",
              "      <th>...</th>\n",
              "      <th>rolling_mean_14</th>\n",
              "      <th>rolling_std_14</th>\n",
              "      <th>rolling_mean_30</th>\n",
              "      <th>rolling_std_30</th>\n",
              "      <th>rolling_mean_60</th>\n",
              "      <th>rolling_std_60</th>\n",
              "      <th>rolling_mean_180</th>\n",
              "      <th>rolling_std_180</th>\n",
              "      <th>rolling_mean_tmp_1_7</th>\n",
              "      <th>rolling_mean_tmp_1_14</th>\n",
              "      <th>rolling_mean_tmp_1_30</th>\n",
              "      <th>rolling_mean_tmp_1_60</th>\n",
              "      <th>rolling_mean_tmp_7_7</th>\n",
              "      <th>rolling_mean_tmp_7_14</th>\n",
              "      <th>rolling_mean_tmp_7_30</th>\n",
              "      <th>rolling_mean_tmp_7_60</th>\n",
              "      <th>rolling_mean_tmp_14_7</th>\n",
              "      <th>rolling_mean_tmp_14_14</th>\n",
              "      <th>rolling_mean_tmp_14_30</th>\n",
              "      <th>rolling_mean_tmp_14_60</th>\n",
              "      <th>snap</th>\n",
              "      <th>Event_total</th>\n",
              "      <th>Event_total</th>\n",
              "      <th>Event_total</th>\n",
              "      <th>Event_total</th>\n",
              "      <th>sales_lag_28_364</th>\n",
              "      <th>sales_lag_35_364</th>\n",
              "      <th>rolling_mean_7_364</th>\n",
              "      <th>rolling_std_7_364</th>\n",
              "      <th>rolling_mean_14_364</th>\n",
              "      <th>rolling_std_14_364</th>\n",
              "      <th>rolling_mean_30_364</th>\n",
              "      <th>rolling_std_30_364</th>\n",
              "      <th>lag_28_35_mean_364</th>\n",
              "      <th>holiday</th>\n",
              "      <th>rolling_holiday_mean_7</th>\n",
              "      <th>rolling_holiday_mean_14</th>\n",
              "      <th>rolling_holiday_mean_30</th>\n",
              "      <th>rolling_holiday_mean_60</th>\n",
              "      <th>rolling_holiday_mean_180</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>1</td>\n",
              "      <td>12.0</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>0</td>\n",
              "      <td>0.459961</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.019760</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.919922</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>2.259766</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>2.544922</td>\n",
              "      <td>4.695312</td>\n",
              "      <td>7.183594</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>HOBBIES_1_009</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>0</td>\n",
              "      <td>1.559570</td>\n",
              "      <td>1.769531</td>\n",
              "      <td>1.559570</td>\n",
              "      <td>0.032745</td>\n",
              "      <td>1.764648</td>\n",
              "      <td>0.881348</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.885742</td>\n",
              "      <td>0.896484</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>2.259766</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>2.544922</td>\n",
              "      <td>0.850098</td>\n",
              "      <td>1.754883</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>0</td>\n",
              "      <td>3.169922</td>\n",
              "      <td>3.169922</td>\n",
              "      <td>2.970703</td>\n",
              "      <td>0.046356</td>\n",
              "      <td>2.980469</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.064453</td>\n",
              "      <td>1.043945</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>2.259766</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>2.544922</td>\n",
              "      <td>0.611328</td>\n",
              "      <td>0.863281</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>HOBBIES_1_012</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>0</td>\n",
              "      <td>5.980469</td>\n",
              "      <td>6.519531</td>\n",
              "      <td>5.980469</td>\n",
              "      <td>0.115967</td>\n",
              "      <td>6.468750</td>\n",
              "      <td>0.916992</td>\n",
              "      <td>3.0</td>\n",
              "      <td>71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.958984</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>2.259766</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>2.544922</td>\n",
              "      <td>0.384766</td>\n",
              "      <td>0.692871</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>HOBBIES_1_015</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>0</td>\n",
              "      <td>0.700195</td>\n",
              "      <td>0.720215</td>\n",
              "      <td>0.680176</td>\n",
              "      <td>0.011337</td>\n",
              "      <td>0.706543</td>\n",
              "      <td>0.972168</td>\n",
              "      <td>3.0</td>\n",
              "      <td>16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.990234</td>\n",
              "      <td>1.001953</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>2.259766</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>2.544922</td>\n",
              "      <td>4.441406</td>\n",
              "      <td>6.703125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 92 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id  ...  rolling_holiday_mean_180\n",
              "0  HOBBIES_1_008_CA_1_validation  ...                       0.0\n",
              "1  HOBBIES_1_009_CA_1_validation  ...                       0.0\n",
              "2  HOBBIES_1_010_CA_1_validation  ...                       0.0\n",
              "3  HOBBIES_1_012_CA_1_validation  ...                       0.0\n",
              "4  HOBBIES_1_015_CA_1_validation  ...                       0.0\n",
              "\n",
              "[5 rows x 92 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90WXZL3_XQhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "str_col=[\"event_name_1\",\"event_type_1\",\"event_name_2\",\"event_type_2\"]\n",
        "grid_df[str_col]=grid_df[str_col].astype(str)\n",
        "tmp_col=[x for x in grid_df.columns if x not in str_col]\n",
        "grid_df.loc[:,str_col]=grid_df[str_col].fillna(\"Normal\")\n",
        "\n",
        "#grid_df=pd.concat([grid_df[str_col],grid_df[tmp_col].fillna(\"Normal\")],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzxOU077WCUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lable Encoder\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "for col in [\"item_id\",\"dept_id\",\"cat_id\",\"event_name_1\",\"event_type_1\",\"event_name_2\",\"event_type_2\"]:\n",
        "  #LabelEncoderのインスタンスを生成\n",
        "  le = LabelEncoder()\n",
        "  #ラベルを覚えさせる\n",
        "  le = le.fit(grid_df[col])\n",
        "  #ラベルを整数に変換\n",
        "  grid_df[col] = le.transform(grid_df[col])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWjBbaRxTbue",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gtqoNEqFCRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=grid_df[train_mask][features_columns]\n",
        "y_train=grid_df[train_mask][TARGET]\n",
        "X_valid=grid_df[valid_mask][features_columns]\n",
        "y_valid=grid_df[valid_mask][TARGET]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z7G3i9YpEgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0fb8fea3-8638-4d67-81dd-406b500dd08a"
      },
      "source": [
        "#X_valid[X_valid.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "X_train.isnull().any()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "item_id                     False\n",
              "dept_id                     False\n",
              "cat_id                      False\n",
              "release                     False\n",
              "sell_price                  False\n",
              "                            ...  \n",
              "rolling_holiday_mean_7      False\n",
              "rolling_holiday_mean_14     False\n",
              "rolling_holiday_mean_30     False\n",
              "rolling_holiday_mean_60     False\n",
              "rolling_holiday_mean_180    False\n",
              "Length: 93, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqvStQ63GNdb",
        "colab_type": "code",
        "outputId": "a52117f0-2b39-45f7-8999-647f56fdf4af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#####Borura\n",
        "# 全部の特徴量で学習\n",
        "model = lgb.LGBMRegressor(objective='tweedie',\n",
        "                          boosting_type=\"gbdt\",\n",
        "                          tweedie_variance_power=1.1,\n",
        "                          subsample=0.5,\n",
        "                          subsample_freq=1,\n",
        "                          min_data_in_leaf=2**12-1,\n",
        "                          feature_fraction=0.5,\n",
        "                          max_bin=100,\n",
        "                          boost_from_average=False,\n",
        "\n",
        "                                num_leaves =2**11-1,\n",
        "                                learning_rate=0.03,\n",
        "                                n_estimators=300,\n",
        "                                metric=\"rmse\",\n",
        "                                eval_set=[[X_valid.values, y_valid.values]]\n",
        "                          )\n",
        "model.fit(X_train.values, y_train.values)\n",
        "y_pred=model.predict(X_train)\n",
        "print('SCORE with ALL Features: %1.2f\\n' % np.sqrt(mean_squared_error(y_train,model.predict(X_train))))\n",
        "print('SCORE with ALL Features: %1.2f\\n' % np.sqrt(mean_squared_error(y_valid,model.predict(X_valid))))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORE with ALL Features: 2.49\n",
            "\n",
            "SCORE with ALL Features: 2.01\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy6mmjtUDHtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiiEpwL2YiC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8297475-50c8-4e05-e6b0-7b5f9caeda30"
      },
      "source": [
        "###ボルタで選択\n",
        "model = lgb.LGBMRegressor(objective='tweedie',\n",
        "                          boosting_type=\"gbdt\",\n",
        "                          tweedie_variance_power=1.1,\n",
        "                          subsample=0.5,\n",
        "                          subsample_freq=1,\n",
        "                          min_data_in_leaf=2**12-1,\n",
        "                          feature_fraction=0.5,\n",
        "                          max_bin=100,\n",
        "                          boost_from_average=False,\n",
        "\n",
        "                                num_leaves =2**11-1,\n",
        "                                learning_rate=0.03,\n",
        "                                n_estimators=300,\n",
        "                                metric=\"rmse\",\n",
        "                                eval_set=[[X_valid.values, y_valid.values]]\n",
        "                          )\n",
        "\n",
        "feat_selector = BorutaPyForLGB(model, n_estimators='auto', two_step=True,max_iter=500,perc=80,verbose=2, random_state=42)\n",
        "feat_selector.fit(X_train.values, y_train.values)\n",
        "print(X_train.columns[feat_selector.support_])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t1 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t93\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t93\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t93\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t93\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t93\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t93\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t93\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 500\n",
            "Confirmed: \t61\n",
            "Tentative: \t5\n",
            "Rejected: \t27\n",
            "Iteration: \t9 / 500\n",
            "Confirmed: \t61\n",
            "Tentative: \t5\n",
            "Rejected: \t27\n",
            "Iteration: \t10 / 500\n",
            "Confirmed: \t61\n",
            "Tentative: \t5\n",
            "Rejected: \t27\n",
            "Iteration: \t11 / 500\n",
            "Confirmed: \t61\n",
            "Tentative: \t5\n",
            "Rejected: \t27\n",
            "Iteration: \t12 / 500\n",
            "Confirmed: \t62\n",
            "Tentative: \t4\n",
            "Rejected: \t27\n",
            "Iteration: \t13 / 500\n",
            "Confirmed: \t62\n",
            "Tentative: \t4\n",
            "Rejected: \t27\n",
            "Iteration: \t14 / 500\n",
            "Confirmed: \t62\n",
            "Tentative: \t4\n",
            "Rejected: \t27\n",
            "Iteration: \t15 / 500\n",
            "Confirmed: \t62\n",
            "Tentative: \t4\n",
            "Rejected: \t27\n",
            "Iteration: \t16 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t17 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t18 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t19 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t20 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t21 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t22 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t23 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t24 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t25 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t26 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t27 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t28 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t29 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t30 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t31 / 500\n",
            "Confirmed: \t63\n",
            "Tentative: \t3\n",
            "Rejected: \t27\n",
            "Iteration: \t32 / 500\n",
            "Confirmed: \t65\n",
            "Tentative: \t1\n",
            "Rejected: \t27\n",
            "Iteration: \t33 / 500\n",
            "Confirmed: \t65\n",
            "Tentative: \t1\n",
            "Rejected: \t27\n",
            "Iteration: \t34 / 500\n",
            "Confirmed: \t65\n",
            "Tentative: \t0\n",
            "Rejected: \t28\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t35 / 500\n",
            "Confirmed: \t65\n",
            "Tentative: \t0\n",
            "Rejected: \t28\n",
            "Index(['item_id', 'dept_id', 'release', 'sell_price', 'price_max', 'price_min',\n",
            "       'price_std', 'price_mean', 'price_norm', 'item_nunique',\n",
            "       'price_momentum_m', 'price_momentum_y', 'event_type_1', 'tm_d', 'tm_w',\n",
            "       'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_dept_id_mean',\n",
            "       'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std',\n",
            "       'sales_lag_28', 'sales_lag_35', 'rolling_mean_7', 'rolling_std_7',\n",
            "       'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30',\n",
            "       'rolling_std_30', 'rolling_mean_60', 'rolling_std_60',\n",
            "       'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7',\n",
            "       'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30',\n",
            "       'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7',\n",
            "       'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30',\n",
            "       'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7',\n",
            "       'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30',\n",
            "       'rolling_mean_tmp_14_60', 'snap', 'Event_total', 'Event_total',\n",
            "       'Event_total', 'sales_lag_28_364', 'rolling_mean_7_364',\n",
            "       'rolling_std_7_364', 'rolling_mean_14_364', 'rolling_std_14_364',\n",
            "       'rolling_mean_30_364', 'rolling_std_30_364', 'holiday',\n",
            "       'rolling_holiday_mean_7', 'rolling_holiday_mean_14',\n",
            "       'rolling_holiday_mean_30', 'rolling_holiday_mean_60',\n",
            "       'rolling_holiday_mean_180'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2xDIktpadUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d286aba7-4282-4e9b-f98f-8d2d45c00d5e"
      },
      "source": [
        " # 選択したFeatureを取り出し\n",
        "X_train_selected = X_train.iloc[:,feat_selector.support_]\n",
        "X_valid_selected = X_valid.iloc[:,feat_selector.support_]\n",
        "print(X_valid_selected.head())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         item_id  dept_id  ...  rolling_holiday_mean_60  rolling_holiday_mean_180\n",
            "4617523     1437        3  ...                 0.897461                  0.716797\n",
            "4617524     1438        3  ...                 0.153809                  0.324951\n",
            "4617525     1439        3  ...                 0.205078                  0.583496\n",
            "4617526     1440        3  ...                 1.692383                  1.458008\n",
            "4617527     1441        3  ...                 0.743652                  1.083008\n",
            "\n",
            "[5 rows x 65 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24e48hBVez38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d6cb52a2-5da0-4a88-8646-776ec9e1337d"
      },
      "source": [
        "feature_df = pd.DataFrame(X_train.columns.tolist(), columns=['features'])\n",
        "feature_df['rank']=feat_selector.ranking_\n",
        "feature_df = feature_df.sort_values('rank', ascending=True).reset_index(drop=True)\n",
        "print ('\\n Top %d features:' % feat_selector.n_features_)\n",
        "print (feature_df.head(feat_selector.n_features_))\n",
        "feature_df.to_csv('boruta-feature-ranking.csv', index=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Top 65 features:\n",
            "                 features  rank\n",
            "0                 item_id     1\n",
            "1   rolling_mean_tmp_7_30     1\n",
            "2   rolling_mean_tmp_7_14     1\n",
            "3    rolling_mean_tmp_7_7     1\n",
            "4   rolling_mean_tmp_1_60     1\n",
            "..                    ...   ...\n",
            "60       enc_dept_id_mean     1\n",
            "61        enc_dept_id_std     1\n",
            "62       enc_item_id_mean     1\n",
            "63        enc_item_id_std     1\n",
            "64           sales_lag_28     1\n",
            "\n",
            "[65 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQRc3cLleASe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5jgE21Zanhg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "19ff0841-4962-43dc-fb49-dac008860b5e"
      },
      "source": [
        "####選択した特徴量で学習\n",
        "model = lgb.LGBMRegressor(objective='tweedie',\n",
        "                          boosting_type=\"gbdt\",\n",
        "                          tweedie_variance_power=1.1,\n",
        "                          subsample=0.5,\n",
        "                          subsample_freq=1,\n",
        "                          min_data_in_leaf=2**12-1,\n",
        "                          feature_fraction=0.5,\n",
        "                          max_bin=100,\n",
        "                          boost_from_average=False,\n",
        "\n",
        "                                num_leaves =2**11-1,\n",
        "                                learning_rate=0.03,\n",
        "                                n_estimators=300,\n",
        "                                metric=\"rmse\",\n",
        "                                eval_set=[[X_valid_selected.values, y_valid.values]]\n",
        "                          )\n",
        "model.fit(X_train_selected.values, y_train.values)\n",
        "print('SCORE with ALL Features: %1.2f\\n' % np.sqrt(mean_squared_error(y_train,model.predict(X_train_selected))))\n",
        "print('SCORE with ALL Features: %1.2f\\n' % np.sqrt(mean_squared_error(y_valid,model.predict(X_valid_selected))))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORE with ALL Features: 2.49\n",
            "\n",
            "SCORE with ALL Features: 2.02\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVn6OlqKoK63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ボルタ2回目"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0YlqDsgL1A6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bba54060-c88b-44da-e4b0-a2313b463446"
      },
      "source": [
        "###ボルタで選択\n",
        "model = lgb.LGBMRegressorr(objective='tweedie',\n",
        "                          boosting_type=\"gbdt\",\n",
        "                          tweedie_variance_power=1.1,\n",
        "                          subsample=0.5,\n",
        "                          subsample_freq=1,\n",
        "                          min_data_in_leaf=2**12-1,\n",
        "                          feature_fraction=0.5,\n",
        "                          max_bin=100,\n",
        "                          boost_from_average=False,\n",
        "\n",
        "                                num_leaves =2**11-1,\n",
        "                                learning_rate=0.03,\n",
        "                                n_estimators=300,\n",
        "                                metric=\"rmse\",\n",
        "                                eval_set=[[X_valid_selected.values, y_valid.values]]\n",
        "                          )\n",
        "\n",
        "feat_selector2 = BorutaPyForLGB(model, n_estimators='auto', two_step=True,max_iter=500,perc=80,verbose=2, random_state=42)\n",
        "feat_selector2.fit(X_train_selected.values, y_train.values)\n",
        "print(X_train.columns[feat_selector.support_])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t1 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t68\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t68\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t68\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t68\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t68\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t68\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 500\n",
            "Confirmed: \t0\n",
            "Tentative: \t68\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 500\n",
            "Confirmed: \t61\n",
            "Tentative: \t0\n",
            "Rejected: \t7\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t9 / 500\n",
            "Confirmed: \t61\n",
            "Tentative: \t0\n",
            "Rejected: \t7\n",
            "Index(['item_id', 'dept_id', 'release', 'sell_price', 'price_max', 'price_min',\n",
            "       'price_std', 'price_mean', 'price_nunique', 'item_nunique',\n",
            "       'price_momentum', 'price_momentum_m', 'price_momentum_y',\n",
            "       'event_name_1', 'event_type_1', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_dw',\n",
            "       'tm_w_end', 'enc_cat_id_mean', 'enc_dept_id_mean', 'enc_dept_id_std',\n",
            "       'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29',\n",
            "       'sales_lag_33', 'sales_lag_35', 'sales_lag_42', 'rolling_mean_7',\n",
            "       'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30',\n",
            "       'rolling_std_30', 'rolling_mean_60', 'rolling_std_60',\n",
            "       'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7',\n",
            "       'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30',\n",
            "       'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7',\n",
            "       'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30',\n",
            "       'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7',\n",
            "       'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30',\n",
            "       'rolling_mean_tmp_14_60', 'snap', 'Event_total', 'sales_lag_28_364',\n",
            "       'rolling_mean_7_364', 'rolling_std_7_364', 'rolling_mean_14_364',\n",
            "       'rolling_std_14_364', 'rolling_mean_30_364', 'rolling_std_30_364',\n",
            "       'holiday', 'rolling_holiday_mean_7', 'rolling_holiday_mean_14',\n",
            "       'rolling_holiday_mean_30', 'rolling_holiday_mean_60',\n",
            "       'rolling_holiday_mean_180'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgZZmP5dMNSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ad836a8a-5fb4-4892-ee69-de1cfc161cb6"
      },
      "source": [
        " # 選択したFeatureを取り出し\n",
        "X_train_selected2 = X_train.iloc[:,feat_selector.support_]\n",
        "X_valid_selected2 = X_valid.iloc[:,feat_selector.support_]\n",
        "print(X_valid_selected2.head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         item_id  dept_id  ...  rolling_holiday_mean_60  rolling_holiday_mean_180\n",
            "4617523     1437        3  ...                 0.897461                  0.716797\n",
            "4617524     1438        3  ...                 0.153809                  0.324951\n",
            "4617525     1439        3  ...                 0.205078                  0.583496\n",
            "4617526     1440        3  ...                 1.692383                  1.458008\n",
            "4617527     1441        3  ...                 0.743652                  1.083008\n",
            "\n",
            "[5 rows x 68 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygoSJ9cTMR1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6d39c409-1d97-4d2c-fb33-db29cec34861"
      },
      "source": [
        "####選択した特徴量で学習\n",
        "model = lgb.LGBMRegressor(objective='tweedie',\n",
        "                          boosting_type=\"gbdt\",\n",
        "                          tweedie_variance_power=1.1,\n",
        "                          subsample=0.5,\n",
        "                          subsample_freq=1,\n",
        "                          min_data_in_leaf=2**12-1,\n",
        "                          feature_fraction=0.5,\n",
        "                          max_bin=100,\n",
        "                          boost_from_average=False,\n",
        "\n",
        "                                num_leaves =2**11-1,\n",
        "                                learning_rate=0.03,\n",
        "                                n_estimators=1400,\n",
        "                                metric=\"rmse\",\n",
        "                                eval_set=[[X_valid_selected2.values, y_valid.values]]\n",
        "                          )\n",
        "model.fit(X_train_selected2.values, y_train.values)\n",
        "print('SCORE with ALL Features: %1.2f\\n' % np.sqrt(mean_squared_error(y_train,model.predict(X_train_selected2))))\n",
        "print('SCORE with ALL Features: %1.2f\\n' % np.sqrt(mean_squared_error(y_valid,model.predict(X_valid_selected2))))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORE with ALL Features: 2.39\n",
            "\n",
            "SCORE with ALL Features: 2.01\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-0XqDrYMh0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuByqdjOVuIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat_selector = BorutaPy(model, \n",
        "                         n_estimators='auto',  # 特徴量の数に比例して、木の本数を増やす\n",
        "                         verbose=2, # 0: no output,1: displays iteration number,2: which features have been selected already\n",
        "                         alpha=0.05, # 有意水準\n",
        "                         max_iter=50, # 試行回数\n",
        "                         random_state=1\n",
        "                        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqOWdUcZGSWh",
        "colab_type": "code",
        "outputId": "c856d706-d9e4-453c-dbe8-47f82503a510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# RandomForestRegressorでBorutaを実行\n",
        "feat_selector.fit(X.values, y.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t1 / 50\n",
            "Confirmed: \t0\n",
            "Tentative: \t88\n",
            "Rejected: \t0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t2 / 50\n",
            "Confirmed: \t0\n",
            "Tentative: \t88\n",
            "Rejected: \t0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t3 / 50\n",
            "Confirmed: \t0\n",
            "Tentative: \t88\n",
            "Rejected: \t0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t4 / 50\n",
            "Confirmed: \t0\n",
            "Tentative: \t88\n",
            "Rejected: \t0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t5 / 50\n",
            "Confirmed: \t0\n",
            "Tentative: \t88\n",
            "Rejected: \t0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t6 / 50\n",
            "Confirmed: \t0\n",
            "Tentative: \t88\n",
            "Rejected: \t0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t7 / 50\n",
            "Confirmed: \t0\n",
            "Tentative: \t88\n",
            "Rejected: \t0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:  2.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t8 / 50\n",
            "Confirmed: \t50\n",
            "Tentative: \t20\n",
            "Rejected: \t18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 169 out of 169 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t9 / 50\n",
            "Confirmed: \t50\n",
            "Tentative: \t20\n",
            "Rejected: \t18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 169 out of 169 | elapsed:  2.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t10 / 50\n",
            "Confirmed: \t50\n",
            "Tentative: \t20\n",
            "Rejected: \t18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 169 out of 169 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t11 / 50\n",
            "Confirmed: \t50\n",
            "Tentative: \t20\n",
            "Rejected: \t18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 169 out of 169 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t12 / 50\n",
            "Confirmed: \t51\n",
            "Tentative: \t19\n",
            "Rejected: \t18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 169 out of 169 | elapsed:  2.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t13 / 50\n",
            "Confirmed: \t51\n",
            "Tentative: \t15\n",
            "Rejected: \t22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 164 out of 164 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t14 / 50\n",
            "Confirmed: \t51\n",
            "Tentative: \t15\n",
            "Rejected: \t22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 164 out of 164 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t15 / 50\n",
            "Confirmed: \t51\n",
            "Tentative: \t15\n",
            "Rejected: \t22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 164 out of 164 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t16 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t14\n",
            "Rejected: \t22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 164 out of 164 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t17 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t18 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t19 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t20 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t21 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t22 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t23 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t24 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t25 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t12\n",
            "Rejected: \t24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t26 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t10\n",
            "Rejected: \t26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t27 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t10\n",
            "Rejected: \t26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t28 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t10\n",
            "Rejected: \t26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t29 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t10\n",
            "Rejected: \t26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t30 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t10\n",
            "Rejected: \t26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t31 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t10\n",
            "Rejected: \t26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t32 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t10\n",
            "Rejected: \t26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t33 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t34 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t35 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t36 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t37 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t38 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t39 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t40 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t41 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t42 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t43 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t44 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t45 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t46 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t47 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t48 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: \t49 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t50 / 50\n",
            "Confirmed: \t52\n",
            "Tentative: \t4\n",
            "Rejected: \t27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(alpha=0.05,\n",
              "         estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
              "                                         criterion='mse', max_depth=7,\n",
              "                                         max_features='sqrt',\n",
              "                                         max_leaf_nodes=None, max_samples=None,\n",
              "                                         min_impurity_decrease=0.0,\n",
              "                                         min_impurity_split=None,\n",
              "                                         min_samples_leaf=1,\n",
              "                                         min_samples_split=2,\n",
              "                                         min_weight_fraction_leaf=0.0,\n",
              "                                         n_estimators=157, n_jobs=-1,\n",
              "                                         oob_score=False,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F749DF49DB0,\n",
              "                                         verbose=True, warm_start=False),\n",
              "         max_iter=50, n_estimators='auto', perc=100,\n",
              "         random_state=RandomState(MT19937) at 0x7F749DF49DB0, two_step=True,\n",
              "         verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZwclihSGd1x",
        "colab_type": "code",
        "outputId": "80e12e70-8d1a-4ad3-9ee4-449ef0092e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "# 選択された特徴量を確認\n",
        "selected = feat_selector.support_\n",
        "print('選択された特徴量の数: %d' % np.sum(selected))\n",
        "print(selected)\n",
        "print(X.columns[selected])\n",
        "print(X.columns[~selected])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "選択された特徴量の数: 52\n",
            "[False False False False  True False False False False  True False False\n",
            " False  True  True False False False False False  True  True False False\n",
            " False False  True  True  True  True  True  True False False  True  True\n",
            "  True False False False False  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True False False False False False False False\n",
            " False False  True  True  True  True  True  True  True  True  True  True\n",
            " False  True  True  True]\n",
            "Index(['sell_price', 'price_norm', 'price_momentum_m', 'price_momentum_y',\n",
            "       'tm_dw', 'tm_w_end', 'enc_item_id_mean', 'enc_item_id_std',\n",
            "       'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31',\n",
            "       'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_41',\n",
            "       'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14',\n",
            "       'rolling_std_14', 'rolling_mean_30', 'rolling_std_30',\n",
            "       'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180',\n",
            "       'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14',\n",
            "       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n",
            "       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n",
            "       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n",
            "       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n",
            "       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'sales_lag_28_364',\n",
            "       'sales_lag_35_364', 'rolling_mean_7_364', 'rolling_std_7_364',\n",
            "       'rolling_mean_14_364', 'rolling_std_14_364', 'rolling_mean_30_364',\n",
            "       'rolling_std_30_364', 'lag_28_35_mean_364', 'holiday',\n",
            "       'rolling_mean_holiday_t7', 'rolling_mean_holiday_t28',\n",
            "       'rolling_mean_holiday_t56'],\n",
            "      dtype='object')\n",
            "Index(['item_id', 'dept_id', 'cat_id', 'release', 'price_max', 'price_min',\n",
            "       'price_std', 'price_mean', 'price_nunique', 'item_nunique',\n",
            "       'price_momentum', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm',\n",
            "       'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean',\n",
            "       'enc_dept_id_std', 'sales_lag_32', 'sales_lag_33', 'sales_lag_37',\n",
            "       'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'snap', 'Event_total',\n",
            "       'Event_total', 'Event_total', 'Event_total', 'Event_total',\n",
            "       'Event_total', 'Event_total', 'Event_total', 'wc'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXtFkQK4GenS",
        "colab_type": "code",
        "outputId": "e3053541-99af-4727-82ca-c899fbe73592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# 選択した特徴量で学習\n",
        "X_selected = X[X.columns[selected]]\n",
        "rf2 = RandomForestRegressor(\n",
        "    n_estimators=50\n",
        "    , criterion='mse'\n",
        "    , max_depth = 7\n",
        "    , max_features = 'sqrt' \n",
        "    , n_jobs=-1\n",
        "    , verbose=True\n",
        "    )\n",
        "rf2.fit(X_selected, y)\n",
        "y_pred=rf2.predict(X_selected)\n",
        "\n",
        "print('RMSE: %1.2f\\n' % np.sqrt(mean_squared_error(y, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  22 out of  50 | elapsed:   21.7s remaining:   27.6s\n",
            "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   31.5s finished\n",
            "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=40)]: Done  22 out of  50 | elapsed:    0.5s remaining:    0.6s\n",
            "[Parallel(n_jobs=40)]: Done  50 out of  50 | elapsed:    0.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RMSE: 2.44\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61rzWwz3PvFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Aux Models\n",
        "# If you don't want to wait hours and hours\n",
        "# to have result you can train each store \n",
        "# in separate kernel and then just join result.\n",
        "\n",
        "# If we want to use pretrained models we can \n",
        "## skip training \n",
        "## (in our case do dummy training\n",
        "##  to show that we are good with memory\n",
        "##  and you can safely use this (all kernel) code)\n",
        "if USE_AUX:\n",
        "    lgb_params['n_estimators'] = 2\n",
        "    \n",
        "# Here is some 'logs' that can compare\n",
        "#Train CA_1\n",
        "#[100]\tvalid_0's rmse: 2.02289\n",
        "#[200]\tvalid_0's rmse: 2.0017\n",
        "#[300]\tvalid_0's rmse: 1.99239\n",
        "#[400]\tvalid_0's rmse: 1.98471\n",
        "#[500]\tvalid_0's rmse: 1.97923\n",
        "#[600]\tvalid_0's rmse: 1.97284\n",
        "#[700]\tvalid_0's rmse: 1.96763\n",
        "#[800]\tvalid_0's rmse: 1.9624\n",
        "#[900]\tvalid_0's rmse: 1.95673\n",
        "#[1000]\tvalid_0's rmse: 1.95201\n",
        "#[1100]\tvalid_0's rmse: 1.9476\n",
        "#[1200]\tvalid_0's rmse: 1.9434\n",
        "#[1300]\tvalid_0's rmse: 1.9392\n",
        "#[1400]\tvalid_0's rmse: 1.93446\n",
        "\n",
        "#Train CA_2\n",
        "#[100]\tvalid_0's rmse: 1.88949\n",
        "#[200]\tvalid_0's rmse: 1.84767\n",
        "#[300]\tvalid_0's rmse: 1.83653\n",
        "#[400]\tvalid_0's rmse: 1.82909\n",
        "#[500]\tvalid_0's rmse: 1.82265\n",
        "#[600]\tvalid_0's rmse: 1.81725\n",
        "#[700]\tvalid_0's rmse: 1.81252\n",
        "#[800]\tvalid_0's rmse: 1.80736\n",
        "#[900]\tvalid_0's rmse: 1.80242\n",
        "#[1000]\tvalid_0's rmse: 1.79821\n",
        "#[1100]\tvalid_0's rmse: 1.794\n",
        "#[1200]\tvalid_0's rmse: 1.78973\n",
        "#[1300]\tvalid_0's rmse: 1.78552\n",
        "#[1400]\tvalid_0's rmse: 1.78158"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJJPOV1IPvF3",
        "colab_type": "code",
        "outputId": "36f5f228-98d8-4a2c-cc05-8b085d45a262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "########################### Train Models\n",
        "#################################################################################\n",
        "for store_id in STORES_IDS:\n",
        "    print('Train', store_id)\n",
        "    \n",
        "    # Get grid for current store\n",
        "    grid_df, features_columns = get_data_by_store(store_id)\n",
        "    \n",
        "    # Masks for \n",
        "    # Train (All data less than 1913)\n",
        "    # \"Validation\" (Last 28 days - not real validatio set)\n",
        "    # Test (All data greater than 1913 day, \n",
        "    #       with some gap for recursive features)\n",
        "    train_mask = grid_df['d']<=END_TRAIN\n",
        "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
        "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
        "    \n",
        "    # Apply masks and save lgb dataset as bin\n",
        "    # to reduce memory spikes during dtype convertations\n",
        "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
        "    # \"To avoid any conversions, you should always use np.float32\"\n",
        "    # or save to bin before start training\n",
        "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
        "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
        "                       label=grid_df[train_mask][TARGET])\n",
        "    train_data.save_binary('train_data.bin')\n",
        "    train_data = lgb.Dataset('train_data.bin')\n",
        "    \n",
        "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
        "                       label=grid_df[valid_mask][TARGET])\n",
        "    \n",
        "    # Saving part of the dataset for later predictions\n",
        "    # Removing features that we need to calculate recursively \n",
        "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
        "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
        "    grid_df = grid_df[keep_cols]\n",
        "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
        "    del grid_df\n",
        "    \n",
        "    # Launch seeder again to make lgb training 100% deterministic\n",
        "    # with each \"code line\" np.random \"evolves\" \n",
        "    # so we need (may want) to \"reset\" it\n",
        "    seed_everything(SEED)\n",
        "    estimator = lgb.train(lgb_params,\n",
        "                          train_data,\n",
        "                          valid_sets = [valid_data],\n",
        "                          verbose_eval = 100,\n",
        "                          )\n",
        "    \n",
        "    # Save model - it's not real '.bin' but a pickle file\n",
        "    # estimator = lgb.Booster(model_file='model.txt')\n",
        "    # can only predict with the best iteration (or the saving iteration)\n",
        "    # pickle.dump gives us more flexibility\n",
        "    # like estimator.predict(TEST, num_iteration=100)\n",
        "    # num_iteration - number of iteration want to predict with, \n",
        "    # NULL or <= 0 means use best iteration\n",
        "    model_name = AUX_MODELS+'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
        "    pickle.dump(estimator, open(model_name, 'wb'))\n",
        "\n",
        "    # Remove temporary files and objects \n",
        "    # to free some hdd space and ram memory\n",
        "    !rm train_data.bin\n",
        "    del train_data, valid_data, estimator\n",
        "    gc.collect()\n",
        "    \n",
        "    # \"Keep\" models features for predictions\n",
        "    MODEL_FEATURES = features_columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train CA_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyY4qfmmPvF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZE0QJMyPvGB",
        "colab_type": "code",
        "outputId": "e74c851c-b6f7-47bf-c85d-665671b8aa53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########################### Predict\n",
        "#################################################################################\n",
        "\n",
        "# Create Dummy DataFrame to store predictions\n",
        "all_preds = pd.DataFrame()\n",
        "\n",
        "# Join back the Test dataset with \n",
        "# a small part of the training data \n",
        "# to make recursive features\n",
        "base_test = get_base_test()\n",
        "\n",
        "# Timer to measure predictions time \n",
        "main_time = time.time()\n",
        "\n",
        "# Loop over each prediction day\n",
        "# As rolling lags are the most timeconsuming\n",
        "# we will calculate it for whole day\n",
        "for PREDICT_DAY in range(1,29):    \n",
        "    print('Predict | Day:', PREDICT_DAY)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Make temporary grid to calculate rolling lags\n",
        "    grid_df = base_test.copy()\n",
        "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "        \n",
        "    for store_id in STORES_IDS:\n",
        "        \n",
        "        # Read all our models and make predictions\n",
        "        # for each day/store pairs\n",
        "        model_path = AUX_MODELS + 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
        "        if USE_AUX:\n",
        "            model_path = AUX_MODELS + model_path\n",
        "        \n",
        "        estimator = pickle.load(open(model_path, 'rb'))\n",
        "        \n",
        "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
        "        store_mask = base_test['store_id']==store_id\n",
        "        \n",
        "        mask = (day_mask)&(store_mask)\n",
        "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "    \n",
        "    # Make good column naming and add \n",
        "    # to all_preds DataFrame\n",
        "    temp_df = base_test[day_mask][['id',TARGET]]\n",
        "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "    if 'id' in list(all_preds):\n",
        "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "    else:\n",
        "        all_preds = temp_df.copy()\n",
        "        \n",
        "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    del temp_df\n",
        "    \n",
        "all_preds = all_preds.reset_index(drop=True)\n",
        "all_preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict | Day: 1\n",
            "##########  0.89 min round |  0.89 min total |  37443.86 day sales |\n",
            "Predict | Day: 2\n",
            "##########  0.65 min round |  1.54 min total |  35370.47 day sales |\n",
            "Predict | Day: 3\n",
            "##########  0.64 min round |  2.18 min total |  34752.00 day sales |\n",
            "Predict | Day: 4\n",
            "##########  0.64 min round |  2.82 min total |  35318.04 day sales |\n",
            "Predict | Day: 5\n",
            "##########  0.64 min round |  3.46 min total |  41597.11 day sales |\n",
            "Predict | Day: 6\n",
            "##########  0.64 min round |  4.10 min total |  52161.03 day sales |\n",
            "Predict | Day: 7\n",
            "##########  0.64 min round |  4.74 min total |  54737.23 day sales |\n",
            "Predict | Day: 8\n",
            "##########  0.64 min round |  5.38 min total |  44382.08 day sales |\n",
            "Predict | Day: 9\n",
            "##########  0.64 min round |  6.02 min total |  44672.14 day sales |\n",
            "Predict | Day: 10\n",
            "##########  0.65 min round |  6.67 min total |  39121.28 day sales |\n",
            "Predict | Day: 11\n",
            "##########  0.64 min round |  7.31 min total |  39305.85 day sales |\n",
            "Predict | Day: 12\n",
            "##########  0.65 min round |  7.96 min total |  46085.61 day sales |\n",
            "Predict | Day: 13\n",
            "##########  0.64 min round |  8.60 min total |  54108.95 day sales |\n",
            "Predict | Day: 14\n",
            "##########  0.64 min round |  9.24 min total |  48040.34 day sales |\n",
            "Predict | Day: 15\n",
            "##########  0.64 min round |  9.88 min total |  44989.59 day sales |\n",
            "Predict | Day: 16\n",
            "##########  0.64 min round |  10.53 min total |  39708.50 day sales |\n",
            "Predict | Day: 17\n",
            "##########  0.64 min round |  11.17 min total |  40602.43 day sales |\n",
            "Predict | Day: 18\n",
            "##########  0.65 min round |  11.82 min total |  40994.58 day sales |\n",
            "Predict | Day: 19\n",
            "##########  0.65 min round |  12.47 min total |  44001.89 day sales |\n",
            "Predict | Day: 20\n",
            "##########  0.64 min round |  13.11 min total |  53769.68 day sales |\n",
            "Predict | Day: 21\n",
            "##########  0.64 min round |  13.76 min total |  55629.90 day sales |\n",
            "Predict | Day: 22\n",
            "##########  0.64 min round |  14.40 min total |  42264.35 day sales |\n",
            "Predict | Day: 23\n",
            "##########  0.65 min round |  15.04 min total |  38234.77 day sales |\n",
            "Predict | Day: 24\n",
            "##########  0.64 min round |  15.69 min total |  37334.49 day sales |\n",
            "Predict | Day: 25\n",
            "##########  0.64 min round |  16.33 min total |  37137.90 day sales |\n",
            "Predict | Day: 26\n",
            "##########  0.64 min round |  16.97 min total |  41917.89 day sales |\n",
            "Predict | Day: 27\n",
            "##########  0.64 min round |  17.61 min total |  50844.68 day sales |\n",
            "Predict | Day: 28\n",
            "##########  0.64 min round |  18.26 min total |  51402.68 day sales |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>F9</th>\n",
              "      <th>F10</th>\n",
              "      <th>F11</th>\n",
              "      <th>F12</th>\n",
              "      <th>F13</th>\n",
              "      <th>F14</th>\n",
              "      <th>F15</th>\n",
              "      <th>F16</th>\n",
              "      <th>F17</th>\n",
              "      <th>F18</th>\n",
              "      <th>F19</th>\n",
              "      <th>F20</th>\n",
              "      <th>F21</th>\n",
              "      <th>F22</th>\n",
              "      <th>F23</th>\n",
              "      <th>F24</th>\n",
              "      <th>F25</th>\n",
              "      <th>F26</th>\n",
              "      <th>F27</th>\n",
              "      <th>F28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
              "      <td>0.801803</td>\n",
              "      <td>0.723683</td>\n",
              "      <td>0.720562</td>\n",
              "      <td>0.762624</td>\n",
              "      <td>0.960074</td>\n",
              "      <td>1.126830</td>\n",
              "      <td>1.161501</td>\n",
              "      <td>0.846897</td>\n",
              "      <td>0.911767</td>\n",
              "      <td>0.834176</td>\n",
              "      <td>0.746047</td>\n",
              "      <td>0.949804</td>\n",
              "      <td>1.120835</td>\n",
              "      <td>0.938112</td>\n",
              "      <td>0.856383</td>\n",
              "      <td>0.791389</td>\n",
              "      <td>0.787631</td>\n",
              "      <td>0.735187</td>\n",
              "      <td>0.810740</td>\n",
              "      <td>1.098141</td>\n",
              "      <td>1.063745</td>\n",
              "      <td>0.814246</td>\n",
              "      <td>0.782200</td>\n",
              "      <td>0.739718</td>\n",
              "      <td>0.801988</td>\n",
              "      <td>0.955650</td>\n",
              "      <td>1.096880</td>\n",
              "      <td>1.022920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
              "      <td>0.168957</td>\n",
              "      <td>0.169508</td>\n",
              "      <td>0.166854</td>\n",
              "      <td>0.179415</td>\n",
              "      <td>0.200086</td>\n",
              "      <td>0.248333</td>\n",
              "      <td>0.286910</td>\n",
              "      <td>0.230361</td>\n",
              "      <td>0.220693</td>\n",
              "      <td>0.194009</td>\n",
              "      <td>0.184153</td>\n",
              "      <td>0.224189</td>\n",
              "      <td>0.271640</td>\n",
              "      <td>0.249853</td>\n",
              "      <td>0.270110</td>\n",
              "      <td>0.243126</td>\n",
              "      <td>0.224538</td>\n",
              "      <td>0.211191</td>\n",
              "      <td>0.222313</td>\n",
              "      <td>0.302337</td>\n",
              "      <td>0.319417</td>\n",
              "      <td>0.232647</td>\n",
              "      <td>0.209414</td>\n",
              "      <td>0.195903</td>\n",
              "      <td>0.208283</td>\n",
              "      <td>0.211919</td>\n",
              "      <td>0.293963</td>\n",
              "      <td>0.300624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
              "      <td>0.421205</td>\n",
              "      <td>0.385268</td>\n",
              "      <td>0.428610</td>\n",
              "      <td>0.409224</td>\n",
              "      <td>0.588067</td>\n",
              "      <td>0.772068</td>\n",
              "      <td>0.719824</td>\n",
              "      <td>0.498321</td>\n",
              "      <td>0.461623</td>\n",
              "      <td>0.456616</td>\n",
              "      <td>0.416941</td>\n",
              "      <td>0.606072</td>\n",
              "      <td>0.686885</td>\n",
              "      <td>0.526176</td>\n",
              "      <td>0.447994</td>\n",
              "      <td>0.435795</td>\n",
              "      <td>0.412512</td>\n",
              "      <td>0.480216</td>\n",
              "      <td>0.567431</td>\n",
              "      <td>0.688343</td>\n",
              "      <td>0.674192</td>\n",
              "      <td>0.479738</td>\n",
              "      <td>0.427863</td>\n",
              "      <td>0.446494</td>\n",
              "      <td>0.436994</td>\n",
              "      <td>0.552482</td>\n",
              "      <td>0.683273</td>\n",
              "      <td>0.664283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
              "      <td>1.569804</td>\n",
              "      <td>1.263994</td>\n",
              "      <td>1.293363</td>\n",
              "      <td>1.424997</td>\n",
              "      <td>1.819924</td>\n",
              "      <td>3.211554</td>\n",
              "      <td>3.341809</td>\n",
              "      <td>1.685654</td>\n",
              "      <td>1.450558</td>\n",
              "      <td>1.445097</td>\n",
              "      <td>1.505973</td>\n",
              "      <td>1.817285</td>\n",
              "      <td>3.099209</td>\n",
              "      <td>2.638359</td>\n",
              "      <td>1.672946</td>\n",
              "      <td>1.383769</td>\n",
              "      <td>1.409273</td>\n",
              "      <td>1.348868</td>\n",
              "      <td>1.839778</td>\n",
              "      <td>2.596627</td>\n",
              "      <td>3.379442</td>\n",
              "      <td>1.649361</td>\n",
              "      <td>1.455321</td>\n",
              "      <td>1.357194</td>\n",
              "      <td>1.374343</td>\n",
              "      <td>1.871913</td>\n",
              "      <td>3.126521</td>\n",
              "      <td>3.394666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
              "      <td>0.956261</td>\n",
              "      <td>0.833377</td>\n",
              "      <td>0.859554</td>\n",
              "      <td>0.887796</td>\n",
              "      <td>1.001389</td>\n",
              "      <td>1.360069</td>\n",
              "      <td>1.331363</td>\n",
              "      <td>0.975193</td>\n",
              "      <td>1.002860</td>\n",
              "      <td>0.946965</td>\n",
              "      <td>0.819512</td>\n",
              "      <td>0.979274</td>\n",
              "      <td>1.361654</td>\n",
              "      <td>1.032811</td>\n",
              "      <td>0.904866</td>\n",
              "      <td>0.950282</td>\n",
              "      <td>0.865686</td>\n",
              "      <td>0.906523</td>\n",
              "      <td>1.005970</td>\n",
              "      <td>1.467324</td>\n",
              "      <td>1.546693</td>\n",
              "      <td>1.020751</td>\n",
              "      <td>0.894866</td>\n",
              "      <td>0.912960</td>\n",
              "      <td>0.862583</td>\n",
              "      <td>1.111127</td>\n",
              "      <td>1.470302</td>\n",
              "      <td>1.419945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30485</th>\n",
              "      <td>FOODS_3_823_WI_3_validation</td>\n",
              "      <td>0.324340</td>\n",
              "      <td>0.302619</td>\n",
              "      <td>0.304654</td>\n",
              "      <td>0.297616</td>\n",
              "      <td>0.334883</td>\n",
              "      <td>0.478178</td>\n",
              "      <td>0.441987</td>\n",
              "      <td>0.422721</td>\n",
              "      <td>0.469880</td>\n",
              "      <td>0.346602</td>\n",
              "      <td>0.415468</td>\n",
              "      <td>0.426395</td>\n",
              "      <td>0.478824</td>\n",
              "      <td>0.519497</td>\n",
              "      <td>0.440944</td>\n",
              "      <td>0.374909</td>\n",
              "      <td>0.422686</td>\n",
              "      <td>0.406410</td>\n",
              "      <td>0.390496</td>\n",
              "      <td>0.494215</td>\n",
              "      <td>0.571374</td>\n",
              "      <td>0.417284</td>\n",
              "      <td>0.422543</td>\n",
              "      <td>0.396604</td>\n",
              "      <td>0.352524</td>\n",
              "      <td>0.347221</td>\n",
              "      <td>0.410777</td>\n",
              "      <td>0.465673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30486</th>\n",
              "      <td>FOODS_3_824_WI_3_validation</td>\n",
              "      <td>0.304704</td>\n",
              "      <td>0.262193</td>\n",
              "      <td>0.262746</td>\n",
              "      <td>0.254403</td>\n",
              "      <td>0.292248</td>\n",
              "      <td>0.378496</td>\n",
              "      <td>0.405401</td>\n",
              "      <td>0.439849</td>\n",
              "      <td>0.449078</td>\n",
              "      <td>0.363105</td>\n",
              "      <td>0.384419</td>\n",
              "      <td>0.411730</td>\n",
              "      <td>0.453814</td>\n",
              "      <td>0.409679</td>\n",
              "      <td>0.421899</td>\n",
              "      <td>0.312726</td>\n",
              "      <td>0.363811</td>\n",
              "      <td>0.360991</td>\n",
              "      <td>0.319476</td>\n",
              "      <td>0.470812</td>\n",
              "      <td>0.483289</td>\n",
              "      <td>0.356131</td>\n",
              "      <td>0.284941</td>\n",
              "      <td>0.261170</td>\n",
              "      <td>0.243323</td>\n",
              "      <td>0.264301</td>\n",
              "      <td>0.346882</td>\n",
              "      <td>0.323683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30487</th>\n",
              "      <td>FOODS_3_825_WI_3_validation</td>\n",
              "      <td>0.685554</td>\n",
              "      <td>0.552145</td>\n",
              "      <td>0.482045</td>\n",
              "      <td>0.518565</td>\n",
              "      <td>0.658581</td>\n",
              "      <td>0.745489</td>\n",
              "      <td>0.930951</td>\n",
              "      <td>1.174510</td>\n",
              "      <td>1.135844</td>\n",
              "      <td>0.758195</td>\n",
              "      <td>1.034439</td>\n",
              "      <td>1.241368</td>\n",
              "      <td>1.247778</td>\n",
              "      <td>1.354342</td>\n",
              "      <td>1.434308</td>\n",
              "      <td>0.852096</td>\n",
              "      <td>1.153077</td>\n",
              "      <td>1.179667</td>\n",
              "      <td>0.957669</td>\n",
              "      <td>1.439983</td>\n",
              "      <td>1.500104</td>\n",
              "      <td>1.036766</td>\n",
              "      <td>0.810120</td>\n",
              "      <td>0.686931</td>\n",
              "      <td>0.595563</td>\n",
              "      <td>0.735723</td>\n",
              "      <td>0.884382</td>\n",
              "      <td>0.914370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30488</th>\n",
              "      <td>FOODS_3_826_WI_3_validation</td>\n",
              "      <td>0.912796</td>\n",
              "      <td>0.837028</td>\n",
              "      <td>0.728609</td>\n",
              "      <td>0.753934</td>\n",
              "      <td>0.909931</td>\n",
              "      <td>1.228229</td>\n",
              "      <td>1.150880</td>\n",
              "      <td>1.111630</td>\n",
              "      <td>1.135888</td>\n",
              "      <td>0.889787</td>\n",
              "      <td>1.015968</td>\n",
              "      <td>1.197305</td>\n",
              "      <td>1.223163</td>\n",
              "      <td>1.317606</td>\n",
              "      <td>1.197976</td>\n",
              "      <td>0.991891</td>\n",
              "      <td>1.035252</td>\n",
              "      <td>1.084933</td>\n",
              "      <td>0.982257</td>\n",
              "      <td>1.327508</td>\n",
              "      <td>1.409575</td>\n",
              "      <td>0.969228</td>\n",
              "      <td>0.881856</td>\n",
              "      <td>0.859823</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.932125</td>\n",
              "      <td>1.073434</td>\n",
              "      <td>1.131151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30489</th>\n",
              "      <td>FOODS_3_827_WI_3_validation</td>\n",
              "      <td>0.204027</td>\n",
              "      <td>1.027674</td>\n",
              "      <td>1.152922</td>\n",
              "      <td>1.699066</td>\n",
              "      <td>2.323571</td>\n",
              "      <td>3.273215</td>\n",
              "      <td>2.201739</td>\n",
              "      <td>1.966134</td>\n",
              "      <td>2.732756</td>\n",
              "      <td>2.233406</td>\n",
              "      <td>2.014543</td>\n",
              "      <td>2.196760</td>\n",
              "      <td>2.459357</td>\n",
              "      <td>2.219731</td>\n",
              "      <td>2.242612</td>\n",
              "      <td>2.012937</td>\n",
              "      <td>2.016482</td>\n",
              "      <td>2.016945</td>\n",
              "      <td>1.901291</td>\n",
              "      <td>2.441984</td>\n",
              "      <td>2.417871</td>\n",
              "      <td>1.944286</td>\n",
              "      <td>1.852534</td>\n",
              "      <td>1.719320</td>\n",
              "      <td>1.654637</td>\n",
              "      <td>2.003748</td>\n",
              "      <td>2.300112</td>\n",
              "      <td>2.298636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30490 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id        F1  ...       F27       F28\n",
              "0      HOBBIES_1_001_CA_1_validation  0.801803  ...  1.096880  1.022920\n",
              "1      HOBBIES_1_002_CA_1_validation  0.168957  ...  0.293963  0.300624\n",
              "2      HOBBIES_1_003_CA_1_validation  0.421205  ...  0.683273  0.664283\n",
              "3      HOBBIES_1_004_CA_1_validation  1.569804  ...  3.126521  3.394666\n",
              "4      HOBBIES_1_005_CA_1_validation  0.956261  ...  1.470302  1.419945\n",
              "...                              ...       ...  ...       ...       ...\n",
              "30485    FOODS_3_823_WI_3_validation  0.324340  ...  0.410777  0.465673\n",
              "30486    FOODS_3_824_WI_3_validation  0.304704  ...  0.346882  0.323683\n",
              "30487    FOODS_3_825_WI_3_validation  0.685554  ...  0.884382  0.914370\n",
              "30488    FOODS_3_826_WI_3_validation  0.912796  ...  1.073434  1.131151\n",
              "30489    FOODS_3_827_WI_3_validation  0.204027  ...  2.300112  2.298636\n",
              "\n",
              "[30490 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym40qYzLPvGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Export\n",
        "#################################################################################\n",
        "# Reading competition sample submission and\n",
        "# merging our predictions\n",
        "# As we have predictions only for \"_validation\" data\n",
        "# we need to do fillna() for \"_evaluation\" items\n",
        "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
        "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
        "submission.to_csv(AUX_MODELS+'submission_v'+str(VER)+'.csv', index=False)\n",
        "submission.to_csv('submission_v'+str(VER)+'.csv', index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU0HId86PvGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary\n",
        "\n",
        "# Of course here is no magic at all.\n",
        "# No \"Novel\" features and no brilliant ideas.\n",
        "# We just carefully joined all\n",
        "# our previous fe work and created a model.\n",
        "\n",
        "# Also!\n",
        "# In my opinion this strategy is a \"dead end\".\n",
        "# Overfits a lot LB and with 1 final submission \n",
        "# you have no option to risk.\n",
        "\n",
        "\n",
        "# Improvement should come from:\n",
        "# Loss function\n",
        "# Data representation\n",
        "# Stable CV\n",
        "# Good features reduction strategy\n",
        "# Predictions stabilization with NN\n",
        "# Trend prediction\n",
        "# Real zero sales detection/classification\n",
        "\n",
        "\n",
        "# Good kernels references \n",
        "## (the order is random and the list is not complete):\n",
        "# https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
        "# https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
        "# https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
        "# https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n",
        "# https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n",
        "# https://www.kaggle.com/yassinealouini/seq2seq\n",
        "# https://www.kaggle.com/kailex/m5-forecaster-v2\n",
        "# https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n",
        "\n",
        "\n",
        "# Features were created in these kernels:\n",
        "## \n",
        "# Mean encodings and PCA options\n",
        "# https://www.kaggle.com/kyakovlev/m5-custom-features\n",
        "##\n",
        "# Lags and rolling lags\n",
        "# https://www.kaggle.com/kyakovlev/m5-lags-features\n",
        "##\n",
        "# Base Grid and base features (calendar/price/etc)\n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "\n",
        "\n",
        "# Personal request\n",
        "# Please don't upvote any ensemble and copypaste kernels\n",
        "## The worst case is ensemble without any analyse.\n",
        "## The best choice - just ignore it.\n",
        "## I would like to see more kernels with interesting and original approaches.\n",
        "## Don't feed copypasters with upvotes.\n",
        "\n",
        "## It doesn't mean that you should not fork and improve others kernels\n",
        "## but I would like to see params and code tuning based on some CV and analyse\n",
        "## and not only on LB probing.\n",
        "## Small changes could be shared in comments and authors can improve their kernel.\n",
        "\n",
        "## Feel free to criticize this kernel as my knowlege is very limited\n",
        "## and I can be wrong in code and descriptions. \n",
        "## Thank you."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzgr6gRwU0gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df, features_columns = get_data_by_store(\"CA_1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR8EZRFZk0d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gruRmp9kliLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDEDPNgNlsV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imp=pd.DataFrame({'feature': features_columns,\n",
        "\n",
        " 'importance':estimator.feature_importance()}).sort_values('importance',\n",
        "\n",
        " ascending=False)\n",
        "imp.to_csv(AUX_MODELS+'importance_v'+str(VER)+'.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eMqhKjFTtbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}