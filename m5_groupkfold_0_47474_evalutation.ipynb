{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "m5_groupkfold_0_47474_evalutation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipejun-ai/m5-accuracy/blob/master/m5_groupkfold_0_47474_evalutation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajgjlzvjFUrk",
        "colab_type": "text"
      },
      "source": [
        "Original source of the kernel:\n",
        "* https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic\n",
        "* https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
        "\n",
        "Please give them an upvote.\n",
        "\n",
        "I have only added GroupKFold CV. Please do give your feedback about speeding up the process or any better way for performing CV. Any feedback will be greatly appreciated. All the trained model and submission files can be found here:\n",
        "* https://www.kaggle.com/ramitjaal/m5groupkfold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "7wTsS7CHFUrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "from sklearn.model_selection import GroupKFold\n",
        "# custom imports\n",
        "from multiprocessing import Pool        # Multiprocess Runs\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia_lMZecFaPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf2b725c-880f-4053-bc9a-486d8ce15310"
      },
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-wCN29sFmm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIRPATH=\"/content/gdrive/My Drive/kaggle/m5-forecasting-accuracy/\"\n",
        "#DIRPATH=\"C:/Users/peiju/Documents/Study/kaggle/m5-forecasting-accuracy/\""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGCzDZXzFUrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helpers\n",
        "#################################################################################\n",
        "## Seeder\n",
        "# :seed to make all processes deterministic     # type: int\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    \n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6caqD7ZFUrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helper to load data by store ID\n",
        "#################################################################################\n",
        "# Read data\n",
        "def get_data_by_store(store):\n",
        "    \n",
        "    # Read and contact basic feature\n",
        "    df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)\n",
        "    \n",
        "    # Leave only relevant store\n",
        "    df = df[df['store_id']==store]\n",
        "\n",
        "    # With memory limits we have to read \n",
        "    # lags and mean encoding features\n",
        "    # separately and drop items that we don't need.\n",
        "    # As our Features Grids are aligned \n",
        "    # we can use index to keep only necessary rows\n",
        "    # Alignment is good for us as concat uses less memory than merge.\n",
        "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
        "    df2 = df2[df2.index.isin(df.index)]\n",
        "    \n",
        "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
        "    df3 = df3[df3.index.isin(df.index)]\n",
        "\n",
        "    #df4 = pd.read_pickle(TREND)\n",
        "    #df4 = df4[df4.index.isin(df.index)]\n",
        "    \n",
        "    df = pd.concat([df, df2], axis=1)\n",
        "    del df2 # to not reach memory limit \n",
        "    \n",
        "    df = pd.concat([df, df3], axis=1)\n",
        "    del df3 # to not reach memory limit \n",
        "\n",
        "    #df4[\"id\"]=df4[\"id\"].str.replace(\"evaluation\",\"validation\")\n",
        "    #df =pd.merge(df,df4,left_on=[\"id\",\"d\"],right_on=[\"id\",\"d\"])\n",
        "    #df = pd.concat([df, df4], axis=1)\n",
        "    #del df4 # to not reach memory limit \n",
        "    \n",
        "    # Create features list\n",
        "    features = [col for col in list(df) if col not in remove_features]\n",
        "    df = df[['id','d',TARGET]+features]\n",
        "    \n",
        "    # Skipping first n rows\n",
        "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
        "    \n",
        "    return df, features\n",
        "\n",
        "# Recombine Test set after training\n",
        "def get_base_test():\n",
        "    base_test = pd.DataFrame()\n",
        "\n",
        "    for store_id in STORES_IDS:\n",
        "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
        "        temp_df['store_id'] = store_id\n",
        "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
        "    \n",
        "    return base_test\n",
        "\n",
        "\n",
        "########################### Helper to make dynamic rolling lags\n",
        "#################################################################################\n",
        "def make_lag(LAG_DAY):\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "\n",
        "def make_lag_roll(LAG_DAY):\n",
        "    shift_day = LAG_DAY[0]\n",
        "    roll_wind = LAG_DAY[1]\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
        "    return lag_df[[col_name]]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIoOwIdtiMdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ73Dp0iiiae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "6e6378e2-b411-416a-fafc-28b08233795f"
      },
      "source": [
        "df[df.d==1914]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>release</th>\n",
              "      <th>sell_price</th>\n",
              "      <th>price_max</th>\n",
              "      <th>price_min</th>\n",
              "      <th>price_std</th>\n",
              "      <th>price_mean</th>\n",
              "      <th>price_norm</th>\n",
              "      <th>price_nunique</th>\n",
              "      <th>item_nunique</th>\n",
              "      <th>price_momentum</th>\n",
              "      <th>price_momentum_m</th>\n",
              "      <th>price_momentum_y</th>\n",
              "      <th>event_name_1</th>\n",
              "      <th>event_type_1</th>\n",
              "      <th>event_name_2</th>\n",
              "      <th>event_type_2</th>\n",
              "      <th>snap_CA</th>\n",
              "      <th>snap_TX</th>\n",
              "      <th>snap_WI</th>\n",
              "      <th>tm_d</th>\n",
              "      <th>tm_w</th>\n",
              "      <th>tm_m</th>\n",
              "      <th>tm_y</th>\n",
              "      <th>tm_wm</th>\n",
              "      <th>tm_dw</th>\n",
              "      <th>tm_w_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>46027957</th>\n",
              "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>224</td>\n",
              "      <td>8.382812</td>\n",
              "      <td>9.578125</td>\n",
              "      <td>8.257812</td>\n",
              "      <td>0.152100</td>\n",
              "      <td>8.289062</td>\n",
              "      <td>0.874512</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.009766</td>\n",
              "      <td>1.007812</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46027958</th>\n",
              "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_002</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>3.970703</td>\n",
              "      <td>3.970703</td>\n",
              "      <td>3.970703</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.970703</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>131</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46027959</th>\n",
              "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_003</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>300</td>\n",
              "      <td>2.970703</td>\n",
              "      <td>2.970703</td>\n",
              "      <td>2.970703</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.970703</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>118</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46027960</th>\n",
              "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_004</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>4.640625</td>\n",
              "      <td>4.640625</td>\n",
              "      <td>4.339844</td>\n",
              "      <td>0.145264</td>\n",
              "      <td>4.527344</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.022461</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46027961</th>\n",
              "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_005</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1914</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16</td>\n",
              "      <td>2.880859</td>\n",
              "      <td>3.080078</td>\n",
              "      <td>2.480469</td>\n",
              "      <td>0.150146</td>\n",
              "      <td>2.941406</td>\n",
              "      <td>0.935059</td>\n",
              "      <td>4.0</td>\n",
              "      <td>161</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.967773</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46058442</th>\n",
              "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
              "      <td>FOODS_3_823</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.980469</td>\n",
              "      <td>2.980469</td>\n",
              "      <td>2.480469</td>\n",
              "      <td>0.171631</td>\n",
              "      <td>2.800781</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>206</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.053711</td>\n",
              "      <td>1.022461</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46058443</th>\n",
              "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
              "      <td>FOODS_3_824</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.480469</td>\n",
              "      <td>2.679688</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.253174</td>\n",
              "      <td>2.507812</td>\n",
              "      <td>0.925293</td>\n",
              "      <td>4.0</td>\n",
              "      <td>135</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.982422</td>\n",
              "      <td>1.112305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46058444</th>\n",
              "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
              "      <td>FOODS_3_825</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.980469</td>\n",
              "      <td>4.378906</td>\n",
              "      <td>3.980469</td>\n",
              "      <td>0.188599</td>\n",
              "      <td>4.117188</td>\n",
              "      <td>0.908691</td>\n",
              "      <td>3.0</td>\n",
              "      <td>150</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.969238</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46058445</th>\n",
              "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
              "      <td>FOODS_3_826</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>1914</td>\n",
              "      <td>1.0</td>\n",
              "      <td>230</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>44</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46058446</th>\n",
              "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
              "      <td>FOODS_3_827</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>1914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>304</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>142</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30490 rows Ã— 34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     id        item_id  ... tm_dw tm_w_end\n",
              "46027957  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  ...     0        0\n",
              "46027958  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  ...     0        0\n",
              "46027959  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  ...     0        0\n",
              "46027960  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  ...     0        0\n",
              "46027961  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  ...     0        0\n",
              "...                                 ...            ...  ...   ...      ...\n",
              "46058442    FOODS_3_823_WI_3_evaluation    FOODS_3_823  ...     0        0\n",
              "46058443    FOODS_3_824_WI_3_evaluation    FOODS_3_824  ...     0        0\n",
              "46058444    FOODS_3_825_WI_3_evaluation    FOODS_3_825  ...     0        0\n",
              "46058445    FOODS_3_826_WI_3_evaluation    FOODS_3_826  ...     0        0\n",
              "46058446    FOODS_3_827_WI_3_evaluation    FOODS_3_827  ...     0        0\n",
              "\n",
              "[30490 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9llWHeh8FUr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Model params\n",
        "#################################################################################\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.1,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 1400,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': 1,\n",
        "                } \n",
        "\n",
        "# Let's look closer on params\n",
        "\n",
        "## 'boosting_type': 'gbdt'\n",
        "# we have 'goss' option for faster training\n",
        "# but it normally leads to underfit.\n",
        "# Also there is good 'dart' mode\n",
        "# but it takes forever to train\n",
        "# and model performance depends \n",
        "# a lot on random factor \n",
        "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
        "\n",
        "## 'objective': 'tweedie'\n",
        "# Tweedie Gradient Boosting for Extremely\n",
        "# Unbalanced Zero-inflated Data\n",
        "# https://arxiv.org/pdf/1811.10192.pdf\n",
        "# and many more articles about tweediie\n",
        "#\n",
        "# Strange (for me) but Tweedie is close in results\n",
        "# to my own ugly loss.\n",
        "# My advice here - make OWN LOSS function\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
        "# I think many of you already using it (after poisson kernel appeared) \n",
        "# (kagglers are very good with \"params\" testing and tuning).\n",
        "# Try to figure out why Tweedie works.\n",
        "# probably it will show you new features options\n",
        "# or data transformation (Target transformation?).\n",
        "\n",
        "## 'tweedie_variance_power': 1.1\n",
        "# default = 1.5\n",
        "# set this closer to 2 to shift towards a Gamma distribution\n",
        "# set this closer to 1 to shift towards a Poisson distribution\n",
        "# my CV shows 1.1 is optimal \n",
        "# but you can make your own choice\n",
        "\n",
        "## 'metric': 'rmse'\n",
        "# Doesn't mean anything to us\n",
        "# as competition metric is different\n",
        "# and we don't use early stoppings here.\n",
        "# So rmse serves just for general \n",
        "# model performance overview.\n",
        "# Also we use \"fake\" validation set\n",
        "# (as it makes part of the training set)\n",
        "# so even general rmse score doesn't mean anything))\n",
        "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
        "\n",
        "## 'subsample': 0.5\n",
        "# Serves to fight with overfit\n",
        "# this will randomly select part of data without resampling\n",
        "# Chosen by CV (my CV can be wrong!)\n",
        "# Next kernel will be about CV\n",
        "\n",
        "##'subsample_freq': 1\n",
        "# frequency for bagging\n",
        "# default value - seems ok\n",
        "\n",
        "## 'learning_rate': 0.03\n",
        "# Chosen by CV\n",
        "# Smaller - longer training\n",
        "# but there is an option to stop \n",
        "# in \"local minimum\"\n",
        "# Bigger - faster training\n",
        "# but there is a chance to\n",
        "# not find \"global minimum\" minimum\n",
        "\n",
        "## 'num_leaves': 2**11-1\n",
        "## 'min_data_in_leaf': 2**12-1\n",
        "# Force model to use more features\n",
        "# We need it to reduce \"recursive\"\n",
        "# error impact.\n",
        "# Also it leads to overfit\n",
        "# that's why we use small \n",
        "# 'max_bin': 100\n",
        "\n",
        "## l1, l2 regularizations\n",
        "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
        "# Good tiny explanation\n",
        "# l2 can work with bigger num_leaves\n",
        "# but my CV doesn't show boost\n",
        "                    \n",
        "## 'n_estimators': 1400\n",
        "# CV shows that there should be\n",
        "# different values for each state/store.\n",
        "# Current value was chosen \n",
        "# for general purpose.\n",
        "# As we don't use any early stopings\n",
        "# careful to not overfit Public LB.\n",
        "\n",
        "##'feature_fraction': 0.5\n",
        "# LightGBM will randomly select \n",
        "# part of features on each iteration (tree).\n",
        "# We have maaaany features\n",
        "# and many of them are \"duplicates\"\n",
        "# and many just \"noise\"\n",
        "# good values here - 0.5-0.7 (by CV)\n",
        "\n",
        "## 'boost_from_average': False\n",
        "# There is some \"problem\"\n",
        "# to code boost_from_average for \n",
        "# custom loss\n",
        "# 'True' makes training faster\n",
        "# BUT carefull use it\n",
        "# https://github.com/microsoft/LightGBM/issues/1514\n",
        "# not our case but good to know cons"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0zdteiGFUr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Vars\n",
        "#################################################################################\n",
        "VER=50\n",
        "SEED = 42                        # We want all things\n",
        "seed_everything(SEED)            # to be as deterministic \n",
        "lgb_params['seed'] = SEED        # as possible\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "\n",
        "#LIMITS and const\n",
        "TARGET      = 'sales'            # Our target\n",
        "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
        "END_TRAIN   = 1913  + 28             # End day of our train set\n",
        "P_HORIZON   = 28                 # Prediction horizon\n",
        "USE_AUX     = False          # Use or not pretrained models\n",
        "\n",
        "#FEATURES to remove\n",
        "## These features lead to overfit\n",
        "## or values not present in test set\n",
        "remove_features = ['id','state_id','store_id',\n",
        "                   'date','wm_yr_wk','d',\n",
        "                   'yhat',\n",
        "                   #'snap_CA','snap_TX','snap_WI',\n",
        "                   TARGET]\n",
        "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
        "                   'enc_dept_id_mean','enc_dept_id_std',\n",
        "                   'enc_item_id_mean','enc_item_id_std'] \n",
        "\n",
        "#PATHS for Features\n",
        "ORIGINAL = DIRPATH\n",
        "BASE     = DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'\n",
        "PRICE    = DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl'\n",
        "CALENDAR = DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl'\n",
        "LAGS     = DIRPATH+'/output/m5-lags-features/lags_df_28.pkl'\n",
        "MEAN_ENC = DIRPATH+'/output/m5-custom-features/mean_encoding_df.pkl'\n",
        "FINAL_SALES = DIRPATH+'/output/m5-custom-features/finalsales_df.pkl'\n",
        "SNAP      =  DIRPATH+'/output/m5-custom-features/snap_df.pkl'\n",
        "TREND= DIRPATH+'/output/prophet/trend.pkl'\n",
        "# AUX(pretrained) Models paths\n",
        "AUX_MODELS = DIRPATH+'/output/m5-three-shades-of-dark-darker-magic/'\n",
        "\n",
        "\n",
        "#STORES ids\n",
        "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
        "STORES_IDS = list(STORES_IDS.unique())\n",
        "#STORES_IDS = ['CA_1'] # It takes around 5hrs to train for each store. Please run multiple kernels for each store.\n",
        "\n",
        "#FOLDS\n",
        "CV_FOLDS = [0,1,2]\n",
        "\n",
        "#SPLITS for lags creation\n",
        "SHIFT_DAY  = 28\n",
        "N_LAGS     = 15\n",
        "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
        "ROLS_SPLIT = []\n",
        "for i in [1,7,14]:\n",
        "    for j in [7,14,30,60]:\n",
        "        ROLS_SPLIT.append([i,j])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JxZqSBN0zsQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFjWLbIcFUsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Aux Models\n",
        "# If you don't want to wait hours and hours\n",
        "# to have result you can train each store \n",
        "# in separate kernel and then just join result.\n",
        "\n",
        "# If we want to use pretrained models we can \n",
        "## skip training \n",
        "## (in our case do dummy training\n",
        "##  to show that we are good with memory\n",
        "##  and you can safely use this (all kernel) code)\n",
        "if USE_AUX:\n",
        "    lgb_params['n_estimators'] = 2\n",
        "    \n",
        "# Here is some 'logs' that can compare\n",
        "#Train CA_1\n",
        "#[100]\tvalid_0's rmse: 2.02289\n",
        "#[200]\tvalid_0's rmse: 2.0017\n",
        "#[300]\tvalid_0's rmse: 1.99239\n",
        "#[400]\tvalid_0's rmse: 1.98471\n",
        "#[500]\tvalid_0's rmse: 1.97923\n",
        "#[600]\tvalid_0's rmse: 1.97284\n",
        "#[700]\tvalid_0's rmse: 1.96763\n",
        "#[800]\tvalid_0's rmse: 1.9624\n",
        "#[900]\tvalid_0's rmse: 1.95673\n",
        "#[1000]\tvalid_0's rmse: 1.95201\n",
        "#[1100]\tvalid_0's rmse: 1.9476\n",
        "#[1200]\tvalid_0's rmse: 1.9434\n",
        "#[1300]\tvalid_0's rmse: 1.9392\n",
        "#[1400]\tvalid_0's rmse: 1.93446\n",
        "\n",
        "#Train CA_2\n",
        "#[100]\tvalid_0's rmse: 1.88949\n",
        "#[200]\tvalid_0's rmse: 1.84767\n",
        "#[300]\tvalid_0's rmse: 1.83653\n",
        "#[400]\tvalid_0's rmse: 1.82909\n",
        "#[500]\tvalid_0's rmse: 1.82265\n",
        "#[600]\tvalid_0's rmse: 1.81725\n",
        "#[700]\tvalid_0's rmse: 1.81252\n",
        "#[800]\tvalid_0's rmse: 1.80736\n",
        "#[900]\tvalid_0's rmse: 1.80242\n",
        "#[1000]\tvalid_0's rmse: 1.79821\n",
        "#[1100]\tvalid_0's rmse: 1.794\n",
        "#[1200]\tvalid_0's rmse: 1.78973\n",
        "#[1300]\tvalid_0's rmse: 1.78552\n",
        "#[1400]\tvalid_0's rmse: 1.78158"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrJW8SmqaR95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df, features_columns = get_data_by_store(\"CA_1\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlXIOdiictl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "96fc07ae-8921-4098-82ce-604f03402c87"
      },
      "source": [
        "grid_df.tail()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>release</th>\n",
              "      <th>sell_price</th>\n",
              "      <th>price_max</th>\n",
              "      <th>price_min</th>\n",
              "      <th>price_std</th>\n",
              "      <th>price_mean</th>\n",
              "      <th>price_norm</th>\n",
              "      <th>price_nunique</th>\n",
              "      <th>item_nunique</th>\n",
              "      <th>price_momentum</th>\n",
              "      <th>price_momentum_m</th>\n",
              "      <th>price_momentum_y</th>\n",
              "      <th>event_name_1</th>\n",
              "      <th>event_type_1</th>\n",
              "      <th>event_name_2</th>\n",
              "      <th>event_type_2</th>\n",
              "      <th>snap_CA</th>\n",
              "      <th>snap_TX</th>\n",
              "      <th>snap_WI</th>\n",
              "      <th>tm_d</th>\n",
              "      <th>tm_w</th>\n",
              "      <th>tm_m</th>\n",
              "      <th>tm_y</th>\n",
              "      <th>tm_wm</th>\n",
              "      <th>tm_dw</th>\n",
              "      <th>tm_w_end</th>\n",
              "      <th>enc_cat_id_mean</th>\n",
              "      <th>enc_cat_id_std</th>\n",
              "      <th>enc_dept_id_mean</th>\n",
              "      <th>enc_dept_id_std</th>\n",
              "      <th>enc_item_id_mean</th>\n",
              "      <th>enc_item_id_std</th>\n",
              "      <th>sales_lag_28</th>\n",
              "      <th>sales_lag_29</th>\n",
              "      <th>sales_lag_30</th>\n",
              "      <th>sales_lag_31</th>\n",
              "      <th>sales_lag_32</th>\n",
              "      <th>sales_lag_33</th>\n",
              "      <th>sales_lag_34</th>\n",
              "      <th>sales_lag_35</th>\n",
              "      <th>sales_lag_36</th>\n",
              "      <th>sales_lag_37</th>\n",
              "      <th>sales_lag_38</th>\n",
              "      <th>sales_lag_39</th>\n",
              "      <th>sales_lag_40</th>\n",
              "      <th>sales_lag_41</th>\n",
              "      <th>sales_lag_42</th>\n",
              "      <th>rolling_mean_7</th>\n",
              "      <th>rolling_std_7</th>\n",
              "      <th>rolling_mean_14</th>\n",
              "      <th>rolling_std_14</th>\n",
              "      <th>rolling_mean_30</th>\n",
              "      <th>rolling_std_30</th>\n",
              "      <th>rolling_mean_60</th>\n",
              "      <th>rolling_std_60</th>\n",
              "      <th>rolling_mean_180</th>\n",
              "      <th>rolling_std_180</th>\n",
              "      <th>rolling_mean_tmp_1_7</th>\n",
              "      <th>rolling_mean_tmp_1_14</th>\n",
              "      <th>rolling_mean_tmp_1_30</th>\n",
              "      <th>rolling_mean_tmp_1_60</th>\n",
              "      <th>rolling_mean_tmp_7_7</th>\n",
              "      <th>rolling_mean_tmp_7_14</th>\n",
              "      <th>rolling_mean_tmp_7_30</th>\n",
              "      <th>rolling_mean_tmp_7_60</th>\n",
              "      <th>rolling_mean_tmp_14_7</th>\n",
              "      <th>rolling_mean_tmp_14_14</th>\n",
              "      <th>rolling_mean_tmp_14_30</th>\n",
              "      <th>rolling_mean_tmp_14_60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4873634</th>\n",
              "      <td>FOODS_3_823_CA_1_evaluation</td>\n",
              "      <td>1969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FOODS_3_823</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>127</td>\n",
              "      <td>2.980469</td>\n",
              "      <td>2.980469</td>\n",
              "      <td>2.480469</td>\n",
              "      <td>0.152222</td>\n",
              "      <td>2.755859</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>236</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.050781</td>\n",
              "      <td>1.030273</td>\n",
              "      <td>NBAFinalsEnd</td>\n",
              "      <td>Sporting</td>\n",
              "      <td>Father's day</td>\n",
              "      <td>Cultural</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.109375</td>\n",
              "      <td>5.769531</td>\n",
              "      <td>2.626953</td>\n",
              "      <td>7.058594</td>\n",
              "      <td>0.846680</td>\n",
              "      <td>1.752930</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.285156</td>\n",
              "      <td>1.496094</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.358398</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.306641</td>\n",
              "      <td>1.083008</td>\n",
              "      <td>1.356445</td>\n",
              "      <td>1.588867</td>\n",
              "      <td>1.838867</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4873635</th>\n",
              "      <td>FOODS_3_824_CA_1_evaluation</td>\n",
              "      <td>1969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FOODS_3_824</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>0</td>\n",
              "      <td>2.480469</td>\n",
              "      <td>2.679688</td>\n",
              "      <td>2.470703</td>\n",
              "      <td>0.086365</td>\n",
              "      <td>2.630859</td>\n",
              "      <td>0.925293</td>\n",
              "      <td>3.0</td>\n",
              "      <td>138</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.945312</td>\n",
              "      <td>0.962891</td>\n",
              "      <td>NBAFinalsEnd</td>\n",
              "      <td>Sporting</td>\n",
              "      <td>Father's day</td>\n",
              "      <td>Cultural</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.109375</td>\n",
              "      <td>5.769531</td>\n",
              "      <td>2.626953</td>\n",
              "      <td>7.058594</td>\n",
              "      <td>0.434326</td>\n",
              "      <td>0.947266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.856934</td>\n",
              "      <td>1.214844</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.617188</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.364258</td>\n",
              "      <td>0.883301</td>\n",
              "      <td>1.462891</td>\n",
              "      <td>0.294434</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4873636</th>\n",
              "      <td>FOODS_3_825_CA_1_evaluation</td>\n",
              "      <td>1969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FOODS_3_825</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>1</td>\n",
              "      <td>3.980469</td>\n",
              "      <td>4.378906</td>\n",
              "      <td>3.980469</td>\n",
              "      <td>0.189697</td>\n",
              "      <td>4.121094</td>\n",
              "      <td>0.908691</td>\n",
              "      <td>3.0</td>\n",
              "      <td>165</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.954102</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NBAFinalsEnd</td>\n",
              "      <td>Sporting</td>\n",
              "      <td>Father's day</td>\n",
              "      <td>Cultural</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.109375</td>\n",
              "      <td>5.769531</td>\n",
              "      <td>2.626953</td>\n",
              "      <td>7.058594</td>\n",
              "      <td>0.700684</td>\n",
              "      <td>1.198242</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.142578</td>\n",
              "      <td>1.214844</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.038086</td>\n",
              "      <td>1.233398</td>\n",
              "      <td>1.194336</td>\n",
              "      <td>1.133789</td>\n",
              "      <td>1.016602</td>\n",
              "      <td>0.950195</td>\n",
              "      <td>1.115234</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4873637</th>\n",
              "      <td>FOODS_3_826_CA_1_evaluation</td>\n",
              "      <td>1969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FOODS_3_826</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>211</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.280273</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NBAFinalsEnd</td>\n",
              "      <td>Sporting</td>\n",
              "      <td>Father's day</td>\n",
              "      <td>Cultural</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.109375</td>\n",
              "      <td>5.769531</td>\n",
              "      <td>2.626953</td>\n",
              "      <td>7.058594</td>\n",
              "      <td>1.108398</td>\n",
              "      <td>1.479492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.571289</td>\n",
              "      <td>1.133789</td>\n",
              "      <td>1.357422</td>\n",
              "      <td>1.082031</td>\n",
              "      <td>0.966797</td>\n",
              "      <td>1.159180</td>\n",
              "      <td>0.966797</td>\n",
              "      <td>1.301758</td>\n",
              "      <td>1.061523</td>\n",
              "      <td>1.415039</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4873638</th>\n",
              "      <td>FOODS_3_827_CA_1_evaluation</td>\n",
              "      <td>1969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FOODS_3_827</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>403</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>137</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NBAFinalsEnd</td>\n",
              "      <td>Sporting</td>\n",
              "      <td>Father's day</td>\n",
              "      <td>Cultural</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2.109375</td>\n",
              "      <td>5.769531</td>\n",
              "      <td>2.626953</td>\n",
              "      <td>7.058594</td>\n",
              "      <td>1.644531</td>\n",
              "      <td>3.115234</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.714844</td>\n",
              "      <td>3.093750</td>\n",
              "      <td>4.355469</td>\n",
              "      <td>3.201172</td>\n",
              "      <td>5.265625</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>5.117188</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>4.027344</td>\n",
              "      <td>3.505859</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id  ...  rolling_mean_tmp_14_60\n",
              "4873634  FOODS_3_823_CA_1_evaluation  ...                     NaN\n",
              "4873635  FOODS_3_824_CA_1_evaluation  ...                     NaN\n",
              "4873636  FOODS_3_825_CA_1_evaluation  ...                     NaN\n",
              "4873637  FOODS_3_826_CA_1_evaluation  ...                     NaN\n",
              "4873638  FOODS_3_827_CA_1_evaluation  ...                     NaN\n",
              "\n",
              "[5 rows x 75 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2kLDx1GFUsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5284300-be6d-46be-eeb1-c22af2bde705"
      },
      "source": [
        "########################### Train Models\n",
        "#################################################################################\n",
        "for store_id in STORES_IDS:\n",
        "#for store_id in [\"CA_1\"]:\n",
        "    print('Train', store_id)\n",
        "    \n",
        "    # Get grid for current store\n",
        "    grid_df, features_columns = get_data_by_store(store_id)\n",
        "    #grid_df[\"sales\"]=grid_df[\"sales\"]*grid_df[\"sell_price\"]\n",
        "    \n",
        "    # Masks for \n",
        "    # Train (All data less than 1913)\n",
        "    # \"Validation\" (Last 28 days - not real validatio set)\n",
        "    # Test (All data greater than 1913 day, \n",
        "    #       with some gap for recursive features)\n",
        "    train_mask = grid_df['d']<=END_TRAIN\n",
        "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
        "    \n",
        "    ## Initiating our GroupKFold\n",
        "    folds = GroupKFold(n_splits=3)\n",
        "\n",
        "    # get subgroups for each week, year pair\n",
        "    grid_df['groups'] = grid_df['tm_w'].astype(str) + '_' + grid_df['tm_y'].astype(str)\n",
        "    split_groups = grid_df[train_mask]['groups']\n",
        "\n",
        "    # Main Data\n",
        "    X,y = grid_df[train_mask][features_columns], grid_df[train_mask][TARGET]\n",
        "  \n",
        "\n",
        "    for n_spilit in range(1,4):\n",
        "        print('Fold:',n_spilit)\n",
        "        print(END_TRAIN-P_HORIZON*n_spilit,\"~\", END_TRAIN-P_HORIZON*(n_spilit-1))\n",
        "        #print(len(trn_idx),len(val_idx))\n",
        "        \n",
        "        train_mask = grid_df['d']<=(END_TRAIN-P_HORIZON*(n_spilit-1))\n",
        "        valid_mask=  (grid_df['d']>(END_TRAIN-P_HORIZON*n_spilit))& (grid_df['d']<=(END_TRAIN-P_HORIZON*(n_spilit-1)))\n",
        "\n",
        "        train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
        "                       label=grid_df[train_mask][TARGET])\n",
        "        valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
        "                       label=grid_df[valid_mask][TARGET])  \n",
        "        \n",
        "        seed_everything(SEED)\n",
        "        estimator = lgb.train(\n",
        "                lgb_params,\n",
        "                train_data,\n",
        "                valid_sets = [train_data, valid_data],\n",
        "                verbose_eval = 100\n",
        "            )\n",
        "        # Save model - it's not real '.bin' but a pickle file\n",
        "        # estimator = lgb.Booster(model_file='model.txt')\n",
        "        # can only predict with the best iteration (or the saving iteration)\n",
        "        # pickle.dump gives us more flexibility\n",
        "        # like estimator.predict(TEST, num_iteration=100)\n",
        "        # num_iteration - number of iteration want to predict with, \n",
        "        # NULL or <= 0 means use best iteration\n",
        "        model_name = AUX_MODELS+'lgb_model_'+store_id+'_'+str(n_spilit)+'_'+str(VER)+'.bin'\n",
        "        if USE_AUX==False:\n",
        "          pickle.dump(estimator, open(model_name, 'wb'))\n",
        "\n",
        "        # Remove temporary files and objects \n",
        "        # to free some hdd space and ram memory\n",
        "        train_data, valid_data, estimator\n",
        "        gc.collect()\n",
        "\n",
        "    # Saving part of the dataset for later predictions\n",
        "    # Removing features that we need to calculate recursively \n",
        "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
        "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
        "    grid_df = grid_df[keep_cols]\n",
        "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
        "    del grid_df\n",
        "\n",
        "    # \"Keep\" models features for predictions\n",
        "    MODEL_FEATURES = features_columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train CA_1\n",
            "Fold: 1\n",
            "1913 ~ 1941\n",
            "[100]\ttraining's rmse: 2.56198\tvalid_1's rmse: 2.0149\n",
            "[200]\ttraining's rmse: 2.46401\tvalid_1's rmse: 1.98529\n",
            "[300]\ttraining's rmse: 2.43091\tvalid_1's rmse: 1.97543\n",
            "[400]\ttraining's rmse: 2.40884\tvalid_1's rmse: 1.96793\n",
            "[500]\ttraining's rmse: 2.39141\tvalid_1's rmse: 1.96261\n",
            "[600]\ttraining's rmse: 2.37691\tvalid_1's rmse: 1.95642\n",
            "[700]\ttraining's rmse: 2.36306\tvalid_1's rmse: 1.95118\n",
            "[800]\ttraining's rmse: 2.3516\tvalid_1's rmse: 1.94617\n",
            "[900]\ttraining's rmse: 2.34052\tvalid_1's rmse: 1.94147\n",
            "[1000]\ttraining's rmse: 2.33052\tvalid_1's rmse: 1.93661\n",
            "[1100]\ttraining's rmse: 2.32107\tvalid_1's rmse: 1.93178\n",
            "[1200]\ttraining's rmse: 2.31228\tvalid_1's rmse: 1.92714\n",
            "[1300]\ttraining's rmse: 2.3039\tvalid_1's rmse: 1.92283\n",
            "[1400]\ttraining's rmse: 2.29565\tvalid_1's rmse: 1.91841\n",
            "Fold: 2\n",
            "1885 ~ 1913\n",
            "[100]\ttraining's rmse: 2.57085\tvalid_1's rmse: 2.03227\n",
            "[200]\ttraining's rmse: 2.47088\tvalid_1's rmse: 2.01475\n",
            "[300]\ttraining's rmse: 2.43664\tvalid_1's rmse: 2.00335\n",
            "[400]\ttraining's rmse: 2.41452\tvalid_1's rmse: 1.99628\n",
            "[500]\ttraining's rmse: 2.3981\tvalid_1's rmse: 1.99061\n",
            "[600]\ttraining's rmse: 2.38309\tvalid_1's rmse: 1.98445\n",
            "[700]\ttraining's rmse: 2.37043\tvalid_1's rmse: 1.97821\n",
            "[800]\ttraining's rmse: 2.35764\tvalid_1's rmse: 1.97268\n",
            "[900]\ttraining's rmse: 2.34723\tvalid_1's rmse: 1.96781\n",
            "[1000]\ttraining's rmse: 2.33663\tvalid_1's rmse: 1.96267\n",
            "[1100]\ttraining's rmse: 2.32727\tvalid_1's rmse: 1.95796\n",
            "[1200]\ttraining's rmse: 2.31875\tvalid_1's rmse: 1.95341\n",
            "[1300]\ttraining's rmse: 2.3098\tvalid_1's rmse: 1.94891\n",
            "[1400]\ttraining's rmse: 2.30177\tvalid_1's rmse: 1.9445\n",
            "Fold: 3\n",
            "1857 ~ 1885\n",
            "[100]\ttraining's rmse: 2.58002\tvalid_1's rmse: 1.98427\n",
            "[200]\ttraining's rmse: 2.47883\tvalid_1's rmse: 1.94826\n",
            "[300]\ttraining's rmse: 2.44465\tvalid_1's rmse: 1.93406\n",
            "[400]\ttraining's rmse: 2.42237\tvalid_1's rmse: 1.92402\n",
            "[500]\ttraining's rmse: 2.4051\tvalid_1's rmse: 1.9165\n",
            "[600]\ttraining's rmse: 2.39036\tvalid_1's rmse: 1.90981\n",
            "[700]\ttraining's rmse: 2.3766\tvalid_1's rmse: 1.90263\n",
            "[800]\ttraining's rmse: 2.36447\tvalid_1's rmse: 1.89676\n",
            "[900]\ttraining's rmse: 2.35398\tvalid_1's rmse: 1.89173\n",
            "[1000]\ttraining's rmse: 2.34342\tvalid_1's rmse: 1.88538\n",
            "[1100]\ttraining's rmse: 2.33354\tvalid_1's rmse: 1.88006\n",
            "[1200]\ttraining's rmse: 2.32439\tvalid_1's rmse: 1.87575\n",
            "[1300]\ttraining's rmse: 2.31531\tvalid_1's rmse: 1.87082\n",
            "[1400]\ttraining's rmse: 2.30637\tvalid_1's rmse: 1.86649\n",
            "Train CA_2\n",
            "Fold: 1\n",
            "1913 ~ 1941\n",
            "[100]\ttraining's rmse: 2.04401\tvalid_1's rmse: 1.94854\n",
            "[200]\ttraining's rmse: 2.00415\tvalid_1's rmse: 1.89191\n",
            "[300]\ttraining's rmse: 1.98927\tvalid_1's rmse: 1.879\n",
            "[400]\ttraining's rmse: 1.97845\tvalid_1's rmse: 1.87031\n",
            "[500]\ttraining's rmse: 1.96993\tvalid_1's rmse: 1.86388\n",
            "[600]\ttraining's rmse: 1.96168\tvalid_1's rmse: 1.85748\n",
            "[700]\ttraining's rmse: 1.9541\tvalid_1's rmse: 1.85129\n",
            "[800]\ttraining's rmse: 1.94729\tvalid_1's rmse: 1.84604\n",
            "[900]\ttraining's rmse: 1.94095\tvalid_1's rmse: 1.84132\n",
            "[1000]\ttraining's rmse: 1.9345\tvalid_1's rmse: 1.83605\n",
            "[1100]\ttraining's rmse: 1.92831\tvalid_1's rmse: 1.83141\n",
            "[1200]\ttraining's rmse: 1.92266\tvalid_1's rmse: 1.82725\n",
            "[1300]\ttraining's rmse: 1.91715\tvalid_1's rmse: 1.82312\n",
            "[1400]\ttraining's rmse: 1.91178\tvalid_1's rmse: 1.81886\n",
            "Fold: 2\n",
            "1885 ~ 1913\n",
            "[100]\ttraining's rmse: 2.04677\tvalid_1's rmse: 1.89426\n",
            "[200]\ttraining's rmse: 2.00653\tvalid_1's rmse: 1.85232\n",
            "[300]\ttraining's rmse: 1.99203\tvalid_1's rmse: 1.84247\n",
            "[400]\ttraining's rmse: 1.98128\tvalid_1's rmse: 1.83484\n",
            "[500]\ttraining's rmse: 1.97192\tvalid_1's rmse: 1.82877\n",
            "[600]\ttraining's rmse: 1.96362\tvalid_1's rmse: 1.82272\n",
            "[700]\ttraining's rmse: 1.95626\tvalid_1's rmse: 1.81724\n",
            "[800]\ttraining's rmse: 1.9491\tvalid_1's rmse: 1.81224\n",
            "[900]\ttraining's rmse: 1.94252\tvalid_1's rmse: 1.80744\n",
            "[1000]\ttraining's rmse: 1.93635\tvalid_1's rmse: 1.80295\n",
            "[1100]\ttraining's rmse: 1.93027\tvalid_1's rmse: 1.79865\n",
            "[1200]\ttraining's rmse: 1.92441\tvalid_1's rmse: 1.79449\n",
            "[1300]\ttraining's rmse: 1.91906\tvalid_1's rmse: 1.79007\n",
            "[1400]\ttraining's rmse: 1.9136\tvalid_1's rmse: 1.78572\n",
            "Fold: 3\n",
            "1857 ~ 1885\n",
            "[100]\ttraining's rmse: 2.04892\tvalid_1's rmse: 1.97282\n",
            "[200]\ttraining's rmse: 2.00962\tvalid_1's rmse: 1.92844\n",
            "[300]\ttraining's rmse: 1.99455\tvalid_1's rmse: 1.90505\n",
            "[400]\ttraining's rmse: 1.98388\tvalid_1's rmse: 1.89111\n",
            "[500]\ttraining's rmse: 1.97513\tvalid_1's rmse: 1.88208\n",
            "[600]\ttraining's rmse: 1.96675\tvalid_1's rmse: 1.87476\n",
            "[700]\ttraining's rmse: 1.95897\tvalid_1's rmse: 1.86738\n",
            "[800]\ttraining's rmse: 1.95174\tvalid_1's rmse: 1.86111\n",
            "[900]\ttraining's rmse: 1.94476\tvalid_1's rmse: 1.85436\n",
            "[1000]\ttraining's rmse: 1.93849\tvalid_1's rmse: 1.84874\n",
            "[1100]\ttraining's rmse: 1.93247\tvalid_1's rmse: 1.8433\n",
            "[1200]\ttraining's rmse: 1.92698\tvalid_1's rmse: 1.83864\n",
            "[1300]\ttraining's rmse: 1.92133\tvalid_1's rmse: 1.83286\n",
            "[1400]\ttraining's rmse: 1.91585\tvalid_1's rmse: 1.82872\n",
            "Train CA_3\n",
            "Fold: 1\n",
            "1913 ~ 1941\n",
            "[100]\ttraining's rmse: 3.67584\tvalid_1's rmse: 2.38034\n",
            "[200]\ttraining's rmse: 3.45946\tvalid_1's rmse: 2.34544\n",
            "[300]\ttraining's rmse: 3.38381\tvalid_1's rmse: 2.32671\n",
            "[400]\ttraining's rmse: 3.34106\tvalid_1's rmse: 2.31477\n",
            "[500]\ttraining's rmse: 3.3096\tvalid_1's rmse: 2.30745\n",
            "[600]\ttraining's rmse: 3.28249\tvalid_1's rmse: 2.30028\n",
            "[700]\ttraining's rmse: 3.2567\tvalid_1's rmse: 2.29397\n",
            "[800]\ttraining's rmse: 3.23511\tvalid_1's rmse: 2.28793\n",
            "[900]\ttraining's rmse: 3.21606\tvalid_1's rmse: 2.28227\n",
            "[1000]\ttraining's rmse: 3.1984\tvalid_1's rmse: 2.27631\n",
            "[1100]\ttraining's rmse: 3.18426\tvalid_1's rmse: 2.27119\n",
            "[1200]\ttraining's rmse: 3.16983\tvalid_1's rmse: 2.26659\n",
            "[1300]\ttraining's rmse: 3.15583\tvalid_1's rmse: 2.26174\n",
            "[1400]\ttraining's rmse: 3.1435\tvalid_1's rmse: 2.2569\n",
            "Fold: 2\n",
            "1885 ~ 1913\n",
            "[100]\ttraining's rmse: 3.69066\tvalid_1's rmse: 2.5011\n",
            "[200]\ttraining's rmse: 3.47272\tvalid_1's rmse: 2.469\n",
            "[300]\ttraining's rmse: 3.39566\tvalid_1's rmse: 2.44526\n",
            "[400]\ttraining's rmse: 3.35434\tvalid_1's rmse: 2.42994\n",
            "[500]\ttraining's rmse: 3.32119\tvalid_1's rmse: 2.4203\n",
            "[600]\ttraining's rmse: 3.29452\tvalid_1's rmse: 2.41265\n",
            "[700]\ttraining's rmse: 3.27098\tvalid_1's rmse: 2.40709\n",
            "[800]\ttraining's rmse: 3.24968\tvalid_1's rmse: 2.39993\n",
            "[900]\ttraining's rmse: 3.23191\tvalid_1's rmse: 2.39466\n",
            "[1000]\ttraining's rmse: 3.21532\tvalid_1's rmse: 2.38888\n",
            "[1100]\ttraining's rmse: 3.19856\tvalid_1's rmse: 2.38344\n",
            "[1200]\ttraining's rmse: 3.18325\tvalid_1's rmse: 2.37869\n",
            "[1300]\ttraining's rmse: 3.17029\tvalid_1's rmse: 2.37422\n",
            "[1400]\ttraining's rmse: 3.1566\tvalid_1's rmse: 2.37\n",
            "Fold: 3\n",
            "1857 ~ 1885\n",
            "[100]\ttraining's rmse: 3.71189\tvalid_1's rmse: 2.7046\n",
            "[200]\ttraining's rmse: 3.49302\tvalid_1's rmse: 2.65184\n",
            "[300]\ttraining's rmse: 3.41809\tvalid_1's rmse: 2.6163\n",
            "[400]\ttraining's rmse: 3.37738\tvalid_1's rmse: 2.59786\n",
            "[500]\ttraining's rmse: 3.34343\tvalid_1's rmse: 2.5825\n",
            "[600]\ttraining's rmse: 3.31654\tvalid_1's rmse: 2.57129\n",
            "[700]\ttraining's rmse: 3.29047\tvalid_1's rmse: 2.56075\n",
            "[800]\ttraining's rmse: 3.26924\tvalid_1's rmse: 2.55038\n",
            "[900]\ttraining's rmse: 3.24976\tvalid_1's rmse: 2.54197\n",
            "[1000]\ttraining's rmse: 3.23153\tvalid_1's rmse: 2.53348\n",
            "[1100]\ttraining's rmse: 3.21496\tvalid_1's rmse: 2.526\n",
            "[1200]\ttraining's rmse: 3.19901\tvalid_1's rmse: 2.51758\n",
            "[1300]\ttraining's rmse: 3.18385\tvalid_1's rmse: 2.51008\n",
            "[1400]\ttraining's rmse: 3.17156\tvalid_1's rmse: 2.50304\n",
            "Train CA_4\n",
            "Fold: 1\n",
            "1913 ~ 1941\n",
            "[100]\ttraining's rmse: 1.51507\tvalid_1's rmse: 1.39281\n",
            "[200]\ttraining's rmse: 1.48649\tvalid_1's rmse: 1.38275\n",
            "[300]\ttraining's rmse: 1.47555\tvalid_1's rmse: 1.37756\n",
            "[400]\ttraining's rmse: 1.46846\tvalid_1's rmse: 1.37327\n",
            "[500]\ttraining's rmse: 1.46273\tvalid_1's rmse: 1.36968\n",
            "[600]\ttraining's rmse: 1.45754\tvalid_1's rmse: 1.36643\n",
            "[700]\ttraining's rmse: 1.45257\tvalid_1's rmse: 1.36322\n",
            "[800]\ttraining's rmse: 1.44786\tvalid_1's rmse: 1.36011\n",
            "[900]\ttraining's rmse: 1.4436\tvalid_1's rmse: 1.35727\n",
            "[1000]\ttraining's rmse: 1.43947\tvalid_1's rmse: 1.3544\n",
            "[1100]\ttraining's rmse: 1.4352\tvalid_1's rmse: 1.35142\n",
            "[1200]\ttraining's rmse: 1.43134\tvalid_1's rmse: 1.34862\n",
            "[1300]\ttraining's rmse: 1.42753\tvalid_1's rmse: 1.34567\n",
            "[1400]\ttraining's rmse: 1.42383\tvalid_1's rmse: 1.34285\n",
            "Fold: 2\n",
            "1885 ~ 1913\n",
            "[100]\ttraining's rmse: 1.51732\tvalid_1's rmse: 1.33262\n",
            "[200]\ttraining's rmse: 1.489\tvalid_1's rmse: 1.327\n",
            "[300]\ttraining's rmse: 1.47735\tvalid_1's rmse: 1.32157\n",
            "[400]\ttraining's rmse: 1.47058\tvalid_1's rmse: 1.31756\n",
            "[500]\ttraining's rmse: 1.4647\tvalid_1's rmse: 1.31439\n",
            "[600]\ttraining's rmse: 1.45911\tvalid_1's rmse: 1.3115\n",
            "[700]\ttraining's rmse: 1.45428\tvalid_1's rmse: 1.30861\n",
            "[800]\ttraining's rmse: 1.44948\tvalid_1's rmse: 1.30564\n",
            "[900]\ttraining's rmse: 1.44512\tvalid_1's rmse: 1.30297\n",
            "[1000]\ttraining's rmse: 1.44079\tvalid_1's rmse: 1.29998\n",
            "[1100]\ttraining's rmse: 1.43671\tvalid_1's rmse: 1.29717\n",
            "[1200]\ttraining's rmse: 1.43285\tvalid_1's rmse: 1.29492\n",
            "[1300]\ttraining's rmse: 1.42895\tvalid_1's rmse: 1.29243\n",
            "[1400]\ttraining's rmse: 1.42525\tvalid_1's rmse: 1.28996\n",
            "Fold: 3\n",
            "1857 ~ 1885\n",
            "[100]\ttraining's rmse: 1.51969\tvalid_1's rmse: 1.38198\n",
            "[200]\ttraining's rmse: 1.49118\tvalid_1's rmse: 1.36891\n",
            "[300]\ttraining's rmse: 1.47986\tvalid_1's rmse: 1.36293\n",
            "[400]\ttraining's rmse: 1.47285\tvalid_1's rmse: 1.35816\n",
            "[500]\ttraining's rmse: 1.46676\tvalid_1's rmse: 1.35329\n",
            "[600]\ttraining's rmse: 1.46155\tvalid_1's rmse: 1.34868\n",
            "[700]\ttraining's rmse: 1.45647\tvalid_1's rmse: 1.3447\n",
            "[800]\ttraining's rmse: 1.45164\tvalid_1's rmse: 1.34155\n",
            "[900]\ttraining's rmse: 1.44716\tvalid_1's rmse: 1.33822\n",
            "[1000]\ttraining's rmse: 1.44309\tvalid_1's rmse: 1.33471\n",
            "[1100]\ttraining's rmse: 1.43893\tvalid_1's rmse: 1.33114\n",
            "[1200]\ttraining's rmse: 1.43518\tvalid_1's rmse: 1.32799\n",
            "[1300]\ttraining's rmse: 1.43146\tvalid_1's rmse: 1.32469\n",
            "[1400]\ttraining's rmse: 1.4278\tvalid_1's rmse: 1.32206\n",
            "Train TX_1\n",
            "Fold: 1\n",
            "1913 ~ 1941\n",
            "[100]\ttraining's rmse: 2.13423\tvalid_1's rmse: 1.61591\n",
            "[200]\ttraining's rmse: 2.06653\tvalid_1's rmse: 1.58813\n",
            "[300]\ttraining's rmse: 2.04038\tvalid_1's rmse: 1.57718\n",
            "[400]\ttraining's rmse: 2.02471\tvalid_1's rmse: 1.56991\n",
            "[500]\ttraining's rmse: 2.01048\tvalid_1's rmse: 1.56444\n",
            "[600]\ttraining's rmse: 1.99887\tvalid_1's rmse: 1.55991\n",
            "[700]\ttraining's rmse: 1.98779\tvalid_1's rmse: 1.55512\n",
            "[800]\ttraining's rmse: 1.97859\tvalid_1's rmse: 1.55144\n",
            "[900]\ttraining's rmse: 1.96864\tvalid_1's rmse: 1.54746\n",
            "[1000]\ttraining's rmse: 1.96095\tvalid_1's rmse: 1.54383\n",
            "[1100]\ttraining's rmse: 1.95259\tvalid_1's rmse: 1.54034\n",
            "[1200]\ttraining's rmse: 1.94381\tvalid_1's rmse: 1.53641\n",
            "[1300]\ttraining's rmse: 1.93603\tvalid_1's rmse: 1.53298\n",
            "[1400]\ttraining's rmse: 1.92862\tvalid_1's rmse: 1.52947\n",
            "Fold: 2\n",
            "1885 ~ 1913\n",
            "[100]\ttraining's rmse: 2.142\tvalid_1's rmse: 1.61197\n",
            "[200]\ttraining's rmse: 2.07414\tvalid_1's rmse: 1.59645\n",
            "[300]\ttraining's rmse: 2.04809\tvalid_1's rmse: 1.58859\n",
            "[400]\ttraining's rmse: 2.03114\tvalid_1's rmse: 1.5834\n",
            "[500]\ttraining's rmse: 2.01791\tvalid_1's rmse: 1.57907\n",
            "[600]\ttraining's rmse: 2.005\tvalid_1's rmse: 1.57489\n",
            "[700]\ttraining's rmse: 1.99459\tvalid_1's rmse: 1.57186\n",
            "[800]\ttraining's rmse: 1.98449\tvalid_1's rmse: 1.56794\n",
            "[900]\ttraining's rmse: 1.97579\tvalid_1's rmse: 1.56442\n",
            "[1000]\ttraining's rmse: 1.96751\tvalid_1's rmse: 1.56073\n",
            "[1100]\ttraining's rmse: 1.95941\tvalid_1's rmse: 1.55733\n",
            "[1200]\ttraining's rmse: 1.95118\tvalid_1's rmse: 1.55431\n",
            "[1300]\ttraining's rmse: 1.94297\tvalid_1's rmse: 1.55087\n",
            "[1400]\ttraining's rmse: 1.9356\tvalid_1's rmse: 1.54723\n",
            "Fold: 3\n",
            "1857 ~ 1885\n",
            "[100]\ttraining's rmse: 2.15236\tvalid_1's rmse: 1.62968\n",
            "[200]\ttraining's rmse: 2.08219\tvalid_1's rmse: 1.59525\n",
            "[300]\ttraining's rmse: 2.05527\tvalid_1's rmse: 1.58376\n",
            "[400]\ttraining's rmse: 2.03761\tvalid_1's rmse: 1.57492\n",
            "[500]\ttraining's rmse: 2.02406\tvalid_1's rmse: 1.57015\n",
            "[600]\ttraining's rmse: 2.01219\tvalid_1's rmse: 1.56463\n",
            "[700]\ttraining's rmse: 2.0012\tvalid_1's rmse: 1.56042\n",
            "[800]\ttraining's rmse: 1.99177\tvalid_1's rmse: 1.55606\n",
            "[900]\ttraining's rmse: 1.98263\tvalid_1's rmse: 1.5519\n",
            "[1000]\ttraining's rmse: 1.97384\tvalid_1's rmse: 1.54841\n",
            "[1100]\ttraining's rmse: 1.9651\tvalid_1's rmse: 1.545\n",
            "[1200]\ttraining's rmse: 1.95688\tvalid_1's rmse: 1.54176\n",
            "[1300]\ttraining's rmse: 1.94887\tvalid_1's rmse: 1.53794\n",
            "[1400]\ttraining's rmse: 1.94142\tvalid_1's rmse: 1.53495\n",
            "Train TX_2\n",
            "Fold: 1\n",
            "1913 ~ 1941\n",
            "[100]\ttraining's rmse: 2.55462\tvalid_1's rmse: 1.77543\n",
            "[200]\ttraining's rmse: 2.44863\tvalid_1's rmse: 1.74906\n",
            "[300]\ttraining's rmse: 2.41092\tvalid_1's rmse: 1.74055\n",
            "[400]\ttraining's rmse: 2.38827\tvalid_1's rmse: 1.73654\n",
            "[500]\ttraining's rmse: 2.37028\tvalid_1's rmse: 1.73267\n",
            "[600]\ttraining's rmse: 2.35533\tvalid_1's rmse: 1.729\n",
            "[700]\ttraining's rmse: 2.34226\tvalid_1's rmse: 1.72557\n",
            "[800]\ttraining's rmse: 2.32945\tvalid_1's rmse: 1.7222\n",
            "[900]\ttraining's rmse: 2.31715\tvalid_1's rmse: 1.71894\n",
            "[1000]\ttraining's rmse: 2.30688\tvalid_1's rmse: 1.71526\n",
            "[1100]\ttraining's rmse: 2.29673\tvalid_1's rmse: 1.71205\n",
            "[1200]\ttraining's rmse: 2.28722\tvalid_1's rmse: 1.7092\n",
            "[1300]\ttraining's rmse: 2.27839\tvalid_1's rmse: 1.70548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp21BAEocFe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(grid_df.d.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMes-isIAJdU",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfFvnG4nAMqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "import gc\n",
        "\n",
        "from sklearn import preprocessing\n",
        "import lightgbm as lgb\n",
        "\n",
        "from typing import Union\n",
        "from tqdm.notebook import tqdm_notebook as tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YgLPdg9ARoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WRMSSEEvaluator(object):\n",
        "\n",
        "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, \n",
        "                 calendar: pd.DataFrame, prices: pd.DataFrame):\n",
        "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
        "        train_target_columns = train_y.columns.tolist()\n",
        "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
        "\n",
        "        train_df['all_id'] = 'all'  # for lv1 aggregation\n",
        "\n",
        "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')]\\\n",
        "                     .columns.tolist()\n",
        "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')]\\\n",
        "                               .columns.tolist()\n",
        "\n",
        "        if not all([c in valid_df.columns for c in id_columns]):\n",
        "            valid_df = pd.concat([train_df[id_columns], valid_df], \n",
        "                                 axis=1, sort=False)\n",
        "\n",
        "        self.train_df = train_df\n",
        "        self.valid_df = valid_df\n",
        "        self.calendar = calendar\n",
        "        self.prices = prices\n",
        "\n",
        "        self.weight_columns = weight_columns\n",
        "        self.id_columns = id_columns\n",
        "        self.valid_target_columns = valid_target_columns\n",
        "\n",
        "        weight_df = self.get_weight_df()\n",
        "\n",
        "        self.group_ids = (\n",
        "            'all_id',\n",
        "            'state_id',\n",
        "            'store_id',\n",
        "            'cat_id',\n",
        "            'dept_id',\n",
        "            ['state_id', 'cat_id'],\n",
        "            ['state_id', 'dept_id'],\n",
        "            ['store_id', 'cat_id'],\n",
        "            ['store_id', 'dept_id'],\n",
        "            'item_id',\n",
        "            ['item_id', 'state_id'],\n",
        "            ['item_id', 'store_id']\n",
        "        )\n",
        "\n",
        "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
        "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
        "            scale = []\n",
        "            for _, row in train_y.iterrows():\n",
        "                series = row.values[np.argmax(row.values != 0):]\n",
        "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
        "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
        "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
        "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)\\\n",
        "                    [valid_target_columns].sum())\n",
        "\n",
        "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
        "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
        "\n",
        "    def get_weight_df(self) -> pd.DataFrame:\n",
        "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
        "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns]\\\n",
        "                    .set_index(['item_id', 'store_id'])\n",
        "        weight_df = weight_df.stack().reset_index()\\\n",
        "                   .rename(columns={'level_2': 'd', 0: 'value'})\n",
        "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
        "\n",
        "        weight_df = weight_df.merge(self.prices, how='left',\n",
        "                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n",
        "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
        "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd'])\\\n",
        "                    .unstack(level=2)['value']\\\n",
        "                    .loc[zip(self.train_df.item_id, self.train_df.store_id), :]\\\n",
        "                    .reset_index(drop=True)\n",
        "        weight_df = pd.concat([self.train_df[self.id_columns],\n",
        "                               weight_df], axis=1, sort=False)\n",
        "        return weight_df\n",
        "\n",
        "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
        "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
        "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
        "        scale = getattr(self, f'lv{lv}_scale')\n",
        "        return (score / scale).map(np.sqrt) \n",
        "\n",
        "    def score(self, valid_preds: Union[pd.DataFrame, \n",
        "                                       np.ndarray]) -> float:\n",
        "        assert self.valid_df[self.valid_target_columns].shape \\\n",
        "               == valid_preds.shape\n",
        "\n",
        "        if isinstance(valid_preds, np.ndarray):\n",
        "            valid_preds = pd.DataFrame(valid_preds, \n",
        "                                       columns=self.valid_target_columns)\n",
        "\n",
        "        valid_preds = pd.concat([self.valid_df[self.id_columns], \n",
        "                                 valid_preds], axis=1, sort=False)\n",
        "\n",
        "        all_scores = []\n",
        "        for i, group_id in enumerate(self.group_ids):\n",
        "\n",
        "            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n",
        "            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n",
        "            \n",
        "            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n",
        "            setattr(self, f'lv{i + 1}_scores', lv_scores)\n",
        "            \n",
        "            weight = getattr(self, f'lv{i + 1}_weight')\n",
        "            lv_scores = pd.concat([weight, lv_scores], axis=1, \n",
        "                                  sort=False).prod(axis=1)\n",
        "            \n",
        "            all_scores.append(lv_scores.sum())\n",
        "            \n",
        "        self.all_scores = all_scores\n",
        "\n",
        "        return np.mean(all_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I63PvJ4h-bq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Predict\n",
        "#################################################################################\n",
        "\n",
        "#for fold_ in CV_FOLDS:\n",
        "for fold_ in range(1,4):\n",
        "    print(\"FOLD:\", fold_)\n",
        "    # Join back the Test dataset with \n",
        "    # a small part of the training data \n",
        "    # to make recursive features\n",
        "    all_preds = pd.DataFrame()\n",
        "    base_test = get_base_test()\n",
        "    # Timer to measure predictions time \n",
        "    main_time = time.time()\n",
        "\n",
        "    # Loop over each prediction day\n",
        "    # As rolling lags are the most timeconsuming\n",
        "    # we will calculate it for whole day\n",
        "    for PREDICT_DAY in range(1,29):    \n",
        "        print('Predict | Day:', PREDICT_DAY, (END_TRAIN+PREDICT_DAY-28))\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Make temporary grid to calculate rolling lags\n",
        "        grid_df = base_test.copy()\n",
        "        grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "\n",
        "        for store_id in STORES_IDS:\n",
        "        \n",
        "            # Read all our models and make predictions\n",
        "            # for each day/store pairs\n",
        "            model_path =  AUX_MODELS + 'lgb_model_'+store_id+'_'+str(fold_)+'_'+str(VER)+'.bin' \n",
        "            if USE_AUX:\n",
        "                model_path =  model_path\n",
        "\n",
        "            estimator = pickle.load(open(model_path, 'rb'))\n",
        "\n",
        "            day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY-28)\n",
        "            store_mask = base_test['store_id']==store_id\n",
        "\n",
        "            mask = (day_mask)&(store_mask)\n",
        "            base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "            #base_test[TARGET][mask]=base_test[TARGET][mask]/base_test[\"sell_price\"][mask]\n",
        "\n",
        "        # Make good column naming and add \n",
        "        # to all_preds DataFrame\n",
        "        temp_df = base_test[day_mask][['id',TARGET]]\n",
        "        temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "        if 'id' in list(all_preds):\n",
        "            all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "        else:\n",
        "            all_preds = temp_df.copy()\n",
        "        all_preds = all_preds.reset_index(drop=True)\n",
        "        print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                      ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                      ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    all_preds.to_csv('all_preds_valid_'+str(fold_)+'.csv',index=False)\n",
        "    del temp_df, all_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vbbWLVWKHYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_preds_1=pd.read_csv('all_preds_valid_1'+'.csv')\n",
        "all_preds_2=pd.read_csv('all_preds_valid_2'+'.csv')\n",
        "all_preds_3=pd.read_csv('all_preds_valid_3'+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUwjqzfQKejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dummy DataFrame to store predictions\n",
        "final_all_preds = pd.DataFrame()\n",
        "final_all_preds['id'] = all_preds_1['id']\n",
        "for item in all_preds_1:\n",
        "    if item!='id':\n",
        "        final_all_preds[item]=(all_preds_1[item]*(3/3))+(all_preds_2[item]*(0.0/3))+(all_preds_3[item]*(0.0/3))\n",
        "final_all_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxXx_HDkAX1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reload data\n",
        "d_dtypes = {}\n",
        "for i in range(1914):\n",
        "    d_dtypes[f'd_{i}'] = np.int32\n",
        "sales = pd.read_csv(DIRPATH + 'sales_train_validation.csv',\n",
        "                    dtype=d_dtypes)\n",
        "\n",
        "# changing wide format to long format for model training\n",
        "d = ['d_' + str(i) for i in range(1802,1914)]\n",
        "sales_mlt = pd.melt(sales, id_vars=['item_id','dept_id','cat_id','store_id',\n",
        "                                    'state_id'], value_vars=d)\n",
        "sales_mlt = sales_mlt.rename(columns={'variable':'d', 'value':'sales'})\n",
        "\n",
        "calendar = pd.read_csv(DIRPATH + 'calendar.csv',\n",
        "                       dtype={'wm_yr_wk': np.int32, 'wday': np.int32, \n",
        "                              'month': np.int32, 'year': np.int32, \n",
        "                              'snap_CA': np.int32, 'snap_TX': np.int32,\n",
        "                              'snap_WI': np.int32})\n",
        "\n",
        "# subsetting calender by traning period\n",
        "calendar = calendar.loc[calendar.d.apply(lambda x: int(x[2:])) \\\n",
        "                        >= int(sales_mlt.d[0][2:]), :]\n",
        "\n",
        "prices = pd.read_csv(DIRPATH + 'sell_prices.csv',\n",
        "                          dtype={'wm_yr_wk': np.int32, \n",
        "                                 'sell_price': np.float32})\n",
        "\n",
        "train_df = sales.iloc[:, :-28]\n",
        "valid_df = sales.iloc[:, -28:]\n",
        "\n",
        "evaluator = WRMSSEEvaluator(train_df, valid_df, calendar, prices)\n",
        "\n",
        "#sort\n",
        "all_preds=pd.merge(train_df[\"id\"],final_all_preds,on=\"id\",how=\"left\")\n",
        "all_preds=all_preds.drop(\"id\",axis=1)\n",
        "\n",
        "WRMSSEE = evaluator.score(all_preds.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwPWKYBmAaaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_viz_df(df,lv):\n",
        "    \n",
        "    df = df.T.reset_index()\n",
        "    if lv in [6,7,8,9,11,12]:\n",
        "        df.columns = [i[0] + '_' + i[1] if i != ('index','') \\\n",
        "                      else i[0] for i in df.columns]\n",
        "    df = df.merge(calendar.loc[:, ['d','date']], how='left', \n",
        "                  left_on='index', right_on='d')\n",
        "    df['date'] = pd.to_datetime(df.date)\n",
        "    df = df.set_index('date')\n",
        "    df = df.drop(['index', 'd'], axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def create_dashboard(evaluator):\n",
        "    \n",
        "    wrmsses = [np.mean(evaluator.all_scores)] + evaluator.all_scores\n",
        "    labels = ['Overall'] + [f'Level {i}' for i in range(1, 13)]\n",
        "\n",
        "    ## WRMSSE by Level\n",
        "    plt.figure(figsize=(12,5))\n",
        "    ax = sns.barplot(x=labels, y=wrmsses)\n",
        "    ax.set(xlabel='', ylabel='WRMSSE')\n",
        "    plt.title('WRMSSE by Level', fontsize=20, fontweight='bold')\n",
        "    for index, val in enumerate(wrmsses):\n",
        "        ax.text(index*1, val+.01, round(val,4), color='black', \n",
        "                ha=\"center\")\n",
        "        \n",
        "    # configuration array for the charts\n",
        "    n_rows = [1, 1, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n",
        "    n_cols = [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
        "    width = [7, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
        "    height = [4, 3, 12, 3, 9, 9, 9, 9, 9, 9, 9, 9]\n",
        "    \n",
        "    for i in range(1,13):\n",
        "        \n",
        "        scores = getattr(evaluator, f'lv{i}_scores')\n",
        "        weights = getattr(evaluator, f'lv{i}_weight')\n",
        "        \n",
        "        if i > 1 and i < 9:\n",
        "            if i < 7:\n",
        "                fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n",
        "            else:\n",
        "                fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
        "                \n",
        "            ## RMSSE plot\n",
        "            scores.plot.bar(width=.8, ax=axs[0], color='g')\n",
        "            axs[0].set_title(f\"RMSSE\", size=14)\n",
        "            axs[0].set(xlabel='', ylabel='RMSSE')\n",
        "            if i >= 4:\n",
        "                axs[0].tick_params(labelsize=8)\n",
        "            for index, val in enumerate(scores):\n",
        "                axs[0].text(index*1, val+.01, round(val,4), color='black', \n",
        "                            ha=\"center\", fontsize=10 if i == 2 else 8)\n",
        "            \n",
        "            ## Weight plot\n",
        "            weights.plot.bar(width=.8, ax=axs[1])\n",
        "            axs[1].set_title(f\"Weight\", size=14)\n",
        "            axs[1].set(xlabel='', ylabel='Weight')\n",
        "            if i >= 4:\n",
        "                axs[1].tick_params(labelsize=8)\n",
        "            for index, val in enumerate(weights):\n",
        "                axs[1].text(index*1, val+.01, round(val,2), color='black', \n",
        "                            ha=\"center\", fontsize=10 if i == 2 else 8)\n",
        "                    \n",
        "            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 ,\n",
        "                         y=1.1, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        trn = create_viz_df(getattr(evaluator, f'lv{i}_train_df')\\\n",
        "                            .iloc[:, -28*3:], i)\n",
        "        val = create_viz_df(getattr(evaluator, f'lv{i}_valid_df'), i)\n",
        "        pred = create_viz_df(getattr(evaluator, f'lv{i}_valid_preds'), i)\n",
        "\n",
        "        n_cate = trn.shape[1] if i < 7 else 9\n",
        "\n",
        "        fig, axs = plt.subplots(n_rows[i-1], n_cols[i-1], \n",
        "                                figsize=(width[i-1],height[i-1]))\n",
        "        if i > 1:\n",
        "            axs = axs.flatten()\n",
        "\n",
        "        ## Time series plot\n",
        "        for k in range(0, n_cate):\n",
        "\n",
        "            ax = axs[k] if i > 1 else axs\n",
        "\n",
        "            trn.iloc[:, k].plot(ax=ax, label='train')\n",
        "            val.iloc[:, k].plot(ax=ax, label='valid')\n",
        "            pred.iloc[:, k].plot(ax=ax, label='pred')\n",
        "            ax.set_title(f\"{trn.columns[k]}  RMSSE:{scores[k]:.4f}\", size=14)\n",
        "            ax.set(xlabel='', ylabel='sales')\n",
        "            ax.tick_params(labelsize=8)\n",
        "            ax.legend(loc='upper left', prop={'size': 10})\n",
        "\n",
        "        if i == 1 or i >= 9:\n",
        "            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 , \n",
        "                         y=1.1, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0uztygSAb0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_dashboard(evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUbcLxA5FUsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Predict\n",
        "#################################################################################\n",
        "\n",
        "for fold_ in range(1,4):\n",
        "    print(\"FOLD:\", fold_)\n",
        "    # Join back the Test dataset with \n",
        "    # a small part of the training data \n",
        "    # to make recursive features\n",
        "    all_preds = pd.DataFrame()\n",
        "    base_test = get_base_test()\n",
        "    # Timer to measure predictions time \n",
        "    main_time = time.time()\n",
        "\n",
        "    # Loop over each prediction day\n",
        "    # As rolling lags are the most timeconsuming\n",
        "    # we will calculate it for whole day\n",
        "    for PREDICT_DAY in range(1,29):    \n",
        "        print('Predict | Day:', PREDICT_DAY)\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Make temporary grid to calculate rolling lags\n",
        "        grid_df = base_test.copy()\n",
        "        grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "\n",
        "        for store_id in STORES_IDS:\n",
        "        \n",
        "            # Read all our models and make predictions\n",
        "            # for each day/store pairs\n",
        "            model_path =  AUX_MODELS + 'lgb_model_'+store_id+'_'+str(fold_)+'_'+str(VER)+'.bin' \n",
        "            if USE_AUX:\n",
        "                model_path =  model_path\n",
        "\n",
        "            estimator = pickle.load(open(model_path, 'rb'))\n",
        "\n",
        "            day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
        "            store_mask = base_test['store_id']==store_id\n",
        "\n",
        "            mask = (day_mask)&(store_mask)\n",
        "            base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "            #base_testt[TARGET][mask]=base_test[TARGET][mask]/base_test[\"sell_price\"][mask]\n",
        "\n",
        "        # Make good column naming and add \n",
        "        # to all_preds DataFrame\n",
        "        temp_df = base_test[day_mask][['id',TARGET]]\n",
        "        temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "        if 'id' in list(all_preds):\n",
        "            all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "        else:\n",
        "            all_preds = temp_df.copy()\n",
        "        all_preds = all_preds.reset_index(drop=True)\n",
        "        print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                      ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                      ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    all_preds.to_csv('all_preds_'+str(fold_)+'.csv',index=False)\n",
        "    del temp_df, all_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4za7WyNXiUwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(MODEL_FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7j_wIZ6FUsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_preds_1=pd.read_csv('all_preds_1'+'.csv')\n",
        "all_preds_2=pd.read_csv('all_preds_2'+'.csv')\n",
        "all_preds_3=pd.read_csv('all_preds_3'+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QurhrKb_FUsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dummy DataFrame to store predictions\n",
        "final_all_preds = pd.DataFrame()\n",
        "final_all_preds['id'] = all_preds_1['id']\n",
        "for item in all_preds_1:\n",
        "    if item!='id':\n",
        "        #final_all_preds[item]=(all_preds_0[item]*(1/3))+(all_preds_1[item]*(0.9/3))+(all_preds_2[item]*(0.95/3))\n",
        "        final_all_preds[item]=(all_preds_1[item]*(1/3))+(all_preds_2[item]*(1/3))+(all_preds_3[item]*(1/3))\n",
        "final_all_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV9vGjUhFUsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Export\n",
        "#################################################################################\n",
        "# Reading competition sample submission and\n",
        "# merging our predictions\n",
        "# As we have predictions only for \"_validation\" data\n",
        "# we need to do fillna() for \"_evaluation\" items\n",
        "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
        "submission[\"id\"]=submission[\"id\"].str.replace(\"validation\",\"evaluation\")\n",
        "submission = submission.merge(final_all_preds, on=['id'], how='left').fillna(0)\n",
        "submission.to_csv(AUX_MODELS+'/submission_v'+str(VER)+'.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfNKkNaglzzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffRUqIWjFUsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary\n",
        "\n",
        "# Of course here is no magic at all.\n",
        "# No \"Novel\" features and no brilliant ideas.\n",
        "# We just carefully joined all\n",
        "# our previous fe work and created a model.\n",
        "\n",
        "# Also!\n",
        "# In my opinion this strategy is a \"dead end\".\n",
        "# Overfits a lot LB and with 1 final submission \n",
        "# you have no option to risk.\n",
        "\n",
        "\n",
        "# Improvement should come from:\n",
        "# Loss function\n",
        "# Data representation\n",
        "# Stable CV\n",
        "# Good features reduction strategy\n",
        "# Predictions stabilization with NN\n",
        "# Trend prediction\n",
        "# Real zero sales detection/classification\n",
        "\n",
        "\n",
        "# Good kernels references \n",
        "## (the order is random and the list is not complete):\n",
        "# https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
        "# https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
        "# https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
        "# https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n",
        "# https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n",
        "# https://www.kaggle.com/yassinealouini/seq2seq\n",
        "# https://www.kaggle.com/kailex/m5-forecaster-v2\n",
        "# https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n",
        "\n",
        "\n",
        "# Features were created in these kernels:\n",
        "## \n",
        "# Mean encodings and PCA options\n",
        "# https://www.kaggle.com/kyakovlev/m5-custom-features\n",
        "##\n",
        "# Lags and rolling lags\n",
        "# https://www.kaggle.com/kyakovlev/m5-lags-features\n",
        "##\n",
        "# Base Grid and base features (calendar/price/etc)\n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "\n",
        "\n",
        "# Personal request\n",
        "# Please don't upvote any ensemble and copypaste kernels\n",
        "## The worst case is ensemble without any analyse.\n",
        "## The best choice - just ignore it.\n",
        "## I would like to see more kernels with interesting and original approaches.\n",
        "## Don't feed copypasters with upvotes.\n",
        "\n",
        "## It doesn't mean that you should not fork and improve others kernels\n",
        "## but I would like to see params and code tuning based on some CV and analyse\n",
        "## and not only on LB probing.\n",
        "## Small changes could be shared in comments and authors can improve their kernel.\n",
        "\n",
        "## Feel free to criticize this kernel as my knowlege is very limited\n",
        "## and I can be wrong in code and descriptions. \n",
        "## Thank you."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9TW9lsnGkFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  MODEL_FEATURES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI_NbjRrBeCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B5QHhsmBpXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}