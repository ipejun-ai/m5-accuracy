{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "m5-custom-features.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipejun-ai/m5-accuracy/blob/master/m5_custom_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND3qXCXvSL_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## In this kernel I would like to show: \n",
        "## 1. FE creation approaches\n",
        "## 2. Sequential fe validation\n",
        "## 3. Dimension reduction\n",
        "## 4. FE validation by Permutation importance\n",
        "## 5. Mean encodings\n",
        "## 6. Parallelization for FE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "t30SwvhXSL_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os, sys, gc, warnings, psutil, random\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDah0n0xSRMO",
        "colab_type": "code",
        "outputId": "ad398efd-9e38-4e55-ceb8-1929aabe5c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#Load google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jOgmCfMSYiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIRPATH=\"/content/gdrive/My Drive/kaggle/\"\n",
        "#DIRPATH=\"C:/Users/peiju/Documents/Study/kaggle/m5-forecasting-accuracy/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "_d8GZ-c_SMAA",
        "colab_type": "code",
        "outputId": "cafdbfd0-bc96-4b96-886e-9b68128a6a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "########################### Load data\n",
        "########################### Basic features were created here:\n",
        "########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "#################################################################################\n",
        "\n",
        "# Read data\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "# Subsampling\n",
        "# to make all calculations faster.\n",
        "# Keep only 5% of original ids.\n",
        "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
        "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
        "\n",
        "# Let's \"inspect\" our grid DataFrame\n",
        "grid_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2960025 entries, 0 to 2960024\n",
            "Data columns (total 34 columns):\n",
            " #   Column            Dtype   \n",
            "---  ------            -----   \n",
            " 0   id                category\n",
            " 1   item_id           category\n",
            " 2   dept_id           category\n",
            " 3   cat_id            category\n",
            " 4   store_id          category\n",
            " 5   state_id          category\n",
            " 6   d                 int16   \n",
            " 7   sales             float64 \n",
            " 8   release           int16   \n",
            " 9   sell_price        float16 \n",
            " 10  price_max         float16 \n",
            " 11  price_min         float16 \n",
            " 12  price_std         float16 \n",
            " 13  price_mean        float16 \n",
            " 14  price_norm        float16 \n",
            " 15  price_nunique     float16 \n",
            " 16  item_nunique      int16   \n",
            " 17  price_momentum    float16 \n",
            " 18  price_momentum_m  float16 \n",
            " 19  price_momentum_y  float16 \n",
            " 20  event_name_1      category\n",
            " 21  event_type_1      category\n",
            " 22  event_name_2      category\n",
            " 23  event_type_2      category\n",
            " 24  snap_CA           category\n",
            " 25  snap_TX           category\n",
            " 26  snap_WI           category\n",
            " 27  tm_d              int8    \n",
            " 28  tm_w              int8    \n",
            " 29  tm_m              int8    \n",
            " 30  tm_y              int8    \n",
            " 31  tm_wm             int8    \n",
            " 32  tm_dw             int8    \n",
            " 33  tm_w_end          int8    \n",
            "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
            "memory usage: 159.7 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShWtUl3LSMAS",
        "colab_type": "code",
        "outputId": "b36a467b-4033-4300-ed71-587027a90308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "########################### Baseline model\n",
        "#################################################################################\n",
        "\n",
        "# We will need some global VARS for future\n",
        "\n",
        "SEED = 42             # Our random seed for everything\n",
        "random.seed(SEED)     # to make all tests \"deterministic\"\n",
        "np.random.seed(SEED)\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "TARGET = 'sales'      # Our Target\n",
        "END_TRAIN = 1913      # And we will use last 28 days as validation\n",
        "\n",
        "# Drop some items from \"TEST\" set part (1914...)\n",
        "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
        "\n",
        "# Features that we want to exclude from training\n",
        "remove_features = ['id','d',TARGET]\n",
        "\n",
        "# Our baseline model serves\n",
        "# to do fast checks of\n",
        "# new features performance \n",
        "\n",
        "# We will use LightGBM for our tests\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
        "                    'objective': 'regression',       # Standart loss for RMSE\n",
        "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
        "                    'subsample': 0.8,                \n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
        "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
        "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
        "                    'feature_fraction': 0.8,\n",
        "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
        "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
        "                    'seed': SEED,\n",
        "                    'verbose': -1,\n",
        "                } \n",
        "\n",
        "## RMSE\n",
        "def rmse(y, y_pred):\n",
        "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
        "\n",
        "# Small function to make fast features tests\n",
        "# estimator = make_fast_test(grid_df)\n",
        "# it will return lgb booster for future analisys\n",
        "def make_fast_test(df):\n",
        "\n",
        "    features_columns = [col for col in list(df) if col not in remove_features]\n",
        "\n",
        "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n",
        "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
        "    \n",
        "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
        "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
        "    \n",
        "    estimator = lgb.train(\n",
        "                            lgb_params,\n",
        "                            train_data,\n",
        "                            valid_sets = [train_data,valid_data],\n",
        "                            verbose_eval = 500,\n",
        "                        )\n",
        "    \n",
        "    return estimator\n",
        "\n",
        "# Make baseline model\n",
        "baseline_model = make_fast_test(grid_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[318]\ttraining's rmse: 2.82659\tvalid_1's rmse: 2.38847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4V8HPqvSMAm",
        "colab_type": "code",
        "outputId": "8761c414-37c9-46f3-fb1e-9ae585035332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "########################### Lets test our normal Lags (7 days)\n",
        "########################### Some more info about lags here:\n",
        "########################### https://www.kaggle.com/kyakovlev/m5-lags-features\n",
        "#################################################################################\n",
        "\n",
        "# Small helper to make lags creation faster\n",
        "from multiprocessing import Pool                # Multiprocess Runs\n",
        "\n",
        "## Multiprocessing Run.\n",
        "# :t_split - int of lags days                   # type: int\n",
        "# :func - Function to apply on each split       # type: python function\n",
        "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "def make_normal_lag(lag_day):\n",
        "    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n",
        "    col_name = 'sales_lag_'+str(lag_day)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "# Launch parallel lag creation\n",
        "# and \"append\" to our grid\n",
        "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[350]\ttraining's rmse: 2.55955\tvalid_1's rmse: 2.2627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DDAWPhvSMBC",
        "colab_type": "code",
        "outputId": "2e276aec-196d-4e8d-c483-3ae435af372d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "########################### Permutation importance Test\n",
        "########################### https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "#################################################################################\n",
        "\n",
        "# Let's creat validation dataset and features\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "\n",
        "# Make normal prediction with our model and save score\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "\n",
        "# Now we are looping over all our numerical features\n",
        "for col in features_columns:\n",
        "    \n",
        "    # We will make validation set copy to restore\n",
        "    # features states on each run\n",
        "    temp_df = validation_df.copy()\n",
        "    \n",
        "    # Error here appears if we have \"categorical\" features and can't \n",
        "    # do np.random.permutation without disrupt categories\n",
        "    # so we need to check if feature is numerical\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        \n",
        "        # If our current rmse score is less than base score\n",
        "        # it means that feature most probably is a bad one\n",
        "        # and our model is learning on noise\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "# Remove Temp data\n",
        "del temp_df, validation_df\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 1 days shift (nearest past) are important\n",
        "## Some other features are not important and probably just noise\n",
        "## Better make several Permutation runs to confirm useless of the feature\n",
        "## link again https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "\n",
        "## price_nunique -0.002 : strong negative values are most probably noise\n",
        "## price_max -0.0002 : values close to 0 need deeper investigation\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standart RMSE 2.262703836420032\n",
            "release 0.0\n",
            "sell_price 0.0031\n",
            "price_max 0.0002\n",
            "price_min -0.0003\n",
            "price_std 0.0056\n",
            "price_mean 0.0041\n",
            "price_norm 0.0085\n",
            "price_nunique -0.0002\n",
            "item_nunique 0.0012\n",
            "price_momentum 0.0003\n",
            "price_momentum_m 0.0073\n",
            "price_momentum_y 0.0013\n",
            "tm_d 0.0034\n",
            "tm_w -0.0002\n",
            "tm_m 0.0008\n",
            "tm_y 0.0\n",
            "tm_wm 0.0002\n",
            "tm_dw 0.1043\n",
            "tm_w_end 0.0122\n",
            "sales_lag_1 0.4644\n",
            "sales_lag_2 0.0232\n",
            "sales_lag_3 0.005\n",
            "sales_lag_4 0.0133\n",
            "sales_lag_5 0.0154\n",
            "sales_lag_6 0.0118\n",
            "sales_lag_7 0.0334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJph3WcmSMBT",
        "colab_type": "text"
      },
      "source": [
        "from eli5 documentation (seems it's perfect explanation)\n",
        "\n",
        "The idea is the following: feature importance can be measured by looking at how much the score (accuracy, mse, rmse, mae, etc. - any score we’re interested in) decreases when a feature is not available.\n",
        "\n",
        "To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n",
        "\n",
        "To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can **replace it with random noise** - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the **same distribution as original feature values** (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
        "\n",
        "---\n",
        "\n",
        "It's not good when feature remove (replaced by noise) but we have better score. Simple and easy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ExQEP7OSMBV",
        "colab_type": "code",
        "outputId": "e680ccaf-8547-4527-bd6c-f7e34c4013d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "########################### Lets test far away Lags (7 days with 56 days shift)\n",
        "########################### and check permutation importance\n",
        "#################################################################################\n",
        "\n",
        "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "for col in features_columns:\n",
        "    temp_df = validation_df.copy()\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "del temp_df, validation_df\n",
        "        \n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 56 days shift (far away past) are not as important\n",
        "## as nearest past lags\n",
        "## and at some point will be just noise for our model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[374]\ttraining's rmse: 2.80503\tvalid_1's rmse: 2.38908\n",
            "Standart RMSE 2.389082243091218\n",
            "release 0.0\n",
            "sell_price 0.0081\n",
            "price_max 0.0119\n",
            "price_min 0.0029\n",
            "price_std 0.0093\n",
            "price_mean -0.0009\n",
            "price_norm 0.0136\n",
            "price_nunique 0.0134\n",
            "item_nunique 0.0048\n",
            "price_momentum 0.0008\n",
            "price_momentum_m 0.0281\n",
            "price_momentum_y 0.0084\n",
            "tm_d 0.0053\n",
            "tm_w 0.0018\n",
            "tm_m 0.003\n",
            "tm_y 0.0\n",
            "tm_wm 0.0003\n",
            "tm_dw 0.1034\n",
            "tm_w_end 0.0177\n",
            "sales_lag_56 0.0093\n",
            "sales_lag_57 0.0031\n",
            "sales_lag_58 0.0109\n",
            "sales_lag_59 0.0065\n",
            "sales_lag_60 0.0\n",
            "sales_lag_61 0.0002\n",
            "sales_lag_62 0.0056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir1VAgGvSMBn",
        "colab_type": "code",
        "outputId": "e99a9631-06ec-48be-deb2-1eb398fd3414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "########################### PCA\n",
        "#################################################################################\n",
        "\n",
        "# The main question here - can we have \n",
        "# almost same rmse boost with less features\n",
        "# less dimensionality?\n",
        "\n",
        "# Lets try PCA and make 7->3 dimensionality reduction\n",
        "\n",
        "# PCA is \"unsupervised\" learning\n",
        "# and with shifted target we can be sure\n",
        "# that we have no Target leakage\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def make_pca(df, pca_col, n_days):\n",
        "    print('PCA:', pca_col, n_days)\n",
        "    \n",
        "    # We don't need any other columns to make pca\n",
        "    pca_df = df[[pca_col,'d',TARGET]]\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\" \n",
        "    # we need to agg first\n",
        "    if pca_col != 'id':\n",
        "        merge_base = pca_df[[pca_col,'d']]\n",
        "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
        "        pca_df[TARGET] = pca_df['sum']\n",
        "        del pca_df['sum']\n",
        "    \n",
        "    # Min/Max scaling\n",
        "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
        "    \n",
        "    # Making \"lag\" in old way (not parallel)\n",
        "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
        "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
        "    pca_df = pca_df.assign(**{\n",
        "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
        "            for l in LAG_DAYS\n",
        "            for col in [TARGET]\n",
        "        })\n",
        "    \n",
        "    pca_columns = list(pca_df)[3:]\n",
        "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
        "    pca = PCA(random_state=SEED)\n",
        "    \n",
        "    # You can use fit_transform here\n",
        "    pca.fit(pca_df[pca_columns])\n",
        "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
        "    \n",
        "    print(pca.explained_variance_ratio_)\n",
        "    \n",
        "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
        "    keep_cols = pca_columns[:3]\n",
        "    print('Columns to keep:', keep_cols)\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\"\n",
        "    # we need merge back our results to merge_base df\n",
        "    # and only than return resulted df\n",
        "    # I'll skip that step here\n",
        "    \n",
        "    return pca_df[keep_cols]\n",
        "\n",
        "\n",
        "# Make PCA\n",
        "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA: id 7\n",
            "[0.72243389 0.06622603 0.05933126 0.04200092 0.0388851  0.03610057\n",
            " 0.03502223]\n",
            "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[289]\ttraining's rmse: 2.64974\tvalid_1's rmse: 2.27982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqPDlCQqSMB1",
        "colab_type": "code",
        "outputId": "312346d3-0436-4182-cbc5-a324fab4d5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "########################### Mean/std target encoding\n",
        "#################################################################################\n",
        "\n",
        "# We will use these three columns for test\n",
        "# (in combination with store_id)\n",
        "icols = ['item_id','cat_id','dept_id']\n",
        "\n",
        "# But we can use any other column or even multiple groups\n",
        "# like these ones\n",
        "#            'state_id',\n",
        "#            'store_id',\n",
        "#            'cat_id',\n",
        "#            'dept_id',\n",
        "#            ['state_id', 'cat_id'],\n",
        "#            ['state_id', 'dept_id'],\n",
        "#            ['store_id', 'cat_id'],\n",
        "#            ['store_id', 'dept_id'],\n",
        "#            'item_id',\n",
        "#            ['item_id', 'state_id'],\n",
        "#            ['item_id', 'store_id']\n",
        "\n",
        "# There are several ways to do \"mean\" encoding\n",
        "## K-fold scheme\n",
        "## LOO (leave one out)\n",
        "## Smoothed/regularized \n",
        "## Expanding mean\n",
        "## etc \n",
        "\n",
        "# You can test as many options as you want\n",
        "# and decide what to use\n",
        "# Because of memory issues you can't \n",
        "# use many features.\n",
        "\n",
        "# We will use simple target encoding\n",
        "# by std and mean agg\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n",
        "    \n",
        "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
        "    joiner = '_'+col+'_encoding_'\n",
        "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
        "    temp_df = temp_df.reset_index()\n",
        "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
        "    del temp_df\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "# Bad thing that for some items  \n",
        "# we are using past and future values.\n",
        "# But we are looking for \"categorical\" similiarity\n",
        "# on a \"long run\". So future here is not a big problem."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding item_id\n",
            "Encoding cat_id\n",
            "Encoding dept_id\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[427]\ttraining's rmse: 2.76863\tvalid_1's rmse: 2.38584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ZdSmM0SMCD",
        "colab_type": "code",
        "outputId": "8fb2c08c-d5c4-4331-bc7a-36a57c007fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Last non O sale\n",
        "#################################################################################\n",
        "\n",
        "def find_last_sale(df,n_day):\n",
        "    \n",
        "    # Limit initial df\n",
        "    ls_df = df[['id','d',TARGET]]\n",
        "    \n",
        "    # Convert target to binary\n",
        "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
        "    \n",
        "    # Make lags to prevent any leakage\n",
        "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
        "\n",
        "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
        "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
        "\n",
        "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
        "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
        "\n",
        "    return ls_df[['last_sale']]\n",
        "\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's rmse: 4.29039\tvalid_1's rmse: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9clO5-XFQDvH",
        "colab_type": "code",
        "outputId": "9d5c8556-8dd4-4683-d2b1-300d107cfc87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#######snap\n",
        "def sum_snap(df):\n",
        "  df[\"snap\"]=0\n",
        "  df[\"snap\"].where(df['state_id'] != 'CA',df[\"snap_CA\"],inplace=True)\n",
        "  df[\"snap\"].where(df['state_id'] != 'TX',df[\"snap_TX\"],inplace=True)\n",
        "  df[\"snap\"].where(df['state_id'] != 'WI',df[\"snap_WI\"],inplace=True)\n",
        "  df.drop([\"snap_CA\",\"snap_TX\",\"snap_WI\"],axis=1)\n",
        "  return df\n",
        "\n",
        "grid_df1=sum_snap(grid_df)\n",
        "\n",
        " # Make features test\n",
        "test_model = make_fast_test(grid_df1) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's rmse: 4.93986\tvalid_1's rmse: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp1uobA_SzAQ",
        "colab_type": "code",
        "outputId": "6c76b836-9380-498b-e1b9-a359c71a59a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "def change_event(df):\n",
        "  event_type = pd.read_pickle(\"/content/gdrive/My Drive/kaggle/df_event_type.pickle.gz\")\n",
        "  event_type_columns=event_type.columns\n",
        "  event_type_columns=event_type_columns.drop(\"d\")\n",
        "\n",
        "  event_type[event_type_columns]=event_type[event_type_columns].astype(\"category\")\n",
        "\n",
        "  event_type[\"d\"]=event_type[\"d\"].str.replace(\"d_\",\"\").astype(\"int16\")\n",
        "  df=pd.merge(df,event_type,left_on=\"d\",right_on=\"d\",how=\"left\")\n",
        "  return df\n",
        "\n",
        "grid_df=change_event(grid_df)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's rmse: 4.93986\tvalid_1's rmse: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAYBTKF8aw3V",
        "colab_type": "code",
        "outputId": "13c533e8-4695-43ec-d9f3-d517dabd7b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "def add_event(df):\n",
        "  event_value = pd.read_pickle(\"/content/gdrive/My Drive/kaggle/output/event_importance/event_value.pkl\")\n",
        "  \n",
        "  event_value_columns=event_value.columns\n",
        "  event_value_columns=event_value_columns.drop(\"d\")\n",
        "\n",
        "  event_value_columns=[\"Event_total\"]\n",
        "\n",
        "  event_value[event_value_columns]=event_value[event_value_columns].astype(\"float16\")\n",
        "\n",
        "  event_value[\"d\"]=event_value[\"d\"].str.replace(\"d_\",\"\").astype(\"int16\")\n",
        "  #cols=[\"d\", 'Sports', 'Religious', 'National', 'Cultural']\n",
        "  cols=[\"d\",\"Event_total\"]\n",
        "  df=pd.merge(df,event_value[cols],left_on=\"d\",right_on=\"d\",how=\"left\")\n",
        "  return df\n",
        "\n",
        "grid_df1=add_event(grid_df)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df1) \n",
        "\n",
        "\n",
        "features_columns = [col for col in list(grid_df1) if col not in remove_features]\n",
        "validation_df = grid_df1[grid_df1['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "for col in features_columns:\n",
        "    temp_df = validation_df.copy()\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "del temp_df, validation_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[321]\ttraining's rmse: 2.81841\tvalid_1's rmse: 2.38378\n",
            "Standart RMSE 2.383784659504859\n",
            "release 0.0\n",
            "sell_price 0.0152\n",
            "price_max 0.0347\n",
            "price_min 0.0111\n",
            "price_std 0.0298\n",
            "price_mean 0.0111\n",
            "price_norm 0.0123\n",
            "price_nunique 0.0081\n",
            "item_nunique 0.0013\n",
            "price_momentum 0.0004\n",
            "price_momentum_m 0.022\n",
            "price_momentum_y 0.0122\n",
            "tm_d 0.0081\n",
            "tm_w 0.0035\n",
            "tm_m 0.0018\n",
            "tm_y 0.0\n",
            "tm_wm 0.0003\n",
            "tm_dw 0.1746\n",
            "tm_w_end 0.0081\n",
            "Event_total 0.0018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKKvw9Pe306",
        "colab_type": "code",
        "outputId": "537dff5a-8793-49d1-e1bb-f9186b685b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "event_value = pd.read_pickle(\"/content/gdrive/My Drive/kaggle/output/event_importance/event_value.pkl\")\n",
        "event_value.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['d', 'Chanukah End', 'Christmas', 'ChristmasEve', 'Cinco De Mayo',\n",
              "       'ColumbusDay', 'Easter', 'Eid al-Fitr', 'EidAlAdha', 'Father's day',\n",
              "       'Halloween', 'IndependenceDay', 'LaborDay', 'LentStart', 'LentWeek2',\n",
              "       'MartinLutherKingDay', 'MemorialDay', 'Mother's day', 'NBAFinalsEnd',\n",
              "       'NBAFinalsStart', 'NewYear', 'OrthodoxChristmas', 'OrthodoxEaster',\n",
              "       'Pesach End', 'PresidentsDay', 'Purim End', 'Ramadan starts',\n",
              "       'StPatricksDay', 'SuperBowl', 'Thanksgiving', 'ValentinesDay',\n",
              "       'VeteransDay', 'NBAFinals', 'Sports', 'Religious', 'National',\n",
              "       'Cultural', 'Event_total'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzvs3ieuSMCO",
        "colab_type": "code",
        "outputId": "cb3b048a-ce59-47d5-8dea-5a00acf2adce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl')\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "icols =  [\n",
        "            ['state_id'],\n",
        "            ['store_id'],\n",
        "            ['cat_id'],\n",
        "            ['dept_id'],\n",
        "            ['state_id', 'cat_id'],\n",
        "            ['state_id', 'dept_id'],\n",
        "            ['store_id', 'cat_id'],\n",
        "            ['store_id', 'dept_id'],\n",
        "            ['item_id'],\n",
        "            ['item_id', 'state_id'],\n",
        "            ['item_id', 'store_id']\n",
        "            ]\n",
        "\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    col_name = '_'+'_'.join(col)+'_'\n",
        "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
        "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding ['state_id']\n",
            "Encoding ['store_id']\n",
            "Encoding ['cat_id']\n",
            "Encoding ['dept_id']\n",
            "Encoding ['state_id', 'cat_id']\n",
            "Encoding ['state_id', 'dept_id']\n",
            "Encoding ['store_id', 'cat_id']\n",
            "Encoding ['store_id', 'dept_id']\n",
            "Encoding ['item_id']\n",
            "Encoding ['item_id', 'state_id']\n",
            "Encoding ['item_id', 'store_id']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEF99yfcSMCa",
        "colab_type": "code",
        "outputId": "ae025ed8-3dfc-4fd8-fbd2-393ce3ebe3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#################################################################################\n",
        "print('Save Mean/Std encoding')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/mean_encoding_df.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Mean/Std encoding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr6C4021SMC1",
        "colab_type": "code",
        "outputId": "808fd918-fcd0-4861-a513-5e7ad830ce6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "########################### Final list of new features\n",
        "#################################################################################\n",
        "grid_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 24 columns):\n",
            " #   Column                     Dtype   \n",
            "---  ------                     -----   \n",
            " 0   id                         category\n",
            " 1   d                          int16   \n",
            " 2   enc_state_id_mean          float16 \n",
            " 3   enc_state_id_std           float16 \n",
            " 4   enc_store_id_mean          float16 \n",
            " 5   enc_store_id_std           float16 \n",
            " 6   enc_cat_id_mean            float16 \n",
            " 7   enc_cat_id_std             float16 \n",
            " 8   enc_dept_id_mean           float16 \n",
            " 9   enc_dept_id_std            float16 \n",
            " 10  enc_state_id_cat_id_mean   float16 \n",
            " 11  enc_state_id_cat_id_std    float16 \n",
            " 12  enc_state_id_dept_id_mean  float16 \n",
            " 13  enc_state_id_dept_id_std   float16 \n",
            " 14  enc_store_id_cat_id_mean   float16 \n",
            " 15  enc_store_id_cat_id_std    float16 \n",
            " 16  enc_store_id_dept_id_mean  float16 \n",
            " 17  enc_store_id_dept_id_std   float16 \n",
            " 18  enc_item_id_mean           float16 \n",
            " 19  enc_item_id_std            float16 \n",
            " 20  enc_item_id_state_id_mean  float16 \n",
            " 21  enc_item_id_state_id_std   float16 \n",
            " 22  enc_item_id_store_id_mean  float16 \n",
            " 23  enc_item_id_store_id_std   float16 \n",
            "dtypes: category(1), float16(22), int16(1)\n",
            "memory usage: 2.1 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBnx1oKQYnQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl')\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2TO1RiUYrF2",
        "colab_type": "code",
        "outputId": "4246ef86-dfa7-42b9-b111-222c4531acd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#################################################################################\n",
        "print('Save final sales')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/finalsales_df.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save final sales\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuc9udfVmtqL",
        "colab_type": "code",
        "outputId": "e5faf786-f21d-4d56-87ec-df85b83855f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Dtype   \n",
            "---  ------     -----   \n",
            " 0   id         category\n",
            " 1   d          int16   \n",
            " 2   last_sale  int16   \n",
            "dtypes: category(1), int16(2)\n",
            "memory usage: 269.7 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvio8QxaCrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df =sum_snap(grid_df)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BM55-OdaS--",
        "colab_type": "code",
        "outputId": "d675cf3b-42bd-4d64-cbdd-2401f62b0174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#################################################################################\n",
        "print('Save snap')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/snap_df.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save snap\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJl1wjrmar7L",
        "colab_type": "code",
        "outputId": "0f3648f8-97fc-4788-a37c-f6b5742df290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Dtype   \n",
            "---  ------  -----   \n",
            " 0   id      category\n",
            " 1   d       int16   \n",
            " 2   snap    int64   \n",
            "dtypes: category(1), int16(1), int64(1)\n",
            "memory usage: 538.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_89Kgd70atfn",
        "colab_type": "code",
        "outputId": "b777e42d-580a-4ba7-baef-2476ae10095b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = change_event(grid_df)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0575a9acfddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Find last non zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Need some \"dances\" to fit in memory limit with groupers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mkeep_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'change_event' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYikdviNWOmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "print('Save event type')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/event_type.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QflRiwMyY3LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdWmAv56Y6LK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = add_event(grid_df)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0nOPpq-ksdb",
        "colab_type": "code",
        "outputId": "719694d2-eb07-4cf2-c406-80dae1b3db04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#################################################################################\n",
        "print('Save event value')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/event_value.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save event value\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdw6sp7MkuGG",
        "colab_type": "code",
        "outputId": "3c68c65c-9d3c-48de-b249-60c55b554629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Dtype   \n",
            "---  ------       -----   \n",
            " 0   id           category\n",
            " 1   d            int16   \n",
            " 2   Event_total  float16 \n",
            "dtypes: category(1), float16(1), int16(1)\n",
            "memory usage: 627.4 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICoSblrvlMqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}