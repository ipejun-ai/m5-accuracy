{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "m5-custom-features.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipejun-ai/m5-accuracy/blob/master/m5_custom_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND3qXCXvSL_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## In this kernel I would like to show: \n",
        "## 1. FE creation approaches\n",
        "## 2. Sequential fe validation\n",
        "## 3. Dimension reduction\n",
        "## 4. FE validation by Permutation importance\n",
        "## 5. Mean encodings\n",
        "## 6. Parallelization for FE"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "t30SwvhXSL_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os, sys, gc, warnings, psutil, random\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDah0n0xSRMO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5702933-7cc5-41b2-ead6-fed48a00a3b4"
      },
      "source": [
        "#Load google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jOgmCfMSYiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIRPATH=\"/content/gdrive/My Drive/kaggle/m5-forecasting-accuracy\"\n",
        "#DIRPATH=\"C:/Users/peiju/Documents/Study/kaggle/m5-forecasting-accuracy/\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "_d8GZ-c_SMAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "09ff09e0-3575-4e33-a94d-5a7085f8e689"
      },
      "source": [
        "########################### Load data\n",
        "########################### Basic features were created here:\n",
        "########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "#################################################################################\n",
        "\n",
        "# Read data\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "# Subsampling\n",
        "# to make all calculations faster.\n",
        "# Keep only 5% of original ids.\n",
        "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
        "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
        "\n",
        "# Let's \"inspect\" our grid DataFrame\n",
        "grid_df.info()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3002725 entries, 0 to 3002724\n",
            "Data columns (total 34 columns):\n",
            " #   Column            Dtype   \n",
            "---  ------            -----   \n",
            " 0   id                category\n",
            " 1   item_id           category\n",
            " 2   dept_id           category\n",
            " 3   cat_id            category\n",
            " 4   store_id          category\n",
            " 5   state_id          category\n",
            " 6   d                 int16   \n",
            " 7   sales             float64 \n",
            " 8   release           int16   \n",
            " 9   sell_price        float16 \n",
            " 10  price_max         float16 \n",
            " 11  price_min         float16 \n",
            " 12  price_std         float16 \n",
            " 13  price_mean        float16 \n",
            " 14  price_norm        float16 \n",
            " 15  price_nunique     float16 \n",
            " 16  item_nunique      int16   \n",
            " 17  price_momentum    float16 \n",
            " 18  price_momentum_m  float16 \n",
            " 19  price_momentum_y  float16 \n",
            " 20  event_name_1      category\n",
            " 21  event_type_1      category\n",
            " 22  event_name_2      category\n",
            " 23  event_type_2      category\n",
            " 24  snap_CA           category\n",
            " 25  snap_TX           category\n",
            " 26  snap_WI           category\n",
            " 27  tm_d              int8    \n",
            " 28  tm_w              int8    \n",
            " 29  tm_m              int8    \n",
            " 30  tm_y              int8    \n",
            " 31  tm_wm             int8    \n",
            " 32  tm_dw             int8    \n",
            " 33  tm_w_end          int8    \n",
            "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
            "memory usage: 162.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShWtUl3LSMAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4fc0faf3-85fa-43a0-e584-4c819fc18e9a"
      },
      "source": [
        "########################### Baseline model\n",
        "#################################################################################\n",
        "\n",
        "# We will need some global VARS for future\n",
        "\n",
        "SEED = 42             # Our random seed for everything\n",
        "random.seed(SEED)     # to make all tests \"deterministic\"\n",
        "np.random.seed(SEED)\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "TARGET = 'sales'      # Our Target\n",
        "END_TRAIN = 1913+28      # And we will use last 28 days as validation\n",
        "\n",
        "SHIFT_DAY=28\n",
        "# Drop some items from \"TEST\" set part (1914...)\n",
        "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
        "\n",
        "# Features that we want to exclude from training\n",
        "remove_features = ['id','d',TARGET]\n",
        "\n",
        "# Our baseline model serves\n",
        "# to do fast checks of\n",
        "# new features performance \n",
        "\n",
        "# We will use LightGBM for our tests\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
        "                    'objective': 'regression',       # Standart loss for RMSE\n",
        "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
        "                    'subsample': 0.8,                \n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
        "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
        "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
        "                    'feature_fraction': 0.8,\n",
        "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
        "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
        "                    'seed': SEED,\n",
        "                    'verbose': -1,\n",
        "                } \n",
        "\n",
        "## RMSE\n",
        "def rmse(y, y_pred):\n",
        "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
        "\n",
        "# Small function to make fast features tests\n",
        "# estimator = make_fast_test(grid_df)\n",
        "# it will return lgb booster for future analisys\n",
        "def make_fast_test(df):\n",
        "\n",
        "    features_columns = [col for col in list(df) if col not in remove_features]\n",
        "\n",
        "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n",
        "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
        "    \n",
        "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
        "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
        "    \n",
        "    estimator = lgb.train(\n",
        "                            lgb_params,\n",
        "                            train_data,\n",
        "                            valid_sets = [train_data,valid_data],\n",
        "                            verbose_eval = 500,\n",
        "                        )\n",
        "    \n",
        "    return estimator\n",
        "\n",
        "# Make baseline model\n",
        "baseline_model = make_fast_test(grid_df)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[379]\ttraining's rmse: 2.79812\tvalid_1's rmse: 2.39787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4V8HPqvSMAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7b81d3c2-1d5d-4af4-c664-97ea734296d2"
      },
      "source": [
        "########################### Lets test our normal Lags (7 days)\n",
        "########################### Some more info about lags here:\n",
        "########################### https://www.kaggle.com/kyakovlev/m5-lags-features\n",
        "#################################################################################\n",
        "\n",
        "# Small helper to make lags creation faster\n",
        "from multiprocessing import Pool                # Multiprocess Runs\n",
        "\n",
        "## Multiprocessing Run.\n",
        "# :t_split - int of lags days                   # type: int\n",
        "# :func - Function to apply on each split       # type: python function\n",
        "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "def make_normal_lag(lag_day):\n",
        "    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n",
        "    col_name = 'sales_lag_'+str(lag_day)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "# Launch parallel lag creation\n",
        "# and \"append\" to our grid\n",
        "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[325]\ttraining's rmse: 2.56457\tvalid_1's rmse: 2.26084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DDAWPhvSMBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "4a87dd0e-c11a-4a1e-b243-910d6b5d98cf"
      },
      "source": [
        "########################### Permutation importance Test\n",
        "########################### https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "#################################################################################\n",
        "\n",
        "# Let's creat validation dataset and features\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "\n",
        "# Make normal prediction with our model and save score\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "\n",
        "# Now we are looping over all our numerical features\n",
        "for col in features_columns:\n",
        "    \n",
        "    # We will make validation set copy to restore\n",
        "    # features states on each run\n",
        "    temp_df = validation_df.copy()\n",
        "    \n",
        "    # Error here appears if we have \"categorical\" features and can't \n",
        "    # do np.random.permutation without disrupt categories\n",
        "    # so we need to check if feature is numerical\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        \n",
        "        # If our current rmse score is less than base score\n",
        "        # it means that feature most probably is a bad one\n",
        "        # and our model is learning on noise\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "# Remove Temp data\n",
        "del temp_df, validation_df\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 1 days shift (nearest past) are important\n",
        "## Some other features are not important and probably just noise\n",
        "## Better make several Permutation runs to confirm useless of the feature\n",
        "## link again https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "\n",
        "## price_nunique -0.002 : strong negative values are most probably noise\n",
        "## price_max -0.0002 : values close to 0 need deeper investigation\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standart RMSE 2.260843574932562\n",
            "release 0.0\n",
            "sell_price 0.0038\n",
            "price_max 0.0001\n",
            "price_min 0.0009\n",
            "price_std 0.0029\n",
            "price_mean 0.0035\n",
            "price_norm 0.0075\n",
            "price_nunique -0.0001\n",
            "item_nunique -0.0\n",
            "price_momentum -0.0001\n",
            "price_momentum_m 0.0034\n",
            "price_momentum_y 0.0005\n",
            "tm_d 0.0106\n",
            "tm_w 0.0003\n",
            "tm_m -0.0001\n",
            "tm_y 0.0\n",
            "tm_wm 0.0002\n",
            "tm_dw 0.1403\n",
            "tm_w_end 0.0087\n",
            "sales_lag_1 0.5897\n",
            "sales_lag_2 0.0465\n",
            "sales_lag_3 0.0219\n",
            "sales_lag_4 0.012\n",
            "sales_lag_5 0.0183\n",
            "sales_lag_6 0.019\n",
            "sales_lag_7 0.0448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJph3WcmSMBT",
        "colab_type": "text"
      },
      "source": [
        "from eli5 documentation (seems it's perfect explanation)\n",
        "\n",
        "The idea is the following: feature importance can be measured by looking at how much the score (accuracy, mse, rmse, mae, etc. - any score we’re interested in) decreases when a feature is not available.\n",
        "\n",
        "To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n",
        "\n",
        "To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can **replace it with random noise** - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the **same distribution as original feature values** (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
        "\n",
        "---\n",
        "\n",
        "It's not good when feature remove (replaced by noise) but we have better score. Simple and easy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ExQEP7OSMBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Lets test far away Lags (7 days with 56 days shift)\n",
        "########################### and check permutation importance\n",
        "#################################################################################\n",
        "\n",
        "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "for col in features_columns:\n",
        "    temp_df = validation_df.copy()\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "del temp_df, validation_df\n",
        "        \n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 56 days shift (far away past) are not as important\n",
        "## as nearest past lags\n",
        "## and at some point will be just noise for our model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir1VAgGvSMBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### PCA\n",
        "#################################################################################\n",
        "\n",
        "# The main question here - can we have \n",
        "# almost same rmse boost with less features\n",
        "# less dimensionality?\n",
        "\n",
        "# Lets try PCA and make 7->3 dimensionality reduction\n",
        "\n",
        "# PCA is \"unsupervised\" learning\n",
        "# and with shifted target we can be sure\n",
        "# that we have no Target leakage\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def make_pca(df, pca_col, n_days):\n",
        "    print('PCA:', pca_col, n_days)\n",
        "    \n",
        "    # We don't need any other columns to make pca\n",
        "    pca_df = df[[pca_col,'d',TARGET]]\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\" \n",
        "    # we need to agg first\n",
        "    if pca_col != 'id':\n",
        "        merge_base = pca_df[[pca_col,'d']]\n",
        "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
        "        pca_df[TARGET] = pca_df['sum']\n",
        "        del pca_df['sum']\n",
        "    \n",
        "    # Min/Max scaling\n",
        "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
        "    \n",
        "    # Making \"lag\" in old way (not parallel)\n",
        "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
        "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
        "    pca_df = pca_df.assign(**{\n",
        "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
        "            for l in LAG_DAYS\n",
        "            for col in [TARGET]\n",
        "        })\n",
        "    \n",
        "    pca_columns = list(pca_df)[3:]\n",
        "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
        "    pca = PCA(random_state=SEED)\n",
        "    \n",
        "    # You can use fit_transform here\n",
        "    pca.fit(pca_df[pca_columns])\n",
        "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
        "    \n",
        "    print(pca.explained_variance_ratio_)\n",
        "    \n",
        "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
        "    keep_cols = pca_columns[:3]\n",
        "    print('Columns to keep:', keep_cols)\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\"\n",
        "    # we need merge back our results to merge_base df\n",
        "    # and only than return resulted df\n",
        "    # I'll skip that step here\n",
        "    \n",
        "    return pca_df[keep_cols]\n",
        "\n",
        "\n",
        "# Make PCA\n",
        "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqPDlCQqSMB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Mean/std target encoding\n",
        "#################################################################################\n",
        "\n",
        "# We will use these three columns for test\n",
        "# (in combination with store_id)\n",
        "icols = ['item_id','cat_id','dept_id']\n",
        "\n",
        "# But we can use any other column or even multiple groups\n",
        "# like these ones\n",
        "#            'state_id',\n",
        "#            'store_id',\n",
        "#            'cat_id',\n",
        "#            'dept_id',\n",
        "#            ['state_id', 'cat_id'],\n",
        "#            ['state_id', 'dept_id'],\n",
        "#            ['store_id', 'cat_id'],\n",
        "#            ['store_id', 'dept_id'],\n",
        "#            'item_id',\n",
        "#            ['item_id', 'state_id'],\n",
        "#            ['item_id', 'store_id']\n",
        "\n",
        "# There are several ways to do \"mean\" encoding\n",
        "## K-fold scheme\n",
        "## LOO (leave one out)\n",
        "## Smoothed/regularized \n",
        "## Expanding mean\n",
        "## etc \n",
        "\n",
        "# You can test as many options as you want\n",
        "# and decide what to use\n",
        "# Because of memory issues you can't \n",
        "# use many features.\n",
        "\n",
        "# We will use simple target encoding\n",
        "# by std and mean agg\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n",
        "    \n",
        "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
        "    joiner = '_'+col+'_encoding_'\n",
        "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
        "    temp_df = temp_df.reset_index()\n",
        "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
        "    del temp_df\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "# Bad thing that for some items  \n",
        "# we are using past and future values.\n",
        "# But we are looking for \"categorical\" similiarity\n",
        "# on a \"long run\". So future here is not a big problem."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ZdSmM0SMCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Last non O sale\n",
        "#################################################################################\n",
        "\n",
        "def find_last_sale(df,n_day):\n",
        "    \n",
        "    # Limit initial df\n",
        "    ls_df = df[['id','d',TARGET]]\n",
        "    \n",
        "    # Convert target to binary\n",
        "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
        "    \n",
        "    # Make lags to prevent any leakage\n",
        "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
        "\n",
        "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
        "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
        "\n",
        "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
        "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
        "\n",
        "    return ls_df[['last_sale']]\n",
        "\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9clO5-XFQDvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######snap\n",
        "def sum_snap(df):\n",
        "  df[\"snap\"]=0\n",
        "  df[\"snap\"].where(df['state_id'] != 'CA',df[\"snap_CA\"],inplace=True)\n",
        "  df[\"snap\"].where(df['state_id'] != 'TX',df[\"snap_TX\"],inplace=True)\n",
        "  df[\"snap\"].where(df['state_id'] != 'WI',df[\"snap_WI\"],inplace=True)\n",
        "  df.drop([\"snap_CA\",\"snap_TX\",\"snap_WI\"],axis=1)\n",
        "  return df\n",
        "\n",
        "grid_df1=sum_snap(grid_df)\n",
        "\n",
        " # Make features test\n",
        "test_model = make_fast_test(grid_df1) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp1uobA_SzAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def change_event(df):\n",
        "  event_type = pd.read_pickle(\"/content/gdrive/My Drive/kaggle/df_event_type.pickle.gz\")\n",
        "  event_type_columns=event_type.columns\n",
        "  event_type_columns=event_type_columns.drop(\"d\")\n",
        "\n",
        "  event_type[event_type_columns]=event_type[event_type_columns].astype(\"category\")\n",
        "\n",
        "  event_type[\"d\"]=event_type[\"d\"].str.replace(\"d_\",\"\").astype(\"int16\")\n",
        "  df=pd.merge(df,event_type,left_on=\"d\",right_on=\"d\",how=\"left\")\n",
        "  return df\n",
        "\n",
        "grid_df=change_event(grid_df)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAYBTKF8aw3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "13c533e8-4695-43ec-d9f3-d517dabd7b22"
      },
      "source": [
        "def add_event(df):\n",
        "  event_value = pd.read_pickle(\"/content/gdrive/My Drive/kaggle/output/event_importance/event_value.pkl\")\n",
        "  \n",
        "  event_value_columns=event_value.columns\n",
        "  event_value_columns=event_value_columns.drop(\"d\")\n",
        "\n",
        "  event_value_columns=[\"Event_total\"]\n",
        "\n",
        "  event_value[event_value_columns]=event_value[event_value_columns].astype(\"float16\")\n",
        "\n",
        "  event_value[\"d\"]=event_value[\"d\"].str.replace(\"d_\",\"\").astype(\"int16\")\n",
        "  #cols=[\"d\", 'Sports', 'Religious', 'National', 'Cultural']\n",
        "  cols=[\"d\",\"Event_total\"]\n",
        "  df=pd.merge(df,event_value[cols],left_on=\"d\",right_on=\"d\",how=\"left\")\n",
        "  return df\n",
        "\n",
        "grid_df1=add_event(grid_df)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df1) \n",
        "\n",
        "\n",
        "features_columns = [col for col in list(grid_df1) if col not in remove_features]\n",
        "validation_df = grid_df1[grid_df1['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "for col in features_columns:\n",
        "    temp_df = validation_df.copy()\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "del temp_df, validation_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[321]\ttraining's rmse: 2.81841\tvalid_1's rmse: 2.38378\n",
            "Standart RMSE 2.383784659504859\n",
            "release 0.0\n",
            "sell_price 0.0152\n",
            "price_max 0.0347\n",
            "price_min 0.0111\n",
            "price_std 0.0298\n",
            "price_mean 0.0111\n",
            "price_norm 0.0123\n",
            "price_nunique 0.0081\n",
            "item_nunique 0.0013\n",
            "price_momentum 0.0004\n",
            "price_momentum_m 0.022\n",
            "price_momentum_y 0.0122\n",
            "tm_d 0.0081\n",
            "tm_w 0.0035\n",
            "tm_m 0.0018\n",
            "tm_y 0.0\n",
            "tm_wm 0.0003\n",
            "tm_dw 0.1746\n",
            "tm_w_end 0.0081\n",
            "Event_total 0.0018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r5X02E-j8sF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "925bf02f-b1ff-44a4-9681-70cbf4aec39f"
      },
      "source": [
        "grid_df[grid_df[\"id\"]==\"HOBBIES_1_008_CA_1_validation\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>release</th>\n",
              "      <th>sell_price</th>\n",
              "      <th>price_max</th>\n",
              "      <th>price_min</th>\n",
              "      <th>price_std</th>\n",
              "      <th>price_mean</th>\n",
              "      <th>price_norm</th>\n",
              "      <th>price_nunique</th>\n",
              "      <th>item_nunique</th>\n",
              "      <th>price_momentum</th>\n",
              "      <th>price_momentum_m</th>\n",
              "      <th>price_momentum_y</th>\n",
              "      <th>event_name_1</th>\n",
              "      <th>event_type_1</th>\n",
              "      <th>event_name_2</th>\n",
              "      <th>event_type_2</th>\n",
              "      <th>snap_CA</th>\n",
              "      <th>snap_TX</th>\n",
              "      <th>snap_WI</th>\n",
              "      <th>tm_d</th>\n",
              "      <th>tm_w</th>\n",
              "      <th>tm_m</th>\n",
              "      <th>tm_y</th>\n",
              "      <th>tm_wm</th>\n",
              "      <th>tm_dw</th>\n",
              "      <th>tm_w_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.459961</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.919922</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1525</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>2</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.459961</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.919922</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3050</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.459961</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.919922</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4575</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.459961</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.919922</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6100</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.459961</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.919922</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.949219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2909700</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1909</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.479980</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.959961</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.014648</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2911225</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1910</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.479980</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.959961</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.014648</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2912750</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1911</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.479980</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.959961</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.014648</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2914275</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1912</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.479980</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.959961</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.014648</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2915800</th>\n",
              "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_008</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1913</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.479980</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.419922</td>\n",
              "      <td>0.01976</td>\n",
              "      <td>0.476318</td>\n",
              "      <td>0.959961</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.014648</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1913 rows × 34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    id        item_id  ... tm_dw tm_w_end\n",
              "0        HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     5        1\n",
              "1525     HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     6        1\n",
              "3050     HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     0        0\n",
              "4575     HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     1        0\n",
              "6100     HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     2        0\n",
              "...                                ...            ...  ...   ...      ...\n",
              "2909700  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     2        0\n",
              "2911225  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     3        0\n",
              "2912750  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     4        0\n",
              "2914275  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     5        1\n",
              "2915800  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  ...     6        1\n",
              "\n",
              "[1913 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shIPP3TSh5h9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0d905a6d-dc91-400f-8491-60c0179af0dc"
      },
      "source": [
        "def add_holiday(df):\n",
        "  df[\"holiday\"]=0\n",
        "  #df[\"holiday\"].where(df[\"tm_dw\"]<=4,1,inplace=True)\n",
        "  df.loc[df[\"tm_dw\"]>=5,\"holiday\"]=1\n",
        "  df.loc[df[\"event_type_1\"]==\"National\",\"holiday\"]=1\n",
        "\n",
        "  #holidayの売上\n",
        "  df[\"h_sales\"]=0\n",
        "  df.loc[df[\"holiday\"]==1,\"h_sales\"]=df[\"sales\"]\n",
        "\n",
        "  #non-holidayの売上\n",
        "  df[\"w_sales\"]=0\n",
        "  df.loc[df[\"holiday\"]==0,\"w_sales\"]=df[\"sales\"]\n",
        "\n",
        "  for i in [7,14,30,60,180]: #,14,30,60,180\n",
        "    print('Rolling period:', i)\n",
        "    df[\"holiday_sum\"] = df.groupby(['id'])[\"h_sales\"].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).sum())\n",
        "    df[\"holiday_count\"] = df.groupby(['id'])[\"holiday\"].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).sum())\n",
        "    \n",
        "    df[\"weekday_sum\"] = df.groupby(['id'])[\"w_sales\"].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).sum())\n",
        "    df[\"weekday_count\"] = i - df[\"holiday_count\"]\n",
        "\n",
        "    df.loc[df[\"holiday\"]==1,\"rolling_holiday_mean_\"+str(i)]=(df[\"holiday_sum\"]/df[\"holiday_count\"]).astype(np.float16)\n",
        "    df.loc[df[\"holiday\"]==0,\"rolling_holiday_mean_\"+str(i)]=(df[\"weekday_sum\"]/df[\"weekday_count\"]).astype(np.float16)\n",
        "    \n",
        "  df=df.drop([\"h_sales\",\"w_sales\",\"holiday_sum\",\"holiday_count\",\"weekday_sum\",\"weekday_count\"],axis=1)\n",
        "  \n",
        "\n",
        "\n",
        "  return df\n",
        "grid_df1=add_holiday(grid_df)\n",
        "grid_df1[grid_df[\"id\"]==\"HOBBIES_1_008_CA_1_validation\"].tail(20)\n",
        "grid_df1.loc[grid_df1[\"event_type_1\"]==\"National\",\"holiday\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rolling period: 7\n",
            "Rolling period: 14\n",
            "Rolling period: 30\n",
            "Rolling period: 60\n",
            "Rolling period: 180\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35075      1\n",
              "35076      1\n",
              "35077      1\n",
              "35078      1\n",
              "35079      1\n",
              "          ..\n",
              "2812095    1\n",
              "2812096    1\n",
              "2812097    1\n",
              "2812098    1\n",
              "2812099    1\n",
              "Name: holiday, Length: 77775, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WksKEZTAwsrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df1[grid_df[\"id\"]==\"HOBBIES_1_008_CA_1_validation\"].to_csv(\"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzvs3ieuSMCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "d1b95270-209b-4b3e-c4e9-75af8c8ca28e"
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl')\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "icols =  [\n",
        "            ['state_id'],\n",
        "            ['store_id'],\n",
        "            ['cat_id'],\n",
        "            ['dept_id'],\n",
        "            ['state_id', 'cat_id'],\n",
        "            ['state_id', 'dept_id'],\n",
        "            ['store_id', 'cat_id'],\n",
        "            ['store_id', 'dept_id'],\n",
        "            ['item_id'],\n",
        "            ['item_id', 'state_id'],\n",
        "            ['item_id', 'store_id']\n",
        "            ]\n",
        "\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    col_name = '_'+'_'.join(col)+'_'\n",
        "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
        "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding ['state_id']\n",
            "Encoding ['store_id']\n",
            "Encoding ['cat_id']\n",
            "Encoding ['dept_id']\n",
            "Encoding ['state_id', 'cat_id']\n",
            "Encoding ['state_id', 'dept_id']\n",
            "Encoding ['store_id', 'cat_id']\n",
            "Encoding ['store_id', 'dept_id']\n",
            "Encoding ['item_id']\n",
            "Encoding ['item_id', 'state_id']\n",
            "Encoding ['item_id', 'store_id']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEF99yfcSMCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "357d6193-1b30-4f6d-e5de-0a3115e82dc6"
      },
      "source": [
        "#################################################################################\n",
        "print('Save Mean/Std encoding')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/mean_encoding_df.pkl')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Mean/Std encoding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr6C4021SMC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "808fd918-fcd0-4861-a513-5e7ad830ce6b"
      },
      "source": [
        "########################### Final list of new features\n",
        "#################################################################################\n",
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 24 columns):\n",
            " #   Column                     Dtype   \n",
            "---  ------                     -----   \n",
            " 0   id                         category\n",
            " 1   d                          int16   \n",
            " 2   enc_state_id_mean          float16 \n",
            " 3   enc_state_id_std           float16 \n",
            " 4   enc_store_id_mean          float16 \n",
            " 5   enc_store_id_std           float16 \n",
            " 6   enc_cat_id_mean            float16 \n",
            " 7   enc_cat_id_std             float16 \n",
            " 8   enc_dept_id_mean           float16 \n",
            " 9   enc_dept_id_std            float16 \n",
            " 10  enc_state_id_cat_id_mean   float16 \n",
            " 11  enc_state_id_cat_id_std    float16 \n",
            " 12  enc_state_id_dept_id_mean  float16 \n",
            " 13  enc_state_id_dept_id_std   float16 \n",
            " 14  enc_store_id_cat_id_mean   float16 \n",
            " 15  enc_store_id_cat_id_std    float16 \n",
            " 16  enc_store_id_dept_id_mean  float16 \n",
            " 17  enc_store_id_dept_id_std   float16 \n",
            " 18  enc_item_id_mean           float16 \n",
            " 19  enc_item_id_std            float16 \n",
            " 20  enc_item_id_state_id_mean  float16 \n",
            " 21  enc_item_id_state_id_std   float16 \n",
            " 22  enc_item_id_store_id_mean  float16 \n",
            " 23  enc_item_id_store_id_std   float16 \n",
            "dtypes: category(1), float16(22), int16(1)\n",
            "memory usage: 2.1 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS0XHbkichvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRd9nwpQZlUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_nonzero(df):\n",
        "  #0count\n",
        "  df[\"is_zero\"] = np.where(df[\"value\"] == 0, 1, 0)\n",
        "  df['rolling_7_zero_count'] = df.groupby(['id'])[\"is_zero\"].shift(28).rolling(7).sum().fillna(0)\n",
        "  df['rolling_28_zero_count'] = df.groupby(['id'])[\"is_zero\"].shift(28).rolling(28).sum().fillna(0)\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QUX_1GDbYIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"MOON PHASE CALCULATION\n",
        "credits to: https://gist.github.com/miklb/ed145757971096565723\n",
        "moonphase.py - Calculate Lunar Phase\n",
        "Author: Sean B. Palmer, inamidst.com\n",
        "Cf. http://en.wikipedia.org/wiki/Lunar_phase#Lunar_phase_calculation\n",
        "\"\"\"\n",
        "import math, decimal\n",
        "dec = decimal.Decimal\n",
        "\n",
        "def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
        "    diff = d - datetime(2001, 1, 1)\n",
        "    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
        "    lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
        "    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
        "    return int(phase_index) & 7\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBnx1oKQYnQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl')\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2TO1RiUYrF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4246ef86-dfa7-42b9-b111-222c4531acd6"
      },
      "source": [
        "#################################################################################\n",
        "print('Save final sales')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/finalsales_df.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save final sales\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuc9udfVmtqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e5faf786-f21d-4d56-87ec-df85b83855f4"
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Dtype   \n",
            "---  ------     -----   \n",
            " 0   id         category\n",
            " 1   d          int16   \n",
            " 2   last_sale  int16   \n",
            "dtypes: category(1), int16(2)\n",
            "memory usage: 269.7 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvio8QxaCrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df =sum_snap(grid_df)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BM55-OdaS--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d675cf3b-42bd-4d64-cbdd-2401f62b0174"
      },
      "source": [
        "#################################################################################\n",
        "print('Save snap')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/snap_df.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save snap\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJl1wjrmar7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0f3648f8-97fc-4788-a37c-f6b5742df290"
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Dtype   \n",
            "---  ------  -----   \n",
            " 0   id      category\n",
            " 1   d       int16   \n",
            " 2   snap    int64   \n",
            "dtypes: category(1), int16(1), int64(1)\n",
            "memory usage: 538.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_89Kgd70atfn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b777e42d-580a-4ba7-baef-2476ae10095b"
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = change_event(grid_df)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0575a9acfddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Find last non zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Need some \"dances\" to fit in memory limit with groupers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mkeep_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'change_event' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYikdviNWOmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "print('Save event type')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/event_type.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QflRiwMyY3LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdWmAv56Y6LK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = add_event(grid_df)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0nOPpq-ksdb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "719694d2-eb07-4cf2-c406-80dae1b3db04"
      },
      "source": [
        "#################################################################################\n",
        "print('Save event value')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/event_value.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save event value\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdw6sp7MkuGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "3c68c65c-9d3c-48de-b249-60c55b554629"
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 46881677 entries, 0 to 46881676\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Dtype   \n",
            "---  ------       -----   \n",
            " 0   id           category\n",
            " 1   d            int16   \n",
            " 2   Event_total  float16 \n",
            "dtypes: category(1), float16(1), int16(1)\n",
            "memory usage: 627.4 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICoSblrvlMqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ba823f59-b462-4cfb-c55e-aead54c78ea7"
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.concat([pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_1.pkl'),\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(DIRPATH+'/output/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = add_holiday(grid_df)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rolling period: 7\n",
            "Rolling period: 14\n",
            "Rolling period: 30\n",
            "Rolling period: 60\n",
            "Rolling period: 180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZQDXf3THoka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bb35013-3eda-4e83-b38e-343004a57001"
      },
      "source": [
        "#################################################################################\n",
        "print('Save event value')\n",
        "grid_df.to_pickle(DIRPATH+'/output/m5-custom-features/holiday_mean.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save event value\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol-yWCuYH1cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}